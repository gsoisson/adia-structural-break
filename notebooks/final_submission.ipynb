{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssM62-sDkX7N"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUeixiC_IJM"
      },
      "outputs": [],
      "source": [
        "# %pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "# !crunch setup-notebook structural-break nRmrHs5mYNH8RZ2ksRNw1qLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "75b022d0-b3ae-4c07-e136-8ea22537731f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 8.0.0\n",
            "available ram: 8.00 gb\n",
            "available cpu: 8 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxxpLCygkX7S"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from joblib import dump, load\n",
        "import re\n",
        "\n",
        "# Typing\n",
        "from typing import Iterable\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# FOr features\n",
        "import math\n",
        "import pywt  # for wavelet transforms\n",
        "from scipy.special import (\n",
        "    gammaincc,\n",
        ")  # for chi-square survival function (ARCH-LM p-values)\n",
        "\n",
        "# Machine learning frameworks\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost.callback import EarlyStopping as XGBEarlyStop\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedGroupKFold, BaseCrossValidator\n",
        "from sklearn.base import ClassifierMixin, BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "# Hyperparameter optimization\n",
        "import optuna\n",
        "from optuna import Trial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDukZtPZkX7U"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNMWHAUbkX7V"
      },
      "outputs": [],
      "source": [
        "# @crunch/keep:on\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# GLOBAL CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RANDOM_STATE = 69\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "EPS = 1e-9\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# INFER CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "INFERENCE_MODE = \"full\"\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# STACKING CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "BASE_LEARNERS = (\n",
        "    \"xgb_main\",\n",
        "    \"xgb_lite\",\n",
        "    \"lgb_main\",\n",
        "    \"cat_main\",\n",
        ")\n",
        "\n",
        "N_SEEDS = 20\n",
        "TOP_SEEDS = 2\n",
        "\n",
        "FULL_REFIT = True\n",
        "FULL_HP_SELECTION = \"consensus\"\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# TRAIN_MODEL CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "N_OPTUNA_TRIALS = 32\n",
        "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "\n",
        "K_OUTER = 5\n",
        "K_MAX_INNER = 5\n",
        "K_STOP_INNER = 1\n",
        "\n",
        "TOPK_MIN_AUC = 0.52\n",
        "TOPK_FEATURES = 60\n",
        "TOPK_ALWAYS_KEEP = []\n",
        "\n",
        "EXCLUDE_FEATURE_KEYWORDS = [\"logit\", \"fisher\", \"logratio\", \"scaled\"]\n",
        "\n",
        "\n",
        "MAX_BIN = 64\n",
        "EARLY_STOPPING = 100\n",
        "\n",
        "\n",
        "MODEL_DIR = Path(\"resources/model\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# FEATURE EXTRACTION CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# Moments\n",
        "QUANTILE_COARSE_GRID = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
        "\n",
        "# Quantiles\n",
        "QUANTILE_FINE_GRID = [0.05, 0.10, 0.25, 0.40, 0.50, 0.60, 0.75, 0.90, 0.95]\n",
        "TOP_K = 5\n",
        "\n",
        "# Crossing rates\n",
        "W_FANO = 50\n",
        "CROSSING_RATE_DEADBAND = 0.1\n",
        "\n",
        "# Autocorrelation\n",
        "ACF_MAX_LAG = 10\n",
        "LBQ_M = 20\n",
        "\n",
        "# Tests & Distances\n",
        "JS_QUANTILE_BINS = np.linspace(0.0, 1.0, 33)\n",
        "MMD_MAX_N = 512\n",
        "\n",
        "# Frequency\n",
        "FREQ_BANDS = ((0.00, 0.05), (0.05, 0.15), (0.15, 0.30), (0.30, 0.50))\n",
        "DWT_WAVELET = \"db2\"\n",
        "DWT_LEVEL = 3\n",
        "ENTROPY_M1, ENTROPY_M2 = 3, 5\n",
        "ENTROPY_TAU = 1\n",
        "\n",
        "# Boundary\n",
        "BOUND_EDGE = 5\n",
        "BOUND_WINDOW_SIZES = [32, 128]\n",
        "BOUND_SKIP_AFTER = 0\n",
        "BOUND_ACF_MAX_LAG = 6\n",
        "ARCH_L = 5\n",
        "BOUND_OFFSETS = (0, 8, 16, 32)\n",
        "\n",
        "# Rolling\n",
        "ROLL_WINDOWS = (10, 20, 50, 100, 200)\n",
        "ROLL_MIN_POS_PER_HALF = 20\n",
        "ROLL_TOPK = 3\n",
        "EWVAR_HALFLIVES = (200, 400)\n",
        "\n",
        "# AR\n",
        "AR_ORDER = 1\n",
        "AR_RIDGE_LAMBDA = 1.0\n",
        "AR_SCORE_CAP = 256\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# PREPROCESS CONFIG\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "# Floors computed as the 5% bottom quantile of s0, s_dz, s_abs, s_dd\n",
        "S0_FLOOR = 0.0008216\n",
        "S_DZ_FLOOR = 1.0909\n",
        "S_ABS_FLOOR = 0.5555\n",
        "S_DD_FLOOR = 1.6383\n",
        "CLIP_QLOW, CLIP_QHIGH = 0.002, 0.998\n",
        "CLIP_MIN_WIDTH = 1.0\n",
        "CLIP_DEFAULT_BAND = 7.0  # computed as the 0.2% and 99.8% quantiles of z_before\n",
        "FEAT_CACHE_DIR = Path(\"resources/features\")\n",
        "FEAT_CACHE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# @crunch/keep:off\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCJImHeJkX7X"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwvOjzsLkX7Y"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Helper functions\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def pad_periods(X, period, max_len):\n",
        "    \"\"\"Return padded 2D array for one period (before/after).\"\"\"\n",
        "    grouped = X.loc[X[\"period\"] == period].groupby(\"id\")[\"value\"].apply(np.array)\n",
        "    n_series = len(grouped)\n",
        "    arr = np.full((n_series, max_len), np.nan, dtype=np.float32)\n",
        "    for i, g in enumerate(grouped):\n",
        "        arr[i, : len(g)] = g\n",
        "    return arr, grouped.index\n",
        "\n",
        "\n",
        "def mad(arr, axis=1):\n",
        "    \"\"\"Median absolute deviation along axis.\"\"\"\n",
        "    med = np.nanmedian(arr, axis=axis, keepdims=True)\n",
        "    return np.nanmedian(np.abs(arr - med), axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def winsorize_pair(\n",
        "    before,\n",
        "    after,\n",
        "    qlow=CLIP_QLOW,\n",
        "    qhigh=CLIP_QHIGH,\n",
        "    min_width=CLIP_MIN_WIDTH,  # required (qh-ql) span in standardized units\n",
        "    default_band=CLIP_DEFAULT_BAND,\n",
        "):\n",
        "    \"\"\"\n",
        "    Winsorize both segments using BEFORE per-ID cutoffs; fallback to default_band if BEFORE span collapses.\n",
        "    Inputs should already be standardized (z).\n",
        "    \"\"\"\n",
        "    # per-ID BEFORE quantiles\n",
        "    ql_id = np.nanquantile(before, qlow, axis=1, keepdims=True)\n",
        "    qh_id = np.nanquantile(before, qhigh, axis=1, keepdims=True)\n",
        "\n",
        "    width = qh_id - ql_id\n",
        "    wide_enough = np.isfinite(width) & (width >= min_width)\n",
        "\n",
        "    # choose fallback band\n",
        "    ql_default, qh_default = -float(default_band), float(default_band)\n",
        "\n",
        "    ql = np.where(wide_enough, ql_id, ql_default)\n",
        "    qh = np.where(wide_enough, qh_id, qh_default)\n",
        "\n",
        "    wb = np.where(np.isnan(before), np.nan, np.clip(before, ql, qh))\n",
        "    wa = np.where(np.isnan(after), np.nan, np.clip(after, ql, qh))\n",
        "    return wb.astype(np.float32), wa.astype(np.float32)\n",
        "\n",
        "\n",
        "def fast_detrend_ols(arr, mean_center=True):\n",
        "    \"\"\"\n",
        "    Vectorized linear detrend per row with NaN masks.\n",
        "    Returns mean-centered residuals (float32).\n",
        "    \"\"\"\n",
        "    y = arr.astype(np.float32, copy=False)\n",
        "    _, T = y.shape\n",
        "\n",
        "    # x-axis: 0..T-1, then center per row to improve numerics\n",
        "    t = np.arange(T, dtype=np.float32)[None, :]  # (1, T)\n",
        "    mask = ~np.isnan(y)  # (n, T)\n",
        "    cnt = mask.sum(axis=1, keepdims=True).astype(np.float32)\n",
        "\n",
        "    t_sum = (mask * t).sum(axis=1, keepdims=True)  # Σ t_i\n",
        "    y_sum = np.nansum(y, axis=1, keepdims=True)  # Σ y_i\n",
        "\n",
        "    t_bar = t_sum / cnt  # \\bar t\n",
        "    y_bar = y_sum / cnt  # \\bar y\n",
        "\n",
        "    tc = t - t_bar  # center x\n",
        "    yc = np.where(mask, y - y_bar, 0.0)  # center y where valid\n",
        "\n",
        "    num = (tc * yc * mask).sum(axis=1, keepdims=True)  # Σ (tc * yc)\n",
        "    den = (tc * tc * mask).sum(axis=1, keepdims=True)  # Σ (tc^2)\n",
        "    den = np.where(den <= EPS, EPS, den)  # floor\n",
        "\n",
        "    slope = num / den\n",
        "    intercept = y_bar - slope * t_bar\n",
        "\n",
        "    yhat = intercept + slope * t\n",
        "    resid = np.where(mask, y - yhat, np.nan)\n",
        "\n",
        "    if mean_center:\n",
        "        resid = resid - np.nanmean(resid, axis=1, keepdims=True)\n",
        "\n",
        "    return resid.astype(np.float32)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Main pipeline\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def process_series(X_train):\n",
        "    \"\"\"Heavy NumPy preprocessing for the entire dataset.\"\"\"\n",
        "\n",
        "    # --- Build padded arrays\n",
        "    max_before = X_train.loc[X_train[\"period\"] == 0].groupby(\"id\").size().max()\n",
        "    max_after = X_train.loc[X_train[\"period\"] == 1].groupby(\"id\").size().max()\n",
        "    before_arr, ids = pad_periods(X_train, 0, max_before)\n",
        "    after_arr, _ = pad_periods(X_train, 1, max_after)\n",
        "\n",
        "    # --- Standardization by before stats (robust)\n",
        "    m0 = np.nanmedian(before_arr, axis=1, keepdims=True)\n",
        "    s0 = 1.4826 * mad(before_arr)\n",
        "    s0 = np.maximum(s0, S0_FLOOR)\n",
        "    z_before = (before_arr - m0) / s0\n",
        "    z_after = (after_arr - m0) / s0\n",
        "\n",
        "    # --- Winsorized standardized (replaces fixed np.clip)\n",
        "    zc_before, zc_after = winsorize_pair(z_before, z_after)\n",
        "\n",
        "    # --- Detrended (per segment) on winsorized standardized\n",
        "    zd_before = fast_detrend_ols(zc_before)\n",
        "    zd_after = fast_detrend_ols(zc_after)\n",
        "\n",
        "    # --- Segment-aware diffs on standardized (pre-winsor)\n",
        "    dz_before = np.diff(z_before, axis=1, prepend=z_before[:, [0]])\n",
        "    dz_after = np.diff(z_after, axis=1, prepend=z_after[:, [0]])\n",
        "\n",
        "    m_dz = np.nanmedian(dz_before, axis=1, keepdims=True)\n",
        "    s_dz = 1.4826 * mad(dz_before)\n",
        "    s_dz = np.maximum(s_dz, S_DZ_FLOOR)\n",
        "\n",
        "    d_before = (dz_before - m_dz) / s_dz\n",
        "    d_after = (dz_after - m_dz) / s_dz\n",
        "\n",
        "    # --- Winsorized diffs (replaces fixed clip on diffs)\n",
        "    dc_before, dc_after = winsorize_pair(d_before, d_after)\n",
        "\n",
        "    # --- Detrend of winsorized diffs\n",
        "    dm_before = fast_detrend_ols(dc_before)\n",
        "    dm_after = fast_detrend_ols(dc_after)\n",
        "\n",
        "    # --- Absolute diffs (from normalized diffs)\n",
        "    a_before = np.abs(d_before)\n",
        "    a_after = np.abs(d_after)\n",
        "\n",
        "    # Winsorize absolute diffs by before’s absolute-diff quantiles\n",
        "    ac_before, ac_after = winsorize_pair(a_before, a_after)\n",
        "\n",
        "    # Detrend of winsorized absolute diffs\n",
        "    am_before = fast_detrend_ols(ac_before)\n",
        "    am_after = fast_detrend_ols(ac_after)\n",
        "\n",
        "    # --- Absolute values of z (pre-winsor)\n",
        "    absz_before = np.abs(z_before)\n",
        "    absz_after = np.abs(z_after)\n",
        "\n",
        "    m_abs = np.nanmedian(absz_before, axis=1, keepdims=True)\n",
        "    s_abs = 1.4826 * mad(absz_before)\n",
        "    s_abs = np.maximum(s_abs, S_ABS_FLOOR)\n",
        "    abs_std_before = (absz_before - m_abs) / s_abs\n",
        "    abs_std_after = (absz_after - m_abs) / s_abs\n",
        "\n",
        "    # Winsorize normalized |z|\n",
        "    abs_c_before, abs_c_after = winsorize_pair(abs_std_before, abs_std_after)\n",
        "\n",
        "    # Detrend of winsorized |z| normalized\n",
        "    abs_m_before = fast_detrend_ols(abs_c_before)\n",
        "    abs_m_after = fast_detrend_ols(abs_c_after)\n",
        "\n",
        "    # Build z^2, winsorize using BEFORE quantiles, then detrend per segment\n",
        "    sq_before = np.square(z_before)\n",
        "    sq_after = np.square(z_after)\n",
        "\n",
        "    sqc_before, sqc_after = winsorize_pair(sq_before, sq_after)\n",
        "    sqm_before = fast_detrend_ols(sqc_before)\n",
        "    sqm_after = fast_detrend_ols(sqc_after)\n",
        "\n",
        "    # --- Second differences (curvature) on standardized z, per segment (no cross-period leakage)\n",
        "    dd_raw_before = np.diff(dz_before, axis=1, prepend=dz_before[:, [0]])\n",
        "    dd_raw_after = np.diff(dz_after, axis=1, prepend=dz_after[:, [0]])\n",
        "\n",
        "    # Robust standardization by BEFORE stats (median/MAD), reuse the diff floor to be conservative\n",
        "    m_dd = np.nanmedian(dd_raw_before, axis=1, keepdims=True)\n",
        "    s_dd = 1.4826 * mad(dd_raw_before)\n",
        "    s_dd = np.maximum(s_dd, S_DD_FLOOR)\n",
        "\n",
        "    dd_before = (dd_raw_before - m_dd) / s_dd\n",
        "    dd_after = (dd_raw_after - m_dd) / s_dd\n",
        "\n",
        "    # Winsorize second differences using BEFORE-based cutoffs\n",
        "    ddc_before, ddc_after = winsorize_pair(dd_before, dd_after)\n",
        "\n",
        "    # Detrend per segment (OLS) on winsorized second differences\n",
        "    ddm_before = fast_detrend_ols(ddc_before)\n",
        "    ddm_after = fast_detrend_ols(ddc_after)\n",
        "\n",
        "    # --- Stitch back into MultiIndex DataFrame\n",
        "    out_list = []\n",
        "    for i, id_val in enumerate(ids):\n",
        "        Lb = np.count_nonzero(~np.isnan(before_arr[i]))\n",
        "        La = np.count_nonzero(~np.isnan(after_arr[i]))\n",
        "        n_total = Lb + La\n",
        "        time_index = np.arange(n_total)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"original\": np.r_[before_arr[i, :Lb], after_arr[i, :La]],\n",
        "                \"period\": np.r_[np.zeros(Lb, dtype=int), np.ones(La, dtype=int)],\n",
        "                \"standardized\": np.r_[z_before[i, :Lb], z_after[i, :La]],\n",
        "                \"clipped\": np.r_[zc_before[i, :Lb], zc_after[i, :La]],\n",
        "                \"detrended\": np.r_[zd_before[i, :Lb], zd_after[i, :La]],\n",
        "                \"diff_standardized\": np.r_[d_before[i, :Lb], d_after[i, :La]],\n",
        "                \"diff_detrended\": np.r_[dm_before[i, :Lb], dm_after[i, :La]],\n",
        "                \"absdiff_detrended\": np.r_[am_before[i, :Lb], am_after[i, :La]],\n",
        "                \"absval_detrended\": np.r_[abs_m_before[i, :Lb], abs_m_after[i, :La]],\n",
        "                \"squared_detrended\": np.r_[sqm_before[i, :Lb], sqm_after[i, :La]],\n",
        "                \"diff2_standardized\": np.r_[dd_before[i, :Lb], dd_after[i, :La]],\n",
        "                \"diff2_detrended\": np.r_[ddm_before[i, :Lb], ddm_after[i, :La]],\n",
        "            },\n",
        "            index=pd.MultiIndex.from_product(\n",
        "                [[id_val], time_index], names=[\"id\", \"time\"]\n",
        "            ),\n",
        "        )\n",
        "        out_list.append(df)\n",
        "\n",
        "    return pd.concat(out_list, axis=0)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# File-handling wrapper\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "def _latest_cache(prefix: str):\n",
        "    files = sorted(\n",
        "        FEAT_CACHE_DIR.glob(f\"{prefix}_*.parquet\"),\n",
        "        key=lambda f: f.stat().st_mtime,\n",
        "        reverse=True,\n",
        "    )\n",
        "    latest = files[0] if files else None\n",
        "    return latest\n",
        "\n",
        "\n",
        "def _save_cache(df: pd.DataFrame, prefix: str) -> Path:\n",
        "    ts = datetime.now().strftime(\"%m%d_%H%M\")\n",
        "    path = FEAT_CACHE_DIR / f\"{prefix}_{ts}.parquet\"\n",
        "    df.to_parquet(path)\n",
        "    return path\n",
        "\n",
        "\n",
        "def detect_non_finite(feats: pd.DataFrame):\n",
        "    arr = feats.to_numpy(dtype=np.float32, copy=False)\n",
        "    mask = ~np.isfinite(arr)\n",
        "    if mask.any():\n",
        "        r, c = np.where(mask)\n",
        "        for i in range(min(5, len(r))):\n",
        "            print(\n",
        "                f\"  at row={feats.index[r[i]]}, col={feats.columns[c[i]]}, val={arr[r[i], c[i]]}\"\n",
        "            )\n",
        "    return\n",
        "\n",
        "\n",
        "def build_preprocessed(X_train, force=False, inference=False):\n",
        "    \"\"\"Check cache, run process_series if needed, and save.\"\"\"\n",
        "    prefix = \"preprocess\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    out = process_series(X_train)\n",
        "\n",
        "    # Sanity check\n",
        "    detect_non_finite(out)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "def save_essentials():\n",
        "    \"\"\"Create a lightweight preprocessed cache to study it in a notebook\n",
        "    because the original 1GB preprocess file cannot be handled by my computer.\"\"\"\n",
        "    path = _latest_cache(\"preprocess\")\n",
        "    cols = [\"original\", \"period\", \"standardized\", \"clipped\", \"detrended\"]\n",
        "    X_prep = pd.read_parquet(path)[cols]\n",
        "    _save_cache(X_prep, \"preprocess_essentials\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ7fgr_lkX7b"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAVwCTHukX7b"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# MOMENTS BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _group_series(\n",
        "    df: pd.DataFrame, col: str, period: int\n",
        ") -> pd.core.groupby.SeriesGroupBy:\n",
        "    return df.loc[df[\"period\"] == period, col].groupby(level=\"id\")\n",
        "\n",
        "\n",
        "def _log(x):\n",
        "    return np.log(np.maximum(x, EPS))\n",
        "\n",
        "\n",
        "def _std(s):\n",
        "    return np.sqrt(np.nanmean((s - np.nanmean(s)) ** 2))\n",
        "\n",
        "\n",
        "def _skew_kurt(s):\n",
        "    # classic (population) skew and excess kurtosis; NaN-safe\n",
        "    m = np.nanmean(s)\n",
        "    v = np.nanmean((s - m) ** 2)\n",
        "    std = np.sqrt(v)\n",
        "    z = (s - m) / std\n",
        "    skew = np.nanmean(z**3)\n",
        "    kurt = np.nanmean(z**4) - 3.0\n",
        "    return float(skew), float(kurt)\n",
        "\n",
        "\n",
        "def _mad(s):\n",
        "    med = np.nanmedian(s)\n",
        "    return np.nanmedian(np.abs(s - med))\n",
        "\n",
        "\n",
        "def _ols_slope(y):\n",
        "    # slope of y vs t (0..n-1), NaN-safe\n",
        "    n = len(y)\n",
        "    t = np.arange(n, dtype=np.float32)\n",
        "    t_bar = t.mean()\n",
        "    y_bar = y.mean()\n",
        "    num = np.sum((t - t_bar) * (y - y_bar))\n",
        "    den = np.sum((t - t_bar) ** 2)\n",
        "    if den <= EPS:\n",
        "        return 0.0\n",
        "    return float(num / den)\n",
        "\n",
        "\n",
        "def _topk_mean(arr, k=TOP_K):\n",
        "    if arr.size == 0:\n",
        "        return np.nan\n",
        "    k = min(k, arr.size)\n",
        "    # np.partition is O(n); take largest k, then mean\n",
        "    part = np.partition(arr, -k)[-k:]\n",
        "    return float(np.nanmean(part))\n",
        "\n",
        "\n",
        "def _bottomk_mean(arr, k=TOP_K):\n",
        "    k = min(k, arr.size)\n",
        "    part = np.partition(arr, k - 1)[:k]\n",
        "    return float(np.nanmean(part))\n",
        "\n",
        "\n",
        "def compute_moments_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    quantile_coarse_grid: list[float] = QUANTILE_COARSE_GRID,\n",
        ") -> pd.DataFrame:\n",
        "    # Load cache\n",
        "    prefix = \"moments\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # Group series\n",
        "    z_col = \"standardized\"\n",
        "    clip_col = \"clipped\"\n",
        "\n",
        "    z_b = _group_series(X_prep, z_col, 0)\n",
        "    z_a = _group_series(X_prep, z_col, 1)\n",
        "    zc_b = _group_series(X_prep, clip_col, 0)\n",
        "    zc_a = _group_series(X_prep, clip_col, 1)\n",
        "    ids = z_b.size().index\n",
        "\n",
        "    # FEATURE BUILDING BLOCKS\n",
        "\n",
        "    # Base moments\n",
        "    mean_b = z_b.mean()\n",
        "    mean_a = z_a.mean()\n",
        "    std_b = z_b.apply(_std)\n",
        "    std_a = z_a.apply(_std)\n",
        "    skew_b = z_b.apply(lambda s: _skew_kurt(s.values)[0])\n",
        "    skew_a = z_a.apply(lambda s: _skew_kurt(s.values)[0])\n",
        "    kurt_b = z_b.apply(lambda s: _skew_kurt(s.values)[1])\n",
        "    kurt_a = z_a.apply(lambda s: _skew_kurt(s.values)[1])\n",
        "\n",
        "    # Medians & MADs\n",
        "    med_b = z_b.median()\n",
        "    med_a = z_a.median()\n",
        "    mad_b = z_b.apply(lambda s: _mad(s.values))\n",
        "    mad_a = z_a.apply(lambda s: _mad(s.values))\n",
        "\n",
        "    # Base quantiles\n",
        "    qs = quantile_coarse_grid\n",
        "    Q10_b, Q25_b, Q50_b, Q75_b, Q90_b = [z_b.quantile(q) for q in qs]\n",
        "    Q10_a, Q25_a, Q50_a, Q75_a, Q90_a = [z_a.quantile(q) for q in qs]\n",
        "\n",
        "    # Robust skew via central asymmetry\n",
        "    rob_skew_b = _log((Q75_b - Q50_b) / (Q50_b - Q25_b + EPS))\n",
        "    rob_skew_a = _log((Q75_a - Q50_a) / (Q50_a - Q25_a + EPS))\n",
        "\n",
        "    # Robust kurtosis pieces: IQR, IDR\n",
        "    iqr_b = Q75_b - Q25_b\n",
        "    iqr_a = Q75_a - Q25_a\n",
        "    idr_b = Q90_b - Q10_b\n",
        "    idr_a = Q90_a - Q10_a\n",
        "\n",
        "    # Trend (slope) on clipped/winsorized standardized series\n",
        "    slope_b = zc_b.apply(lambda s: _ols_slope(s.values))\n",
        "    slope_a = zc_a.apply(lambda s: _ols_slope(s.values))\n",
        "\n",
        "    # FEATURE COMPUTATION\n",
        "\n",
        "    # Robust location shift, Δmedian/MAD (on original) = median_after (on standardized)\n",
        "    med_delta = med_a - med_b  # med_b should be 0\n",
        "\n",
        "    # Robust scale shift\n",
        "    mad_logratio = _log((mad_a + EPS) / (mad_b + EPS))\n",
        "\n",
        "    # Classic-vs-robust contrasts (per segment)\n",
        "    mean_vs_med = mean_a - med_a - (mean_b - med_b)\n",
        "    std_vs_mad = _log(std_a / (1.4826 * mad_a + EPS)) - _log(\n",
        "        std_b / (1.4826 * mad_b + EPS)\n",
        "    )\n",
        "    skew_contrast = skew_a - rob_skew_a - skew_b - rob_skew_b\n",
        "    kurt_contrast = (\n",
        "        kurt_a - _log(idr_a / (iqr_a + EPS)) - (kurt_b - _log(idr_b / (iqr_b + EPS)))\n",
        "    )\n",
        "\n",
        "    # Slope\n",
        "    slope_delta = slope_a - slope_b\n",
        "\n",
        "    # Assemble\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            # robust moments\n",
        "            \"med_delta\": med_delta,\n",
        "            \"mad_logratio\": mad_logratio,\n",
        "            # contrasts\n",
        "            \"mean_vs_med\": mean_vs_med,\n",
        "            \"std_vs_mad\": std_vs_mad,\n",
        "            \"skew_contrast\": skew_contrast,\n",
        "            \"kurt_contrast\": kurt_contrast,\n",
        "            # slope\n",
        "            \"slope_delta\": slope_delta,\n",
        "        },\n",
        "        index=ids,\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "    if not inference:\n",
        "        _save_cache(df, prefix)\n",
        "    return df\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# QUANTILES BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def compute_quantiles_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    quantile_fine_grid: list[float] = QUANTILE_FINE_GRID,\n",
        "    top_k: int = TOP_K,\n",
        ") -> pd.DataFrame:\n",
        "    # Load cache\n",
        "    prefix = \"quantiles\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # Group series\n",
        "    z_col = \"standardized\"\n",
        "    z_b = _group_series(X_prep, z_col, 0)\n",
        "    z_a = _group_series(X_prep, z_col, 1)\n",
        "    ids = z_b.size().index\n",
        "\n",
        "    # FEATURE BUILDING BLOCKS\n",
        "\n",
        "    # Base quantiles\n",
        "    qs = quantile_fine_grid\n",
        "    qb = z_b.quantile(q=qs).unstack(level=-1)\n",
        "    qa = z_a.quantile(q=qs).unstack(level=-1)\n",
        "    Q5_b, Q10_b, Q25_b, Q40_b, Q50_b, Q60_b, Q75_b, Q90_b, Q95_b = [qb[q] for q in qs]\n",
        "    Q5_a, Q10_a, Q25_a, Q40_a, Q50_a, Q60_a, Q75_a, Q90_a, Q95_a = [qa[q] for q in qs]\n",
        "\n",
        "    IQR_b = Q75_b - Q25_b\n",
        "    IQR_a = Q75_a - Q25_a\n",
        "    IDR_b = Q90_b - Q10_b\n",
        "    IDR_a = Q90_a - Q10_a\n",
        "\n",
        "    IDR_logratio = _log((IDR_a + EPS) / (IDR_b + EPS))\n",
        "\n",
        "    # Tail asymmetry (robust skew)\n",
        "    central_asym_b = _log((Q75_b - Q50_b) / (Q50_b - Q25_b + EPS))\n",
        "    central_asym_a = _log((Q75_a - Q50_a) / (Q50_a - Q25_a + EPS))\n",
        "\n",
        "    shoulder_asym_b = _log((Q90_b - Q75_b) / (Q25_b - Q10_b + EPS))\n",
        "    shoulder_asym_a = _log((Q90_a - Q75_a) / (Q25_a - Q10_a + EPS))\n",
        "\n",
        "    tail_asym_b = _log((Q95_b - Q90_b) / (Q10_b - Q5_b + EPS))\n",
        "    tail_asym_a = _log((Q95_a - Q90_a) / (Q10_a - Q5_a + EPS))\n",
        "\n",
        "    # Tail thickness / peakedness (robust kurtosis)\n",
        "    central_weight_b = _log(IQR_b / (Q60_b - Q40_b + EPS))\n",
        "    central_weight_a = _log(IQR_a / (Q60_a - Q40_a + EPS))\n",
        "\n",
        "    shoulder_weight_b = _log(IDR_b / (IQR_b + EPS))\n",
        "    shoulder_weight_a = _log(IDR_a / (IQR_a + EPS))\n",
        "\n",
        "    tail_weight_b = _log((Q95_b - Q5_b) / (IDR_b + EPS))\n",
        "    tail_weight_a = _log((Q95_a - Q5_a) / (IDR_a + EPS))\n",
        "\n",
        "    # Moors-like variant: ((Q90 - Q60) - (Q40 - Q10)) / (Q75 - Q25)\n",
        "    moors_b = ((Q90_b - Q60_b) - (Q40_b - Q10_b)) / (IQR_b + EPS)\n",
        "    moors_a = ((Q90_a - Q60_a) - (Q40_a - Q10_a)) / (IQR_a + EPS)\n",
        "\n",
        "    # Tail decay rates\n",
        "    central_decay_b = _log((Q75_b - Q60_b) / (Q60_b - Q50_b + EPS))\n",
        "    central_decay_a = _log((Q75_a - Q60_a) / (Q60_a - Q50_a + EPS))\n",
        "\n",
        "    shoulder_decay_b = _log((Q90_b - Q75_b) / (Q75_b - Q60_b + EPS))\n",
        "    shoulder_decay_a = _log((Q90_a - Q75_a) / (Q75_a - Q60_a + EPS))\n",
        "\n",
        "    tail_decay_b = _log((Q95_b - Q90_b) / (Q90_b - Q75_b + EPS))\n",
        "    tail_decay_a = _log((Q95_a - Q90_a) / (Q90_a - Q75_a + EPS))\n",
        "\n",
        "    # Tail extremes (top/bottom k means) relative to IDR\n",
        "    topk_b = z_b.apply(lambda s: _topk_mean(s.values, top_k))\n",
        "    topk_a = z_a.apply(lambda s: _topk_mean(s.values, top_k))\n",
        "    botk_b = z_b.apply(lambda s: _bottomk_mean(s.values, top_k))\n",
        "    botk_a = z_a.apply(lambda s: _bottomk_mean(s.values, top_k))\n",
        "\n",
        "    upper_ext_b = _log(np.maximum(topk_b, EPS) / (IDR_b + EPS))\n",
        "    upper_ext_a = _log(np.maximum(topk_a, EPS) / (IDR_a + EPS))\n",
        "\n",
        "    # For lower extension we expect botk to be negative; use -mean_bottom_k\n",
        "    lower_ext_b = _log(np.maximum(-botk_b, EPS) / (IDR_b + EPS))\n",
        "    lower_ext_a = _log(np.maximum(-botk_a, EPS) / (IDR_a + EPS))\n",
        "\n",
        "    # Extreme tail asymmetry: log(mean_top_k / (-mean_bottom_k))\n",
        "    # Guard denominator sign; if mean_bottom_k >= 0, push to EPS to avoid invalid log.\n",
        "    eta_b = _log(np.maximum(topk_b, EPS) / np.maximum(-botk_b, EPS))\n",
        "    eta_a = _log(np.maximum(topk_a, EPS) / np.maximum(-botk_a, EPS))\n",
        "\n",
        "    # FEATURE COMPUTATION\n",
        "\n",
        "    # Base quantiles\n",
        "    Q5_delta = Q5_a - Q5_b\n",
        "    Q10_delta = Q10_a - Q10_b\n",
        "    Q25_delta = Q25_a - Q25_b\n",
        "    Q40_delta = Q40_a - Q40_b\n",
        "    Q60_delta = Q60_a - Q60_b\n",
        "    Q75_delta = Q75_a - Q75_b\n",
        "    Q90_delta = Q90_a - Q90_b\n",
        "    Q95_delta = Q95_a - Q95_b\n",
        "\n",
        "    # Tail asymmetry (robust skew)\n",
        "    central_asym = central_asym_a - central_asym_b\n",
        "    shoulder_asym = shoulder_asym_a - shoulder_asym_b\n",
        "    tail_asym = tail_asym_a - tail_asym_b\n",
        "\n",
        "    # Tail thickness / peakedness (robust kurtosis)\n",
        "    central_weight = central_weight_a - central_weight_b\n",
        "    shoulder_weight = shoulder_weight_a - shoulder_weight_b\n",
        "    tail_weight = tail_weight_a - tail_weight_b\n",
        "    moors = moors_a - moors_b\n",
        "\n",
        "    # Tail decay rates\n",
        "    central_decay = central_decay_a - central_decay_b\n",
        "    shoulder_decay = shoulder_decay_a - shoulder_decay_b\n",
        "    tail_decay = tail_decay_a - tail_decay_b\n",
        "\n",
        "    # Tail extremes\n",
        "    upper_ext = upper_ext_a - upper_ext_b\n",
        "    lower_ext = lower_ext_a - lower_ext_b\n",
        "    extreme_tail_asym = eta_a - eta_b\n",
        "\n",
        "    # Assemble\n",
        "    q_df = pd.DataFrame(\n",
        "        {\n",
        "            # base quantiles\n",
        "            \"Q5_delta\": Q5_delta,\n",
        "            \"Q10_delta\": Q10_delta,\n",
        "            \"Q25_delta\": Q25_delta,\n",
        "            \"Q40_delta\": Q40_delta,\n",
        "            \"Q60_delta\": Q60_delta,\n",
        "            \"Q75_delta\": Q75_delta,\n",
        "            \"Q90_delta\": Q90_delta,\n",
        "            \"Q95_delta\": Q95_delta,\n",
        "            \"IDR_logratio\": IDR_logratio,\n",
        "            # tail asym\n",
        "            \"central_asym\": central_asym,\n",
        "            \"shoulder_asym\": shoulder_asym,\n",
        "            \"tail_asym\": tail_asym,\n",
        "            # tail thickness\n",
        "            \"central_weight\": central_weight,\n",
        "            \"shoulder_weight\": shoulder_weight,\n",
        "            \"tail_weight\": tail_weight,\n",
        "            \"moors\": moors,\n",
        "            # decay rates\n",
        "            \"central_decay\": central_decay,\n",
        "            \"shoulder_decay\": shoulder_decay,\n",
        "            \"tail_decay\": tail_decay,\n",
        "            # extremes\n",
        "            \"upper_ext\": upper_ext,\n",
        "            \"lower_ext\": lower_ext,\n",
        "            \"extreme_tail_asym\": extreme_tail_asym,\n",
        "        },\n",
        "        index=ids,\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "    if not inference:\n",
        "        _save_cache(q_df, prefix)\n",
        "    return q_df\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# RATES BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def jeffreys_logit(k: int, m: int) -> float:\n",
        "    \"\"\"Logit of Jeffreys-smoothed rate: p~ = (k+0.5)/(m+1).\"\"\"\n",
        "    if m <= 0:\n",
        "        raise ValueError\n",
        "    p = (k + 0.5) / (m + 1.0)\n",
        "    p = np.clip(p, EPS, 1 - EPS)\n",
        "    return _log(p / (1.0 - p))\n",
        "\n",
        "\n",
        "def exceedance_logits(\n",
        "    z: np.ndarray, k_abs: float, k_fixed: float\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"P(|z|>k_abs) and P(|z|>k_fixed) → Jeffreys logits.\"\"\"\n",
        "    m = z.size\n",
        "    k1 = int(np.sum(np.abs(z) > k_abs))\n",
        "    k2 = int(np.sum(np.abs(z) > k_fixed))\n",
        "    return jeffreys_logit(k1, m), jeffreys_logit(k2, m)\n",
        "\n",
        "\n",
        "def upper_lower_logits(\n",
        "    z: np.ndarray, q_low: float, q_high: float\n",
        ") -> tuple[float, float]:\n",
        "    \"\"\"P(z > q_high), P(z < q_low) → Jeffreys logits.\"\"\"\n",
        "    m = z.size\n",
        "    ku = int(np.sum(z > q_high))\n",
        "    kl = int(np.sum(z < q_low))\n",
        "    return jeffreys_logit(ku, m), jeffreys_logit(kl, m)\n",
        "\n",
        "\n",
        "def fano_burstiness(z: np.ndarray, thr_abs: float, w: int) -> float:\n",
        "    \"\"\"Fano factor of windowed counts of |z|>thr_abs over fixed window size w.\"\"\"\n",
        "    n = z.size\n",
        "    n_w = (n // w) * w\n",
        "    if n_w == 0:\n",
        "        return 0.0\n",
        "    blocks = z[:n_w].reshape(-1, w)\n",
        "    hits = np.sum(np.abs(blocks) > thr_abs, axis=1).astype(np.float32)\n",
        "    mu = hits.mean()\n",
        "    if mu <= 0:\n",
        "        return 0.0\n",
        "    var = hits.var(ddof=0)\n",
        "    return float(var / (mu + EPS))\n",
        "\n",
        "\n",
        "def crossing_rates_logits(\n",
        "    z: np.ndarray, Q: float, eps: float\n",
        ") -> tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Up/Down/Total crossing rates (per transition), with deadband:\n",
        "        up:   z_t <= Q - eps and z_{t+1} >= Q + eps\n",
        "        down: z_t >= Q + eps and z_{t+1} <= Q - eps\n",
        "    Returns Jeffreys-smoothed logits for up, down, total.\n",
        "    \"\"\"\n",
        "    a = z[:-1]\n",
        "    b = z[1:]\n",
        "    m = a.size\n",
        "    up = np.sum((a <= (Q - eps)) & (b >= (Q + eps)))\n",
        "    dn = np.sum((a >= (Q + eps)) & (b <= (Q - eps)))\n",
        "    tot = int(up + dn)\n",
        "    return (\n",
        "        jeffreys_logit(int(up), m),\n",
        "        jeffreys_logit(int(dn), m),\n",
        "        jeffreys_logit(tot, m),\n",
        "    )\n",
        "\n",
        "\n",
        "def median_crossing_asym(z: np.ndarray, Q: float, eps: float) -> float:\n",
        "    \"\"\"(UP_CR50 - DOWN_CR50) / (UP_CR50 + DOWN_CR50 + 1), using raw (unsmoothed) rates.\"\"\"\n",
        "    a = z[:-1]\n",
        "    b = z[1:]\n",
        "    up = np.sum((a <= (Q - eps)) & (b >= (Q + eps)))\n",
        "    dn = np.sum((a >= (Q + eps)) & (b <= (Q - eps)))\n",
        "    denom = up + dn\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float((up - dn) / (denom + 1))  # +1 prevents +1 or -1 for actanh\n",
        "\n",
        "\n",
        "def mean_log_res_time(z: np.ndarray, Q: float, eps: float) -> float:\n",
        "    \"\"\"\n",
        "    Average time between *median* crossings (using deadband).\n",
        "    Compute distances between consecutive (up or down) events; log of mean.\n",
        "    \"\"\"\n",
        "    a, b = z[:-1], z[1:]\n",
        "    m = a.size\n",
        "    cross_idx = np.where(\n",
        "        ((a <= Q - eps) & (b >= Q + eps)) | ((a >= Q + eps) & (b <= Q - eps))\n",
        "    )[0]\n",
        "    tot = cross_idx.size\n",
        "    if tot == 0:\n",
        "        return 0.0\n",
        "    if tot == 1:\n",
        "        # approx mean gap ≈ (m)/(tot+1)\n",
        "        return _log(m / (tot + 1.0) + EPS)\n",
        "    gaps = np.diff(cross_idx)\n",
        "    return _log(np.mean(gaps) + EPS)\n",
        "\n",
        "\n",
        "def fisher_delta(a: float, b: float, eps: float = EPS) -> float:\n",
        "    \"\"\"atanh(a) - atanh(b) with safe clamping for inputs in [-1,1].\"\"\"\n",
        "    lo, hi = -1 + eps, 1 - eps\n",
        "    return float(np.arctanh(np.clip(a, lo, hi)) - np.arctanh(np.clip(b, lo, hi)))\n",
        "\n",
        "\n",
        "def compute_rates_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    quantile_fine_grid: list[float] = QUANTILE_FINE_GRID,\n",
        "    w_fano: int = W_FANO,\n",
        "    crossing_rate_deadband: float = CROSSING_RATE_DEADBAND,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Rates block (after − before or log-ratios), computed per id via a single groupby-apply.\n",
        "    Uses BEFORE quantiles as thresholds for both segments.\n",
        "    \"\"\"\n",
        "    prefix = \"rates\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # keep only what we need\n",
        "    cols = [\"standardized\", \"original\", \"period\"]\n",
        "    df = X_prep[cols].copy()\n",
        "\n",
        "    def _one_id(g: pd.DataFrame) -> pd.Series:\n",
        "        # split\n",
        "        gb = g[g[\"period\"] == 0]\n",
        "        ga = g[g[\"period\"] == 1]\n",
        "\n",
        "        zb = gb[\"standardized\"].to_numpy(np.float32, copy=False)\n",
        "        za = ga[\"standardized\"].to_numpy(np.float32, copy=False)\n",
        "        xb = gb[\"original\"].to_numpy(np.float32, copy=False)\n",
        "        xa = ga[\"original\"].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # BEFORE thresholds\n",
        "        qs = quantile_fine_grid\n",
        "        Q5_b, _, Q25_b, _, Q50_b, _, Q75_b, _, Q95_b = np.quantile(zb, qs)\n",
        "\n",
        "        # Extreme events (Jeffreys-smoothed logits)\n",
        "        b1, b2 = exceedance_logits(zb, k_abs=Q95_b, k_fixed=3.0)\n",
        "        a1, a2 = exceedance_logits(za, k_abs=Q95_b, k_fixed=3.0)\n",
        "\n",
        "        u_b, l_b = upper_lower_logits(zb, q_low=Q5_b, q_high=Q95_b)\n",
        "        u_a, l_a = upper_lower_logits(za, q_low=Q5_b, q_high=Q95_b)\n",
        "\n",
        "        # Burstiness: Fano on |z|>Q95_before, then log-ratio\n",
        "        fb = fano_burstiness(zb, thr_abs=Q95_b, w=w_fano)\n",
        "        fa = fano_burstiness(za, thr_abs=Q95_b, w=w_fano)\n",
        "        fano_logratio = _log((fa + EPS) / (fb + EPS))\n",
        "\n",
        "        # 2) Crossing rates (deadband ε), keep totals at Q25/Q50/Q75\n",
        "        _, _, cr25_b_t = crossing_rates_logits(zb, Q25_b, crossing_rate_deadband)\n",
        "        _, _, cr25_a_t = crossing_rates_logits(za, Q25_b, crossing_rate_deadband)\n",
        "\n",
        "        _, _, cr50_b_t = crossing_rates_logits(zb, Q50_b, crossing_rate_deadband)\n",
        "        _, _, cr50_a_t = crossing_rates_logits(za, Q50_b, crossing_rate_deadband)\n",
        "\n",
        "        _, _, cr75_b_t = crossing_rates_logits(zb, Q75_b, crossing_rate_deadband)\n",
        "        _, _, cr75_a_t = crossing_rates_logits(za, Q75_b, crossing_rate_deadband)\n",
        "\n",
        "        # Median crossing asymmetry (Fisher-z delta) & log mean residence time\n",
        "        med_asym_b = median_crossing_asym(zb, Q50_b, crossing_rate_deadband)\n",
        "        med_asym_a = median_crossing_asym(za, Q50_b, crossing_rate_deadband)\n",
        "        med_cross_asym = fisher_delta(med_asym_a, med_asym_b)\n",
        "\n",
        "        # CR decay\n",
        "        cr_decay = (cr75_a_t - cr25_a_t) - (cr75_b_t - cr25_b_t)\n",
        "\n",
        "        # 3) % zeros (original) → Jeffreys-logit delta\n",
        "        kb, mb = int(np.isclose(xb, 0.0).sum()), xb.size\n",
        "        ka, ma = int(np.isclose(xa, 0.0).sum()), xa.size\n",
        "        pct_zeros = jeffreys_logit(ka, ma) - jeffreys_logit(kb, mb)\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"abs_excd_q95_logit_delta\": a1 - b1,\n",
        "                \"abs_excd_3_logit_delta\": a2 - b2,\n",
        "                \"up_excd_q95_logit_delta\": u_a - u_b,\n",
        "                \"low_excd_q5_logit_delta\": l_a - l_b,\n",
        "                \"fano_logratio\": fano_logratio,\n",
        "                \"signflips_logit_delta\": cr50_a_t - cr50_b_t,\n",
        "                \"CR25_logit_delta\": cr25_a_t - cr25_b_t,\n",
        "                \"CR75_logit_delta\": cr75_a_t - cr75_b_t,\n",
        "                \"med_cross_asym_fisher_delta\": med_cross_asym,\n",
        "                \"cr_decay_logit_delta\": cr_decay,\n",
        "                \"pct_zeros_logit_delta\": pct_zeros,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    out = (\n",
        "        df.groupby(level=\"id\", sort=False, group_keys=False)\n",
        "        .apply(_one_id)\n",
        "        .astype(np.float32)\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# AUTOCORRELATION BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def acf_1d(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
        "    \"\"\"Sample ACF r[1..max_lag]; x is 1D float32.\"\"\"\n",
        "    n = x.size\n",
        "    # mean-center (detrended is near zero-mean, but do it anyway)\n",
        "    x = x - x.mean()\n",
        "    var = x.var()\n",
        "    if var <= 0:\n",
        "        return np.zeros(max_lag, dtype=np.float32)\n",
        "    r = np.empty(max_lag, dtype=np.float32)\n",
        "    # O(nK) naive is fine for K<=20\n",
        "    for k in range(1, max_lag + 1):\n",
        "        num = np.dot(x[k:], x[:-k])\n",
        "        r[k - 1] = num / ((n - k) * var)  # consistent scaling across lags\n",
        "    return r\n",
        "\n",
        "\n",
        "def pacf_yw(x: np.ndarray, max_lag: int) -> np.ndarray:\n",
        "    \"\"\"PACF via Yule–Walker / Durbin–Levinson; returns pacf[1..max_lag].\"\"\"\n",
        "    r = acf_1d(x, max_lag)  # r[1..K]\n",
        "    r0 = 1.0\n",
        "    # build autocorr sequence r_full[0..K]\n",
        "    r_full = np.concatenate(([r0], r))\n",
        "    pacf = np.zeros(max_lag, dtype=np.float32)\n",
        "    # Durbin–Levinson\n",
        "    phi = np.zeros((max_lag + 1, max_lag + 1), dtype=np.float32)\n",
        "    sig = np.empty(max_lag + 1, dtype=np.float32)\n",
        "    phi[1, 1] = r_full[1]\n",
        "    sig[1] = 1 - r_full[1] ** 2\n",
        "    pacf[0] = phi[1, 1]\n",
        "    for k in range(2, max_lag + 1):\n",
        "        num = r_full[k] - np.dot(phi[1:k, k - 1], r_full[1:k][::-1])\n",
        "        den = sig[k - 1] if sig[k - 1] > 0 else EPS\n",
        "        phi[k, k] = num / den\n",
        "        for j in range(1, k):\n",
        "            phi[j, k] = phi[j, k - 1] - phi[k, k] * phi[k - j, k - 1]\n",
        "        sig[k] = sig[k - 1] * (1 - phi[k, k] ** 2)\n",
        "        pacf[k - 1] = phi[k, k]\n",
        "    return pacf\n",
        "\n",
        "\n",
        "def ljung_box_z(x: np.ndarray, m: int) -> float:\n",
        "    \"\"\"Length/m-invariant LBQ z-score using unbiased ACF and common m.\"\"\"\n",
        "    n = x.size\n",
        "    r = acf_1d(x.astype(np.float32), m)  # unbiased ACF (your fixed version)\n",
        "    ks = np.arange(1, m + 1, dtype=np.float32)\n",
        "    Q = n * (n + 2.0) * np.sum((r**2) / (n - ks))\n",
        "    z = (Q - m) / np.sqrt(2.0 * m)\n",
        "    return float(0.0 if not np.isfinite(z) else z)\n",
        "\n",
        "\n",
        "def compute_autocorr_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    acf_max_lag: int = ACF_MAX_LAG,  # ACF/PACF lags 1..acf_max_lag for shape/summaries\n",
        "    lbq_m: int = LBQ_M,  # Ljung–Box Q statistic uses lbq_m\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Autocorrelation features on the *detrended* standardized series.\n",
        "\n",
        "    Outputs (one row per id):\n",
        "      - acf1_delta, acf2_delta                   : ACF lag-1/2 (after − before)\n",
        "      - pacf1_delta, pacf2_delta                 : PACF lag-1/2 (after − before)\n",
        "      - short_lag_acf_dep_delta                  : Σ_{ℓ=1..K} |ρ(ℓ)|/ℓ  (after − before)\n",
        "      - lbq_stat_delta                           : Ljung–Box Q_m (after − before), m = m_lbq\n",
        "      - alt_signed_sum_delta                     : (Σ_{ℓ=1..K} (−1)^{ℓ−1} ρ_a(ℓ)) − same_before\n",
        "\n",
        "    Notes:\n",
        "      • Crashes if a segment contains non-finite values (enforce data hygiene).\n",
        "      • Fast O(nK) per id (K small), no heavy FFT needed.\n",
        "    \"\"\"\n",
        "    # Caching\n",
        "    prefix = \"autocorr\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # Group series\n",
        "    zcol = \"detrended\"\n",
        "    zd_b = _group_series(X_prep, zcol, 0)\n",
        "    zd_a = _group_series(X_prep, zcol, 1)\n",
        "\n",
        "    # Materialize arrays once (NumPy, float32 for stability)\n",
        "    zd_b = zd_b.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    zd_a = zd_a.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    ids = zd_b.index\n",
        "\n",
        "    # ---------- compute per id ----------\n",
        "    acf1_d, acf2_d = [], []\n",
        "    pacf2_d = []\n",
        "    shortlag_d = []\n",
        "    lbq_d = []\n",
        "    alt_sum_d = []\n",
        "\n",
        "    for i in ids:\n",
        "        xb = zd_b.loc[i].astype(np.float32, copy=False)\n",
        "        xa = zd_a.loc[i].astype(np.float32, copy=False)\n",
        "\n",
        "        # ACF/PACF up to K\n",
        "        acf_b = acf_1d(xb, acf_max_lag)  # r[1..acf_max_lag]\n",
        "        acf_a = acf_1d(xa, acf_max_lag)\n",
        "        pacf_b = pacf_yw(xb, acf_max_lag)\n",
        "        pacf_a = pacf_yw(xa, acf_max_lag)\n",
        "\n",
        "        # lag-1/2 deltas\n",
        "        acf1_d.append(acf_a[0] - acf_b[0] if acf_max_lag >= 1 else 0.0)\n",
        "        acf2_d.append(acf_a[1] - acf_b[1] if acf_max_lag >= 2 else 0.0)\n",
        "        pacf2_d.append(pacf_a[1] - pacf_b[1] if acf_max_lag >= 2 else 0.0)\n",
        "\n",
        "        # short-lag dependence Σ |ρ(ℓ)|/ℓ, starting from l=2\n",
        "        weights = 1.0 / np.arange(2, acf_max_lag + 1, dtype=np.float32)\n",
        "        s_b = np.sum(np.abs(acf_b[1:]) * weights)\n",
        "        s_a = np.sum(np.abs(acf_a[1:]) * weights)\n",
        "        shortlag_d.append(s_a - s_b)\n",
        "\n",
        "        # Ljung–Box Q (m = lbq_m)\n",
        "        lbq_d.append(ljung_box_z(xa, lbq_m) - ljung_box_z(xb, lbq_m))\n",
        "\n",
        "        # Alternative signed sum Σ (-1)^{ℓ-1} ρ(ℓ)\n",
        "        signs = np.where((np.arange(1, acf_max_lag + 1) % 2) == 1, 1.0, -1.0)\n",
        "        alt_b = float(np.sum(signs * acf_b))\n",
        "        alt_a = float(np.sum(signs * acf_a))\n",
        "        alt_sum_d.append(alt_a - alt_b)\n",
        "\n",
        "    autoc = pd.DataFrame(\n",
        "        {\n",
        "            \"acf1_delta\": acf1_d,\n",
        "            \"acf2_delta\": acf2_d,\n",
        "            \"pacf2_delta\": pacf2_d,\n",
        "            \"shortlag_l1_delta\": shortlag_d,\n",
        "            \"lbq_stat_delta\": lbq_d,\n",
        "            \"alt_signed_sum_delta\": alt_sum_d,\n",
        "        },\n",
        "        index=ids,\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(autoc, prefix)\n",
        "    return autoc\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# TESTS & DISTANCES BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _idr(x):\n",
        "    q = np.quantile(x, [0.25, 0.75])\n",
        "    return float(q[1] - q[0])\n",
        "\n",
        "\n",
        "def _ks_normalized(before: np.ndarray, after: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Kolmogorov two-sample normalized statistic:\n",
        "      KS_norm = (sqrt(n_eff) + 0.12 + 0.11/sqrt(n_eff)) * D,\n",
        "    where n_eff = n0*n1/(n0+n1) and D is the sup ECDF distance.\n",
        "    Returns 0.0 if either segment is empty.\n",
        "    \"\"\"\n",
        "    n0, n1 = before.size, after.size\n",
        "    if n0 == 0 or n1 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # compute D (two-sample KS) inline (merge-walk)\n",
        "    xa = np.sort(before)\n",
        "    xb = np.sort(after)\n",
        "    ia = ib = 0\n",
        "    cdfa = cdfb = 0.0\n",
        "    D = 0.0\n",
        "    while ia < n0 and ib < n1:\n",
        "        if xa[ia] <= xb[ib]:\n",
        "            ia += 1\n",
        "            cdfa = ia / n0\n",
        "        else:\n",
        "            ib += 1\n",
        "            cdfb = ib / n1\n",
        "        D = max(D, abs(cdfa - cdfb))\n",
        "    if ia < n0:\n",
        "        D = max(D, abs(1.0 - cdfb))\n",
        "    if ib < n1:\n",
        "        D = max(D, abs(cdfa - 1.0))\n",
        "\n",
        "    n_eff = (n0 * n1) / (n0 + n1)\n",
        "    s = np.sqrt(max(n_eff, 1.0))\n",
        "    return float((s + 0.12 + 0.11 / s) * D)\n",
        "\n",
        "\n",
        "def _css_normalized(before: np.ndarray, after: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Inclán–Tiao CUSUM-of-squares with Brownian-bridge scaling:\n",
        "      CSS_norm = sqrt(n) * max_t | S_t / S_T - t/T |,\n",
        "    computed on the concatenated series (before || after).\n",
        "    \"\"\"\n",
        "    if before.size + after.size <= 1:\n",
        "        return 0.0\n",
        "    y = np.concatenate([before, after])\n",
        "    n = y.size\n",
        "    s2 = np.cumsum(y * y)\n",
        "    ST = s2[-1]\n",
        "    if ST <= 0:\n",
        "        return 0.0\n",
        "    t = np.arange(1, n + 1, dtype=np.float32)\n",
        "    D = s2 / ST - t / n\n",
        "    css = float(np.max(np.abs(D)))\n",
        "    return float(np.sqrt(n) * css)\n",
        "\n",
        "\n",
        "def _wasserstein_quant(a, b, qs, scale):\n",
        "    Qa = np.quantile(a, qs)\n",
        "    Qb = np.quantile(b, qs)\n",
        "    w1 = np.mean(np.abs(Qa - Qb))\n",
        "    denom = max(scale, 1e-8)\n",
        "    return float(w1 / denom)\n",
        "\n",
        "\n",
        "def _js_divergence(a, b, q_edges):\n",
        "    # Quantile-based bin edges from BEFORE segment\n",
        "    edges = np.quantile(a, q_edges)\n",
        "    # ensure strictly increasing edges (collapse-safe)\n",
        "    edges = np.unique(edges)\n",
        "    # histograms\n",
        "    pa, _ = np.histogram(a, bins=edges, density=False)\n",
        "    pb, _ = np.histogram(b, bins=edges, density=False)\n",
        "    # Jeffreys smoothing (0.5) → probabilities\n",
        "    pa = pa.astype(np.float32) + 0.5\n",
        "    pb = pb.astype(np.float32) + 0.5\n",
        "    pa /= pa.sum()\n",
        "    pb /= pb.sum()\n",
        "    m = 0.5 * (pa + pb)\n",
        "    # JS in nats\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        KL_am = np.sum(pa * (np.log(pa) - np.log(m)))\n",
        "        KL_bm = np.sum(pb * (np.log(pb) - np.log(m)))\n",
        "    JS = 0.5 * (KL_am + KL_bm)\n",
        "    if not np.isfinite(JS):\n",
        "        return 0.0\n",
        "    return float(JS)\n",
        "\n",
        "\n",
        "def _mmd2_rbf(a, b, sigma, mmd_max_n):\n",
        "    # Unbiased MMD^2; cap sample sizes for speed\n",
        "    na = a.size\n",
        "    nb = b.size\n",
        "    if na > mmd_max_n:\n",
        "        idx = np.linspace(0, na - 1, mmd_max_n, dtype=int)\n",
        "        a = a[idx]\n",
        "        na = a.size\n",
        "    if nb > mmd_max_n:\n",
        "        idx = np.linspace(0, nb - 1, mmd_max_n, dtype=int)\n",
        "        b = b[idx]\n",
        "        nb = b.size\n",
        "    gamma = 1.0 / (2.0 * sigma * sigma)\n",
        "\n",
        "    def _kxx(x):\n",
        "        # exclude diagonal for unbiased estimator\n",
        "        d2 = (x[:, None] - x[None, :]) ** 2\n",
        "        np.fill_diagonal(d2, 0.0)\n",
        "        K = np.exp(-gamma * d2)\n",
        "        return K.sum() / (x.size * (x.size - 1) + EPS)\n",
        "\n",
        "    def _kxy(x, y):\n",
        "        d2 = (x[:, None] - y[None, :]) ** 2\n",
        "        K = np.exp(-gamma * d2)\n",
        "        return K.mean()\n",
        "\n",
        "    kxx = _kxx(a) if na > 1 else 0.0\n",
        "    kyy = _kxx(b) if nb > 1 else 0.0\n",
        "    kxy = _kxy(a, b)\n",
        "    return float(kxx + kyy - 2.0 * kxy)\n",
        "\n",
        "\n",
        "def _gaussian_glr_per(before: np.ndarray, after: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Per-sample Gaussian GLR for joint mean+variance change (length-invariant).\n",
        "    LRT = n*log(s2_pooled) - n0*log(s2_b) - n1*log(s2_a); return LRT / n.\n",
        "    s2_* are MLE variances about their own means (ddof=0).\n",
        "    \"\"\"\n",
        "    n0 = before.size\n",
        "    n1 = after.size\n",
        "    n = n0 + n1\n",
        "    s2_b = float(np.var(before, ddof=0))\n",
        "    s2_a = float(np.var(after, ddof=0))\n",
        "    # guard against degenerate variance\n",
        "    s2_b = max(s2_b, EPS)\n",
        "    s2_a = max(s2_a, EPS)\n",
        "    s2_p = (n0 * s2_b + n1 * s2_a) / n\n",
        "    s2_p = max(s2_p, EPS)\n",
        "    lrt = n * np.log(s2_p) - n0 * np.log(s2_b) - n1 * np.log(s2_a)\n",
        "    return float(lrt / n)\n",
        "\n",
        "\n",
        "def _arch_neglogp(x: np.ndarray, L: int) -> float:\n",
        "    \"\"\"\n",
        "    ARCH-LM: compute LM = n*R^2, then -log p with df=L (p from chi-square upper tail).\n",
        "    Uses SciPy's gammaincc (regularized upper incomplete gamma).\n",
        "    \"\"\"\n",
        "    # You already have _arch_lm_LM(x, L) elsewhere; if not, substitute here.\n",
        "    LM = _arch_lm_LM(x, L=L)\n",
        "    # p = P[Chi2_L >= LM] = gammaincc(L/2, LM/2)\n",
        "    p = float(gammaincc(0.5 * L, 0.5 * max(LM, 0.0)))\n",
        "    return float(-np.log(max(p, EPS)))\n",
        "\n",
        "\n",
        "def compute_tests_distances_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    js_quantile_bins: np.ndarray = JS_QUANTILE_BINS,  # e.g., np.linspace(0,1,33)\n",
        "    mmd_max_n: int = MMD_MAX_N,  # e.g., 512\n",
        "    arch_L: int = ARCH_L,  # e.g., 5\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Length-stable tests & distances between BEFORE and AFTER on standardized z.\n",
        "\n",
        "    Tests (length-invariant forms):\n",
        "      - gauss_glr_per_sample      : per-sample Gaussian GLR (mean+variance)\n",
        "      - css_norm                  : √n * CSS on concatenated series\n",
        "      - archlm_neglogp_delta      : (-log p)_after - (-log p)_before   [ARCH-LM, df=arch_L]\n",
        "\n",
        "    Distances (length-agnostic):\n",
        "      - ks_norm                   : Kolmogorov D with Massey normalization\n",
        "      - js_divergence             : quantile-binned JS with Jeffreys smoothing\n",
        "      - mmd2_rbf_idrband_equal    : unbiased MMD² (RBF), equal-size subsample, σ = IDR_before/2\n",
        "    \"\"\"\n",
        "    prefix = \"tests_distances\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # standardized segments\n",
        "    z_b = _group_series(X_prep, \"standardized\", 0).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    z_a = _group_series(X_prep, \"standardized\", 1).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    ids = z_b.index\n",
        "\n",
        "    # accumulators\n",
        "    glr_per_L, css_norm_L, arch_neglogp_delta_L = [], [], []\n",
        "    ks_norm_L, js_div_L, mmd2_equal_L = [], [], []\n",
        "\n",
        "    for i in ids:\n",
        "        b = z_b.loc[i]\n",
        "        a = z_a.loc[i]\n",
        "\n",
        "        # --- Tests (normalized) ---\n",
        "        glr_per = _gaussian_glr_per(b, a)\n",
        "        css_norm = _css_normalized(b, a)\n",
        "\n",
        "        arch_b = _arch_neglogp(b, arch_L)\n",
        "        arch_a = _arch_neglogp(a, arch_L)\n",
        "        arch_delta = float(arch_a - arch_b)\n",
        "\n",
        "        # --- Distances (no caps) ---\n",
        "        ks_norm = _ks_normalized(b, a)\n",
        "        js = _js_divergence(b, a, js_quantile_bins)\n",
        "\n",
        "        # MMD² with equal-size subsampling (deterministic linspace indices)\n",
        "        m = int(min(b.size, a.size, mmd_max_n))\n",
        "        if m >= 2:\n",
        "            idx_b = np.linspace(0, b.size - 1, m, dtype=int)\n",
        "            idx_a = np.linspace(0, a.size - 1, m, dtype=int)\n",
        "            bb = b[idx_b]\n",
        "            aa = a[idx_a]\n",
        "            sigma = max(_idr(b), EPS) / 2.0\n",
        "            mmd2 = _mmd2_rbf(\n",
        "                bb, aa, sigma, mmd_max_n=m\n",
        "            )  # mmd_max_n=m since already subsampled equally\n",
        "        else:\n",
        "            mmd2 = 0.0\n",
        "\n",
        "        # collect\n",
        "        glr_per_L.append(glr_per)\n",
        "        css_norm_L.append(css_norm)\n",
        "        arch_neglogp_delta_L.append(arch_delta)\n",
        "        ks_norm_L.append(ks_norm)\n",
        "        js_div_L.append(js)\n",
        "        mmd2_equal_L.append(mmd2)\n",
        "\n",
        "    out = pd.DataFrame(\n",
        "        {\n",
        "            \"gauss_glr_per_sample\": glr_per_L,\n",
        "            \"css_norm\": css_norm_L,\n",
        "            \"archlm_neglogp_delta\": arch_neglogp_delta_L,\n",
        "            \"ks_norm\": ks_norm_L,\n",
        "            \"js_divergence\": js_div_L,\n",
        "            \"mmd2_rbf_idrband_equal\": mmd2_equal_L,\n",
        "        },\n",
        "        index=ids,\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# FREQUENCY BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _psd_rfft(x: np.ndarray):\n",
        "    \"\"\"Return (freq in [0,0.5], power spectrum). Mean-centered; no window for speed.\"\"\"\n",
        "    n = x.size\n",
        "    y = x - x.mean()\n",
        "    fft = np.fft.rfft(y)\n",
        "    P = (fft.real**2 + fft.imag**2) / max(n, 1)\n",
        "    f = np.fft.rfftfreq(n, d=1.0)  # normalized to sampling step 1 → Nyquist 0.5\n",
        "    return f, P\n",
        "\n",
        "\n",
        "def _spectral_centroid(f, P):\n",
        "    tot = P.sum()\n",
        "    return float((f * P).sum() / (tot + EPS)) if tot > 0 else 0.0\n",
        "\n",
        "\n",
        "def _log_flatness(P):\n",
        "    \"\"\"log( geometric_mean / arithmetic_mean ).\"\"\"\n",
        "    Pp = P + EPS\n",
        "    return float(np.exp(np.mean(np.log(Pp))) / np.mean(Pp) + 0.0)  # flatness in (0,1]\n",
        "    # we’ll convert to log-domain delta below\n",
        "\n",
        "\n",
        "def _bandpower_logratio(f, P, bands):\n",
        "    tot = P.sum()\n",
        "    if tot <= 0:\n",
        "        return [0.0] * len(bands)\n",
        "    out = []\n",
        "    for lo, hi in bands:\n",
        "        m = (f >= lo) & (f < hi)\n",
        "        frac = P[m].sum() / (tot + EPS)\n",
        "        out.append(frac)\n",
        "    return out  # we’ll take log-ratio a/b later\n",
        "\n",
        "\n",
        "def _dwt_l3_ratio(x, dwt_wavelet, dwt_level):\n",
        "    coeffs = pywt.wavedec(x, dwt_wavelet, level=dwt_level, mode=\"symmetric\")\n",
        "    # coeffs = [cA_L, cD_L, ..., cD_1]\n",
        "    details = coeffs[1:]  # list of arrays cD_L..cD_1\n",
        "    if len(details) < 3:\n",
        "        return 0.0\n",
        "    cD3 = details[-3]  # level-3 detail\n",
        "    e3 = float(np.sum(cD3 * cD3))\n",
        "    etot = float(sum(np.sum(d * d) for d in details)) + EPS\n",
        "    return e3 / etot\n",
        "\n",
        "\n",
        "def _perm_entropy(x: np.ndarray, m: int = 3, tau: int = 1) -> float:\n",
        "    \"\"\"\n",
        "    Normalized permutation entropy in [0,1] using ordinal patterns.\n",
        "    Deterministic tie-breaking via stable argsort.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=np.float64)\n",
        "    n = x.size\n",
        "    span = (m - 1) * tau\n",
        "    N = n - span\n",
        "    if m < 2 or tau < 1 or N <= 0:\n",
        "        return 0.0\n",
        "    # counts\n",
        "    counts = {}\n",
        "    # stable tie-breaking: argsort twice to get ranks\n",
        "    for i in range(N):\n",
        "        window = x[i : i + span + 1 : tau]\n",
        "        # rank vector (0..m-1) with stable tie handling\n",
        "        order = np.argsort(window, kind=\"mergesort\")\n",
        "        ranks = np.empty(m, dtype=np.int64)\n",
        "        ranks[order] = np.arange(m, dtype=np.int64)\n",
        "        key = tuple(ranks.tolist())\n",
        "        counts[key] = counts.get(key, 0) + 1\n",
        "    total = float(sum(counts.values()))\n",
        "    if total <= 0:\n",
        "        return 0.0\n",
        "    probs = np.fromiter((c / total for c in counts.values()), dtype=np.float64)\n",
        "    H = -np.sum(probs * np.log(probs + EPS))\n",
        "    Hmax = np.log(math.factorial(m))\n",
        "    return float(H / (Hmax + EPS))\n",
        "\n",
        "\n",
        "def compute_frequency_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    bands: tuple = FREQ_BANDS,  # normalized freq bands\n",
        "    dwt_wavelet: str = DWT_WAVELET,\n",
        "    dwt_level: int = DWT_LEVEL,\n",
        "    entropy_m1: int = ENTROPY_M1,\n",
        "    entropy_m2: int = ENTROPY_M2,\n",
        "    entropy_tau: int = ENTROPY_TAU,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Frequency & entropy features per id.\n",
        "\n",
        "    Spectral (on detrended):\n",
        "      - spectral centroid (delta)\n",
        "      - spectral flatness (delta of log-flatness)\n",
        "      - band power fractions over 'bands' (log-ratio after/before per band)\n",
        "      - DWT level-3 detail energy / total detail energy (log-ratio after/before)\n",
        "    \"\"\"\n",
        "    prefix = \"frequency\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # --- choose series ---\n",
        "    z_std = \"standardized\"\n",
        "    z_det = \"detrended\"\n",
        "\n",
        "    z_b = _group_series(X_prep, z_std, 0)\n",
        "    z_a = _group_series(X_prep, z_std, 1)\n",
        "    zd_b = _group_series(X_prep, z_det, 0)\n",
        "    zd_a = _group_series(X_prep, z_det, 1)\n",
        "\n",
        "    z_b = z_b.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    z_a = z_a.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    zd_b = zd_b.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    zd_a = zd_a.apply(lambda s: s.to_numpy(dtype=np.float32, copy=False))\n",
        "    ids = zd_b.index\n",
        "\n",
        "    # ---- accumulators ----\n",
        "    log_flatness_d = []\n",
        "    band_logratios = [[] for _ in bands]  # list of lists\n",
        "    dwt_l3_logratio = []\n",
        "    perm_m3_delta = []\n",
        "    perm_m5_delta = []\n",
        "\n",
        "    # ---- compute per id ----\n",
        "    for i in ids:\n",
        "        xb = zd_b.loc[i]\n",
        "        xa = zd_a.loc[i]\n",
        "\n",
        "        # FFT stats (detrended)\n",
        "        fb, Pb = _psd_rfft(xb)\n",
        "        fa, Pa = _psd_rfft(xa)\n",
        "\n",
        "        # Spectral flatness (use log-flatness then delta)\n",
        "        sf_b = _log_flatness(Pb)\n",
        "        sf_a = _log_flatness(Pa)\n",
        "        # store as log of flatness (so delta is log-ratio); equivalently take log here\n",
        "        log_flatness_d.append(np.log(sf_a + EPS) - np.log(sf_b + EPS))\n",
        "\n",
        "        # Band powers → fractions → log-ratios\n",
        "        frac_b = _bandpower_logratio(fb, Pb, bands)\n",
        "        frac_a = _bandpower_logratio(fa, Pa, bands)\n",
        "        for bi, (fb_i, fa_i) in enumerate(zip(frac_b, frac_a)):\n",
        "            band_logratios[bi].append(np.log((fa_i + EPS) / (fb_i + EPS)))\n",
        "\n",
        "        # DWT L3 energy ratio (detrended) → log-ratio\n",
        "        r_b = _dwt_l3_ratio(xb, dwt_wavelet, dwt_level)\n",
        "        r_a = _dwt_l3_ratio(xa, dwt_wavelet, dwt_level)\n",
        "        dwt_l3_logratio.append(np.log((r_a + EPS) / (r_b + EPS)))\n",
        "\n",
        "        # permutation entropy deltas (m=3 and m=5, tau=1)\n",
        "        zb = z_b.loc[i]\n",
        "        za = z_a.loc[i]\n",
        "        pe_b_m3 = _perm_entropy(zb, m=entropy_m1, tau=entropy_tau)\n",
        "        pe_a_m3 = _perm_entropy(za, m=entropy_m1, tau=entropy_tau)\n",
        "        pe_b_m5 = _perm_entropy(zb, m=entropy_m2, tau=entropy_tau)\n",
        "        pe_a_m5 = _perm_entropy(za, m=entropy_m2, tau=entropy_tau)\n",
        "        perm_m3_delta.append(pe_a_m3 - pe_b_m3)\n",
        "        perm_m5_delta.append(pe_a_m5 - pe_b_m5)\n",
        "\n",
        "    # ---- assemble ----\n",
        "    cols = {\n",
        "        \"spec_flatness_logratio\": log_flatness_d,\n",
        "        \"dwt_l3_energy_logratio\": dwt_l3_logratio,\n",
        "        \"perm_entropy_m3_delta\": perm_m3_delta,\n",
        "        \"perm_entropy_m5_delta\": perm_m5_delta,\n",
        "    }\n",
        "    # add band logratios with names\n",
        "    for bi, _ in enumerate(bands):\n",
        "        cols[f\"bandpower_b{bi + 1}_logratio\"] = band_logratios[bi]\n",
        "\n",
        "    out = pd.DataFrame(cols, index=ids, dtype=np.float32)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# DIFFERENCES BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _shortlag_L1(x: np.ndarray, K: int) -> float:\n",
        "    r = acf_1d(x.astype(np.float32), K)  # r[1..K]\n",
        "    w = 1.0 / np.arange(2, K + 1, dtype=np.float32)\n",
        "    return float(np.sum(np.abs(r[1:]) * w))\n",
        "\n",
        "\n",
        "def compute_differences_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    quantile_fine_grid: list[float] = QUANTILE_FINE_GRID,\n",
        "    crossing_rate_deadband: float = CROSSING_RATE_DEADBAND,\n",
        "    lbq_m: int = LBQ_M,\n",
        "    acf_max_lag: int = ACF_MAX_LAG,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns per-id features (float32):\n",
        "      - diff_mad_logratio                   : log(MAD(d))_after − log(MAD(d))_before\n",
        "      - diff_w1_scaled                      : Wasserstein-1(d_a,d_b) via quantile grid / IDR_before\n",
        "      - diff_signflips_logit_delta          : logit(sign-flip rate)_after − logit(...)_before\n",
        "      - diff_shortlag_l1_delta              : Σ_{ℓ=1..K} |ACF_dm(ℓ)| / ℓ  (delta)\n",
        "      - diff_lbq_stat_delta                 : Ljung–Box Q(m) (delta)\n",
        "      - diff_spec_centroid_delta            : spectral centroid (delta)\n",
        "      - absdiff_up_excd_q95_logit_delta     : logit P(a > T95_before) (delta)\n",
        "      - absdiff_w1_scaled                   : Wasserstein-1(a_a,a_b) via quantile grid / IDR_before\n",
        "      - absdiff_acf1_delta                  : lag-1 ACF(am) (delta)\n",
        "    \"\"\"\n",
        "    prefix = \"differences\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # keep only needed cols to speed up groupby.apply\n",
        "    cols = [\n",
        "        \"period\",\n",
        "        \"diff_standardized\",\n",
        "        \"diff_detrended\",\n",
        "        \"absdiff_detrended\",\n",
        "    ]\n",
        "    df = X_prep[cols].copy()\n",
        "\n",
        "    # ---------- per-id computation ----------\n",
        "    def _one_id(g: pd.DataFrame) -> pd.Series:\n",
        "        gb = g[g[\"period\"] == 0]\n",
        "        ga = g[g[\"period\"] == 1]\n",
        "\n",
        "        # fetch arrays\n",
        "        d_b = gb[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        d_a = ga[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        dm_b = gb[\"diff_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        dm_a = ga[\"diff_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        am_b = gb[\"absdiff_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        am_a = ga[\"absdiff_detrended\"].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # Median-crossing rate change (deadband) on standardized; Q=0\n",
        "        _, _, cr_b = crossing_rates_logits(d_b, Q=0.0, eps=crossing_rate_deadband)\n",
        "        _, _, cr_a = crossing_rates_logits(d_a, Q=0.0, eps=crossing_rate_deadband)\n",
        "        signflips_logit_delta = float(cr_a - cr_b)\n",
        "\n",
        "        # Extreme events (Jeffreys-smoothed logits)\n",
        "        qs = quantile_fine_grid\n",
        "        Qb = np.quantile(d_b, qs)\n",
        "        Qb5, Qb95 = Qb[0], Qb[8]\n",
        "        _, b2 = exceedance_logits(d_b, k_abs=Qb95, k_fixed=3.0)\n",
        "        _, a2 = exceedance_logits(d_a, k_abs=Qb95, k_fixed=3.0)\n",
        "\n",
        "        u_b, l_b = upper_lower_logits(d_b, q_low=Qb5, q_high=Qb95)\n",
        "        u_a, l_a = upper_lower_logits(d_a, q_low=Qb5, q_high=Qb95)\n",
        "\n",
        "        # --- On dm: acf1_delta, short-lag L1, Ljung–Box Q, spectral centroid (all deltas) ---\n",
        "        diff_shortlag_l1_delta = _shortlag_L1(dm_a, acf_max_lag) - _shortlag_L1(\n",
        "            dm_b, acf_max_lag\n",
        "        )\n",
        "        diff_lbq_delta = ljung_box_z(dm_a, lbq_m) - ljung_box_z(dm_b, lbq_m)\n",
        "\n",
        "        # Spectral centroid\n",
        "        fa, Pa = _psd_rfft(dm_a)\n",
        "        fb, Pb = _psd_rfft(dm_b)\n",
        "        diff_spec_centroid_delta = _spectral_centroid(fa, Pa) - _spectral_centroid(\n",
        "            fb, Pb\n",
        "        )\n",
        "\n",
        "        # --- On am: volatility clustering → lag-1 ACF delta ---\n",
        "        r1_b = acf_1d(am_b.astype(np.float32), 1)[0]\n",
        "        r1_a = acf_1d(am_a.astype(np.float32), 1)[0]\n",
        "        absdiff_acf1_delta = float(r1_a - r1_b)\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"diff_abs_excd_3_logit_delta\": a2 - b2,\n",
        "                \"diff_up_excd_q95_logit_delta\": u_a - u_b,\n",
        "                \"diff_low_excd_q5_logit_delta\": l_a - l_b,\n",
        "                \"diff_signflips_logit_delta\": signflips_logit_delta,\n",
        "                \"diff_shortlag_l1_delta\": diff_shortlag_l1_delta,\n",
        "                \"diff_lbq_stat_delta\": diff_lbq_delta,\n",
        "                \"diff_spec_centroid_delta\": diff_spec_centroid_delta,\n",
        "                \"absdiff_acf1_delta\": absdiff_acf1_delta,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    out = (\n",
        "        df.groupby(level=\"id\", sort=False, group_keys=False)\n",
        "        .apply(_one_id)\n",
        "        .astype(np.float32)\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# ABSOLUTE BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def compute_absolute_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    acf_max_lag: int = ACF_MAX_LAG,\n",
        "    lbq_m: int = LBQ_M,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Absolute magnitudes (on 'absval_detrended'):\n",
        "      - absval_acf1_delta       : lag-1 ACF (after − before)\n",
        "      - absval_lbq_stat_delta   : Ljung–Box Q(m) (after − before), m=20 by default\n",
        "    \"\"\"\n",
        "    prefix = \"absolute\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    col = \"absval_detrended\"\n",
        "    df = X_prep[[col, \"period\"]].copy()\n",
        "\n",
        "    def _one_id(g: pd.DataFrame) -> pd.Series:\n",
        "        gb = g[g[\"period\"] == 0][col].to_numpy(np.float32, copy=False)\n",
        "        ga = g[g[\"period\"] == 1][col].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # lag-1 ACF\n",
        "        r1_b = acf_1d(gb, 1)[0]\n",
        "        r1_a = acf_1d(ga, 1)[0]\n",
        "        acf1_delta = float(r1_a - r1_b)\n",
        "\n",
        "        # Shortlag L1\n",
        "        shortlag_l1_delta = _shortlag_L1(ga, acf_max_lag) - _shortlag_L1(\n",
        "            gb, acf_max_lag\n",
        "        )\n",
        "\n",
        "        # LBQ stat delta\n",
        "        lbq_delta = ljung_box_z(ga, lbq_m) - ljung_box_z(gb, lbq_m)\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"absval_acf1_delta\": acf1_delta,\n",
        "                \"absval_shortlag_L1_delta\": shortlag_l1_delta,\n",
        "                \"absval_lbq_stat_delta\": lbq_delta,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    out = (\n",
        "        df.groupby(level=\"id\", sort=False, group_keys=False)\n",
        "        .apply(_one_id)\n",
        "        .astype(np.float32)\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# SQUARED BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def compute_squared_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    acf_max_lag: int = ACF_MAX_LAG,\n",
        "    lbq_m: int = LBQ_M,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Absolute magnitudes (on 'squared_detrended'):\n",
        "      - squared_acf1_delta       : lag-1 ACF (after − before)\n",
        "      -\n",
        "      - squared_lbq_stat_delta   : Ljung–Box Q(m) (after − before), m=20 by default\n",
        "    \"\"\"\n",
        "    prefix = \"squared\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    col = \"squared_detrended\"\n",
        "    df = X_prep[[col, \"period\"]].copy()\n",
        "\n",
        "    def _one_id(g: pd.DataFrame) -> pd.Series:\n",
        "        gb = g[g[\"period\"] == 0][col].to_numpy(np.float32, copy=False)\n",
        "        ga = g[g[\"period\"] == 1][col].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # lag-1 ACF\n",
        "        r1_b = acf_1d(gb, 1)[0]\n",
        "        r1_a = acf_1d(ga, 1)[0]\n",
        "        acf1_delta = float(r1_a - r1_b)\n",
        "\n",
        "        # Shortlag L1\n",
        "        shortlag_l1_delta = _shortlag_L1(ga, acf_max_lag) - _shortlag_L1(\n",
        "            gb, acf_max_lag\n",
        "        )\n",
        "\n",
        "        # LBQ stat delta\n",
        "        lbq_delta = ljung_box_z(ga, lbq_m) - ljung_box_z(gb, lbq_m)\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"squared_acf1_delta\": acf1_delta,\n",
        "                \"squared_shortlag_L1_delta\": shortlag_l1_delta,\n",
        "                \"squared_lbq_stat_delta\": lbq_delta,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    out = (\n",
        "        df.groupby(level=\"id\", sort=False, group_keys=False)\n",
        "        .apply(_one_id)\n",
        "        .astype(np.float32)\n",
        "    )\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# BOUNDARY LOCAL BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _window_around_boundary(\n",
        "    arr_before: np.ndarray,\n",
        "    arr_after: np.ndarray,\n",
        "    w_before: int,\n",
        "    w_after: int,\n",
        "    skip_after: int = 0,\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Return boundary-local windows:\n",
        "      before: last w_before samples,\n",
        "      after : first w_after samples (optionally skip 'skip_after' transient).\n",
        "    \"\"\"\n",
        "    if arr_before.size == 0 or arr_after.size == 0:\n",
        "        return arr_before, arr_after\n",
        "    wb = int(min(w_before, arr_before.size))\n",
        "    sa = int(min(skip_after, max(0, arr_after.size - 1)))\n",
        "    wa = int(min(w_after, max(0, arr_after.size - sa)))\n",
        "    b = arr_before[-wb:] if wb > 0 else arr_before[:0]\n",
        "    a = arr_after[sa : sa + wa] if wa > 0 else arr_after[:0]\n",
        "    return b.astype(np.float32, copy=False), a.astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "def _ecdf_logit_against_before(v: float, before_sorted: np.ndarray) -> float:\n",
        "    \"\"\"Jeffreys-smoothed logit rank of value v against BEFORE sample.\"\"\"\n",
        "    n = before_sorted.size\n",
        "    lt = np.searchsorted(before_sorted, v, side=\"left\")\n",
        "    le = np.searchsorted(before_sorted, v, side=\"right\")\n",
        "    rank = lt + 0.5 * (le - lt)\n",
        "    p = (rank + 0.5) / (n + 1.0)\n",
        "    p = np.clip(p, EPS, 1 - EPS)\n",
        "    return float(np.log(p / (1.0 - p)))\n",
        "\n",
        "\n",
        "def _cliffs_delta(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Cliff's delta in [-1, 1]: P(a>b) - P(a<b).\n",
        "    O(n log n) using searchsorted counts.\n",
        "    \"\"\"\n",
        "    na, nb = a.size, b.size\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    b_sorted = np.sort(b)\n",
        "    # counts for each a: how many b are < a, and > a\n",
        "    lt_counts = np.searchsorted(b_sorted, a, side=\"left\")  # b < a\n",
        "    gt_counts = nb - np.searchsorted(b_sorted, a, side=\"right\")  # b > a\n",
        "    gt = int(np.sum(lt_counts))  # P(a>b) numerator\n",
        "    lt = int(np.sum(gt_counts))  # P(a<b) numerator\n",
        "    return float((gt - lt) / (na * nb))\n",
        "\n",
        "\n",
        "def _chow_F(y_b: np.ndarray, y_a: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Chow F test at the boundary for y ~ a + b t.\n",
        "    Returns the F-statistic. Uses clipped/detrended to limit outliers.\n",
        "    \"\"\"\n",
        "\n",
        "    def _ols_ssr(y):\n",
        "        n = y.size\n",
        "        if n < 2:\n",
        "            return 0.0, 0\n",
        "        t = np.arange(n, dtype=np.float32)\n",
        "        X = np.c_[np.ones(n), t]\n",
        "        # OLS via normal equations (2x2)\n",
        "        XtX = X.T @ X\n",
        "        Xty = X.T @ y\n",
        "        beta = np.linalg.solve(XtX, Xty)\n",
        "        resid = y - X @ beta\n",
        "        return float(np.dot(resid, resid)), 2  # ssr, k\n",
        "\n",
        "    ssr_b, k = _ols_ssr(y_b)\n",
        "    ssr_a, _ = _ols_ssr(y_a)\n",
        "    y = np.concatenate([y_b, y_a])\n",
        "    ssr_pooled, _ = _ols_ssr(y)\n",
        "    n_b, n_a = max(0, y_b.size), max(0, y_a.size)\n",
        "    n = n_b + n_a\n",
        "    # F = ((SSR_pooled - (SSR_b+SSR_a)) / k) / ((SSR_b+SSR_a) / (n - 2k))\n",
        "    num = (ssr_pooled - (ssr_b + ssr_a)) / max(k, 1)\n",
        "    den = (ssr_b + ssr_a) / max(n - 2 * k, 1)\n",
        "    F = num / den if den > 0 else 0.0\n",
        "    return float(max(F, 0.0)) if np.isfinite(F) else 0.0\n",
        "\n",
        "\n",
        "def _cusum_signed_stat(y_b: np.ndarray, y_a: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Signed CUSUM across boundary: build combined series (mean-centered),\n",
        "    compute cumulative sum; report jump as end_after_cusum - last_before_cusum,\n",
        "    normalized by std * sqrt(n) for rough scale invariance.\n",
        "    \"\"\"\n",
        "    y_b = y_b.astype(np.float32)\n",
        "    y_a = y_a.astype(np.float32)\n",
        "    if y_b.size + y_a.size < 3:\n",
        "        return 0.0\n",
        "    y = np.concatenate([y_b, y_a])\n",
        "    y = y - y.mean()\n",
        "    s = np.cumsum(y)\n",
        "    j = y_b.size\n",
        "    num = s[-1] - s[j - 1] if j > 0 else s[-1]\n",
        "    denom = (y.std(ddof=0) + EPS) * np.sqrt(y.size)\n",
        "    return float(num / denom)\n",
        "\n",
        "\n",
        "def _gaussian_glr(zb: np.ndarray, za: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Gaussian log-likelihood ratio at the given split:\n",
        "      LRT = n*log(var_all) - n0*log(var_b) - n1*log(var_a)\n",
        "    Uses MLE variances (ddof=0) about each segment's own mean.\n",
        "    \"\"\"\n",
        "    n0 = int(zb.size)\n",
        "    n1 = int(za.size)\n",
        "    n = n0 + n1\n",
        "    if n0 <= 1 or n1 <= 1:\n",
        "        return 0.0\n",
        "    m0 = float(np.mean(zb))\n",
        "    v0 = float(np.mean((zb - m0) ** 2))\n",
        "    m1 = float(np.mean(za))\n",
        "    v1 = float(np.mean((za - m1) ** 2))\n",
        "    y = np.concatenate([zb, za], axis=0)\n",
        "    m = float(np.mean(y))\n",
        "    v = float(np.mean((y - m) ** 2))\n",
        "    v0 = max(v0, EPS)\n",
        "    v1 = max(v1, EPS)\n",
        "    v = max(v, EPS)\n",
        "    return float(n * np.log(v) - n0 * np.log(v0) - n1 * np.log(v1))\n",
        "\n",
        "\n",
        "def _arch_lm_LM(z: np.ndarray, L: int = 5) -> float:\n",
        "    \"\"\"\n",
        "    Engle's ARCH LM statistic on a single segment z (standardized):\n",
        "      1) center z, set e2 = (z - mean(z))^2\n",
        "      2) regress e2[L:] on [1, e2_{t-1},...,e2_{t-L}]\n",
        "      3) LM = n_eff * R^2\n",
        "    \"\"\"\n",
        "    z = z.astype(np.float64)\n",
        "    n = z.size\n",
        "    if n <= L + 1:\n",
        "        return 0.0\n",
        "    e2 = (z - z.mean()) ** 2\n",
        "    Y = e2[L:]\n",
        "    Xcols = [np.ones_like(Y)]\n",
        "    for j in range(1, L + 1):\n",
        "        Xcols.append(e2[L - j : n - j])\n",
        "    X = np.column_stack(Xcols)  # shape (n_eff, 1+L)\n",
        "    beta, *_ = np.linalg.lstsq(X, Y, rcond=None)\n",
        "    Yhat = X @ beta\n",
        "    ssr = float(np.sum((Yhat - Y.mean()) ** 2))  # regression sum of squares\n",
        "    sst = float(np.sum((Y - Y.mean()) ** 2)) + EPS  # total sum of squares\n",
        "    R2 = ssr / sst\n",
        "    n_eff = Y.shape[0]\n",
        "    return float(n_eff * R2)\n",
        "\n",
        "\n",
        "def compute_boundary_local_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    w_local: list[int] = BOUND_WINDOW_SIZES,\n",
        "    skip_after: int = BOUND_SKIP_AFTER,\n",
        "    acf_K: int = BOUND_ACF_MAX_LAG,\n",
        "    eps_deadband: float = CROSSING_RATE_DEADBAND,\n",
        "    quantile_coarse_grid: list[float] = QUANTILE_COARSE_GRID,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Boundary-local features (windowed around the split): jumps, local scale/trend,\n",
        "    crossings/residence, short-lag ACF/L1, spectral centroid on detrended, local W1,\n",
        "    RMS logratio, diff median jump, Cliff's delta, Chow F, signed CUSUM.\n",
        "\n",
        "    If `w_local` is a list (e.g., [32, 64, 128]), compute the full feature set\n",
        "    for each window size and suffix columns with `_w{size}`.\n",
        "    \"\"\"\n",
        "    w_list = sorted(set(int(w) for w in w_local))\n",
        "\n",
        "    prefix = \"boundary_local\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    require_cols = [\n",
        "        \"period\",\n",
        "        \"standardized\",\n",
        "        \"clipped\",\n",
        "        \"detrended\",\n",
        "        \"diff_standardized\",\n",
        "        \"diff2_standardized\",\n",
        "        \"diff2_detrended\",\n",
        "    ]\n",
        "    g = X_prep[require_cols].groupby(level=\"id\", sort=False)\n",
        "\n",
        "    def _one_id(df: pd.DataFrame, w_curr: int) -> pd.Series:\n",
        "        b = df[df[\"period\"] == 0]\n",
        "        a = df[df[\"period\"] == 1]\n",
        "\n",
        "        z_b = b[\"standardized\"].to_numpy(np.float32, copy=False)\n",
        "        z_a = a[\"standardized\"].to_numpy(np.float32, copy=False)\n",
        "        zc_b = b[\"clipped\"].to_numpy(np.float32, copy=False)\n",
        "        zc_a = a[\"clipped\"].to_numpy(np.float32, copy=False)\n",
        "        zd_b = b[\"detrended\"].to_numpy(np.float32, copy=False)\n",
        "        zd_a = a[\"detrended\"].to_numpy(np.float32, copy=False)\n",
        "        dz_b = b[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        dz_a = a[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        dd_b = b[\"diff2_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        dd_a = a[\"diff2_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        ddm_b = b[\"diff2_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        ddm_a = a[\"diff2_detrended\"].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # Local windows (use current w)\n",
        "        z_bw, z_aw = _window_around_boundary(z_b, z_a, w_curr, w_curr, skip_after)\n",
        "        zc_bw, zc_aw = _window_around_boundary(zc_b, zc_a, w_curr, w_curr, skip_after)\n",
        "        zd_bw, zd_aw = _window_around_boundary(zd_b, zd_a, w_curr, w_curr, skip_after)\n",
        "        dz_bw, dz_aw = _window_around_boundary(dz_b, dz_a, w_curr, w_curr, skip_after)\n",
        "        dd_bw, dd_aw = _window_around_boundary(dd_b, dd_a, w_curr, w_curr, skip_after)\n",
        "        ddmw, ddaw = _window_around_boundary(ddm_b, ddm_a, w_curr, w_curr, skip_after)\n",
        "\n",
        "        # Local median jump\n",
        "        local_median_jump = (\n",
        "            float(np.median(z_aw) - np.median(z_bw))\n",
        "            if (z_aw.size and z_bw.size)\n",
        "            else 0.0\n",
        "        )\n",
        "\n",
        "        # Local scale logratio (MAD)\n",
        "        mad_b = 1.4826 * _mad(z_bw) + EPS\n",
        "        mad_a = 1.4826 * _mad(z_aw) + EPS\n",
        "        local_scale_logratio = float(np.log(mad_a) - np.log(mad_b))\n",
        "\n",
        "        # Local slope jump on detrended\n",
        "        slope_jump = _ols_slope(zd_aw) - _ols_slope(zd_bw)\n",
        "\n",
        "        # Local IDR logratio (Q10..Q90 on standardized)\n",
        "        qs = quantile_coarse_grid\n",
        "        Qb = np.quantile(z_bw, qs) if z_bw.size else np.zeros(len(qs), dtype=np.float32)\n",
        "        Qa = np.quantile(z_aw, qs) if z_aw.size else np.zeros(len(qs), dtype=np.float32)\n",
        "        Q10_b, Q90_b = Qb[0], Qb[-1]\n",
        "        Q10_a, Q90_a = Qa[0], Qa[-1]\n",
        "        idr_b = Q90_b - Q10_b + EPS\n",
        "        idr_a = Q90_a - Q10_a + EPS\n",
        "        IDR_logratio = float(np.log(idr_a) - np.log(idr_b))\n",
        "\n",
        "        # Exceedances using BEFORE thresholds\n",
        "        _, Qb95 = np.quantile(z_b, [0.05, 0.95]) if z_b.size else (0.0, 0.0)\n",
        "        b1, b2 = exceedance_logits(z_bw, k_abs=Qb95, k_fixed=3.0)\n",
        "        a1, a2 = exceedance_logits(z_aw, k_abs=Qb95, k_fixed=3.0)\n",
        "\n",
        "        # Crossing rates & asymmetry @ median (Q=0) with deadband\n",
        "        _, _, cr_b = crossing_rates_logits(z_bw, Q=0.0, eps=eps_deadband)\n",
        "        _, _, cr_a = crossing_rates_logits(z_aw, Q=0.0, eps=eps_deadband)\n",
        "        signflips_logit_delta = float(cr_a - cr_b)\n",
        "        asym_b = median_crossing_asym(z_bw, Q=0.0, eps=eps_deadband)\n",
        "        asym_a = median_crossing_asym(z_aw, Q=0.0, eps=eps_deadband)\n",
        "        med_cross_asym_fisher_delta = fisher_delta(asym_a, asym_b)\n",
        "\n",
        "        # Residence-time logratio\n",
        "        res_time_logratio = float(\n",
        "            mean_log_res_time(z_aw, Q=0.0, eps=eps_deadband)\n",
        "            - mean_log_res_time(z_bw, Q=0.0, eps=eps_deadband)\n",
        "        )\n",
        "\n",
        "        # ACF/short-lag on detrended local windows\n",
        "        r1_b = acf_1d(zd_bw, 1)[0] if zd_bw.size > 1 else 0.0\n",
        "        r1_a = acf_1d(zd_aw, 1)[0] if zd_aw.size > 1 else 0.0\n",
        "        acf1_local_delta = float(r1_a - r1_b)\n",
        "        shortlag_local_delta = _shortlag_L1(zd_aw, acf_K) - _shortlag_L1(zd_bw, acf_K)\n",
        "\n",
        "        # Spectral centroid delta on detrended local windows\n",
        "        fa, Pa = _psd_rfft(zd_aw)\n",
        "        fb, Pb = _psd_rfft(zd_bw)\n",
        "        spec_centroid_local_delta = _spectral_centroid(fa, Pa) - _spectral_centroid(\n",
        "            fb, Pb\n",
        "        )\n",
        "\n",
        "        # Local W1 on standardized (scaled by local IQR_b)\n",
        "        iqr_b = (Qb[-2] - Qb[1]) if len(Qb) >= 4 else (Q90_b - Q10_b)\n",
        "        w1_scale = max(iqr_b, EPS)\n",
        "        w1_scaled = _wasserstein_quant(z_aw, z_bw, quantile_coarse_grid, w1_scale)\n",
        "\n",
        "        # Local RMS logratio on detrended\n",
        "        rms_b = float(np.sqrt(np.mean(zd_bw**2))) if zd_bw.size else 0.0\n",
        "        rms_a = float(np.sqrt(np.mean(zd_aw**2))) if zd_aw.size else 0.0\n",
        "        rms_logratio = np.log(rms_a + EPS) - np.log(rms_b + EPS)\n",
        "\n",
        "        # Cliff's delta (standardized windows)\n",
        "        cd = _cliffs_delta(z_aw, z_bw)\n",
        "        cliffs_fisher_delta = fisher_delta(cd, 0.0)\n",
        "\n",
        "        # Chow F on clipped local windows\n",
        "        chow_F = _chow_F(zc_bw, zc_aw)\n",
        "\n",
        "        # Signed CUSUM on detrended local windows\n",
        "        cusum_signed = _cusum_signed_stat(zd_bw, zd_aw)\n",
        "\n",
        "        # Gauss GLR\n",
        "        gauss_glr = _gaussian_glr(z_bw, z_aw)\n",
        "\n",
        "        # Diff median jump\n",
        "        local_diff_median_jump = float(np.median(dz_aw) - np.median(dz_bw))\n",
        "\n",
        "        # Diff2 MAD logratio\n",
        "        mad_b2 = 1.4826 * _mad(dd_bw) + EPS\n",
        "        mad_a2 = 1.4826 * _mad(dd_aw) + EPS\n",
        "        local_dd_MAD_logratio = float(np.log(mad_a2) - np.log(mad_b2))\n",
        "\n",
        "        # Diff2 ACF1 delta on detrended local windows\n",
        "        r1_b2 = acf_1d(ddmw, 1)[0] if ddmw.size > 1 else 0.0\n",
        "        r1_a2 = acf_1d(ddaw, 1)[0] if ddaw.size > 1 else 0.0\n",
        "        local_dd_acf1_delta = float(r1_a2 - r1_b2)\n",
        "\n",
        "        # Diff2 Local W1 scaled by local IQR_before (use coarse grid)\n",
        "        Qb2 = np.quantile(dd_bw, quantile_coarse_grid)\n",
        "        idr_local_b = float(Qb2[-1] - Qb2[0]) + EPS\n",
        "        local_dd_w1_scaled = _wasserstein_quant(\n",
        "            dd_aw, dd_bw, quantile_coarse_grid, idr_local_b\n",
        "        )\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"local_median_jump\": local_median_jump,\n",
        "                \"local_MAD_logratio\": local_scale_logratio,\n",
        "                \"local_slope_jump\": slope_jump,\n",
        "                \"local_IDR_logratio\": IDR_logratio,\n",
        "                \"local_abs_excd_q95_logit_delta\": a1 - b1,\n",
        "                \"local_abs_excd_3_logit_delta\": a2 - b2,\n",
        "                \"local_signflips_logit_delta\": signflips_logit_delta,\n",
        "                \"local_med_cross_asym_fisher_delta\": med_cross_asym_fisher_delta,\n",
        "                \"local_res_time_logratio\": res_time_logratio,\n",
        "                \"local_acf1_delta\": acf1_local_delta,\n",
        "                \"local_shortlag_L1_delta\": shortlag_local_delta,\n",
        "                \"local_spec_centroid_delta\": spec_centroid_local_delta,\n",
        "                \"local_w1_scaled\": w1_scaled,\n",
        "                \"local_rms_logratio\": rms_logratio,\n",
        "                \"local_cliffs_fisher_delta\": cliffs_fisher_delta,\n",
        "                \"local_chow_F\": chow_F,\n",
        "                \"local_cusum_signed\": cusum_signed,\n",
        "                \"local_gauss_glr\": gauss_glr,\n",
        "                \"local_diff_median_jump\": local_diff_median_jump,\n",
        "                \"local_dd_MAD_logratio\": local_dd_MAD_logratio,\n",
        "                \"local_dd_acf1_delta\": local_dd_acf1_delta,\n",
        "                \"local_dd_w1_scaled\": local_dd_w1_scaled,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    # compute per-window, suffix columns, then concat horizontally\n",
        "    pieces = []\n",
        "    for w in w_list:\n",
        "        out_w = g.apply(lambda df: _one_id(df, w)).astype(np.float32)\n",
        "        out_w = out_w.add_suffix(f\"_w{w}\")  # suffix all columns for clarity\n",
        "        pieces.append(out_w)\n",
        "\n",
        "    out = pd.concat(pieces, axis=1)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# BOUNDARY EDGE BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def compute_boundary_edge_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    w_edge: int = BOUND_EDGE,  # edge window length\n",
        "    offsets: tuple[int, ...] = BOUND_OFFSETS,  # after-side offsets to test\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute edge-Window median jump and rank-logit delta at multiple after-side offsets.\n",
        "    For each id:\n",
        "      • boundary_dz_o{off}           : median(after[off:off+w_edge]) − median(before[-w_edge:])\n",
        "      • ranklogit_delta_o{off}       : logit(rank(v_a vs BEFORE)) − logit(rank(v_b vs BEFORE))\n",
        "      • edge_dz_max_abs, edge_ranklogit_max_abs\n",
        "\n",
        "    Notes:\n",
        "      - Uses standardized z (robust to scale/shift).\n",
        "      - Independent of w_local; only relies on w_edge and offsets.\n",
        "      - If a window is empty, uses 0.0 for that value.\n",
        "    \"\"\"\n",
        "    prefix = \"boundary_edge\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # pull standardized segments as numpy\n",
        "    z_b = _group_series(X_prep, \"standardized\", 0).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    z_a = _group_series(X_prep, \"standardized\", 1).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    ids = z_b.index\n",
        "\n",
        "    # prebuild column names\n",
        "    dz_cols = [f\"edge_dz_o{off}\" for off in offsets]\n",
        "    rk_cols = [f\"edge_ranklogit_delta_o{off}\" for off in offsets]\n",
        "\n",
        "    rows = []\n",
        "    for i in ids:\n",
        "        zb = z_b.loc[i]\n",
        "        za = z_a.loc[i]\n",
        "\n",
        "        # BEFORE edge median (one-time)\n",
        "        wb = int(min(w_edge, zb.size))\n",
        "        z_be = zb[-wb:] if wb > 0 else zb[:0]\n",
        "        v_b = float(np.median(z_be)) if z_be.size else 0.0\n",
        "\n",
        "        # Jeffreys-smoothed logit rank of v_b vs BEFORE ECDF (one-time)\n",
        "        zb_sorted = np.sort(zb) if zb.size else np.array([], dtype=np.float32)\n",
        "        logit_vb = _ecdf_logit_against_before(v_b, zb_sorted) if zb_sorted.size else 0.0\n",
        "\n",
        "        # per-offset computations\n",
        "        dz_vals = []\n",
        "        rk_vals = []\n",
        "\n",
        "        for off in offsets:\n",
        "            # AFTER edge median at offset\n",
        "            start = int(min(max(off, 0), max(za.size - 1, 0)))\n",
        "            wa = int(min(w_edge, max(za.size - start, 0)))\n",
        "            z_ae = za[start : start + wa] if wa > 0 else za[:0]\n",
        "            v_a = float(np.median(z_ae)) if z_ae.size else 0.0\n",
        "\n",
        "            # signed jump\n",
        "            dz = v_a - v_b\n",
        "            dz_vals.append(dz)\n",
        "\n",
        "            # rank-logit delta vs BEFORE ECDF\n",
        "            logit_va = _ecdf_logit_against_before(v_a, zb_sorted)\n",
        "            rk_vals.append(float(logit_va - logit_vb))\n",
        "\n",
        "        dz_arr = np.asarray(dz_vals, dtype=np.float32)\n",
        "        rk_arr = np.asarray(rk_vals, dtype=np.float32)\n",
        "\n",
        "        # summaries (max by absolute magnitude, keep signed value)\n",
        "        dz_arg = int(np.argmax(np.abs(dz_arr)))\n",
        "        rk_arg = int(np.argmax(np.abs(rk_arr)))\n",
        "\n",
        "        row = {\n",
        "            **{c: v for c, v in zip(dz_cols, dz_arr.tolist())},\n",
        "            **{c: v for c, v in zip(rk_cols, rk_arr.tolist())},\n",
        "            \"edge_dz_max_abs\": float(dz_arr[dz_arg]),\n",
        "            \"edge_ranklogit_max_abs\": float(rk_arr[rk_arg]),\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    out = pd.DataFrame(rows, index=ids, dtype=np.float32)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# CURVATURE (SECOND DIFFERENCES) BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def compute_curvature_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    acf_max_lag: int = ACF_MAX_LAG,\n",
        "    lbq_m: int = LBQ_M,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Global features on second differences (curvature) built from:\n",
        "      - diff2_standardized (dd) and diff2_detrended (ddm)\n",
        "      - diff_standardized (d) for curv_vs_slope ratio\n",
        "    Outputs per id:\n",
        "      curv_energy_logratio,\n",
        "      curv_vs_slope_logratio, dd_posrate_logit_delta,\n",
        "      dd_acf1_delta, dd_shortlag_L1_delta, dd_lbq_z_delta,\n",
        "      dd_spec_centroid_delta\n",
        "    \"\"\"\n",
        "    prefix = \"curvature\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    cols = [\"period\", \"diff2_standardized\", \"diff2_detrended\", \"diff_standardized\"]\n",
        "    g = X_prep[cols].groupby(level=\"id\", sort=False)\n",
        "\n",
        "    def _lbq_z(x: np.ndarray, m: int) -> float:\n",
        "        m_eff = int(min(m, max(1, x.size - 1)))\n",
        "        r = acf_1d(x.astype(np.float32), m_eff)\n",
        "        n = x.size\n",
        "        ks = np.arange(1, m_eff + 1, dtype=np.float32)\n",
        "        Q = n * (n + 2.0) * np.sum((r**2) / (n - ks))\n",
        "        return float((Q - m_eff) / np.sqrt(2.0 * m_eff)) if np.isfinite(Q) else 0.0\n",
        "\n",
        "    def _one_id(df: pd.DataFrame) -> pd.Series:\n",
        "        b = df[df[\"period\"] == 0]\n",
        "        a = df[df[\"period\"] == 1]\n",
        "\n",
        "        dd_b = b[\"diff2_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        dd_a = a[\"diff2_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        ddm_b = b[\"diff2_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        ddm_a = a[\"diff2_detrended\"].to_numpy(np.float32, copy=False)\n",
        "        d_b = b[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "        d_a = a[\"diff_standardized\"].to_numpy(np.float32, copy=False)\n",
        "\n",
        "        # Energies\n",
        "        e_b = float(np.mean(dd_b * dd_b)) if dd_b.size else 0.0\n",
        "        e_a = float(np.mean(dd_a * dd_a)) if dd_a.size else 0.0\n",
        "        curv_energy_logratio = float(np.log(e_a + EPS) - np.log(e_b + EPS))\n",
        "\n",
        "        s_b = float(np.mean(d_b * d_b)) if d_b.size else 0.0\n",
        "        s_a = float(np.mean(d_a * d_a)) if d_a.size else 0.0\n",
        "        curv_vs_slope_logratio = float(\n",
        "            np.log((e_a / (s_a + EPS)) + EPS) - np.log((e_b / (s_b + EPS)) + EPS)\n",
        "        )\n",
        "\n",
        "        # Sign dynamics (P(dd > 0) Jeffreys-logit delta)\n",
        "        pos_b = int(np.sum(dd_b > 0.0))\n",
        "        m_b = dd_b.size if dd_b.size else 1\n",
        "        pos_a = int(np.sum(dd_a > 0.0))\n",
        "        m_a = dd_a.size if dd_a.size else 1\n",
        "        dd_posrate_logit_delta = jeffreys_logit(pos_a, m_a) - jeffreys_logit(pos_b, m_b)\n",
        "\n",
        "        # Dependence on detrended curvature\n",
        "        dd_acf1_delta = float(\n",
        "            (acf_1d(ddm_a, 1)[0] if ddm_a.size > 1 else 0.0)\n",
        "            - (acf_1d(ddm_b, 1)[0] if ddm_b.size > 1 else 0.0)\n",
        "        )\n",
        "        dd_shortlag_L1_delta = _shortlag_L1(ddm_a, acf_max_lag) - _shortlag_L1(\n",
        "            ddm_b, acf_max_lag\n",
        "        )\n",
        "\n",
        "        m_id = int(min(lbq_m, max(1, ddm_b.size - 1), max(1, ddm_a.size - 1)))\n",
        "        dd_lbq_z_delta = _lbq_z(ddm_a, m_id) - _lbq_z(ddm_b, m_id)\n",
        "\n",
        "        # Spectral centroid on ddm\n",
        "        fa, Pa = _psd_rfft(ddm_a)\n",
        "        fb, Pb = _psd_rfft(ddm_b)\n",
        "        dd_spec_centroid_delta = _spectral_centroid(fa, Pa) - _spectral_centroid(fb, Pb)\n",
        "\n",
        "        return pd.Series(\n",
        "            {\n",
        "                \"curv_energy_logratio\": curv_energy_logratio,\n",
        "                \"curv_vs_slope_logratio\": curv_vs_slope_logratio,\n",
        "                \"dd_posrate_logit_delta\": dd_posrate_logit_delta,\n",
        "                \"dd_acf1_delta\": dd_acf1_delta,\n",
        "                \"dd_shortlag_L1_delta\": dd_shortlag_L1_delta,\n",
        "                \"dd_lbq_z_delta\": dd_lbq_z_delta,\n",
        "                \"dd_spec_centroid_delta\": dd_spec_centroid_delta,\n",
        "            },\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "    out = g.apply(_one_id).astype(np.float32)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# ROLLING BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _cumsums_z(z):\n",
        "    S = np.empty(z.size + 1, dtype=np.float32)\n",
        "    S[0] = 0.0\n",
        "    np.cumsum(z, out=S[1:])\n",
        "    S2 = np.empty(z.size + 1, dtype=np.float32)\n",
        "    S2[0] = 0.0\n",
        "    np.cumsum(z * z, out=S2[1:])\n",
        "    return S, S2\n",
        "\n",
        "\n",
        "def _cumsums_dm2(dm):\n",
        "    Sd = np.empty(dm.size + 1, dtype=np.float32)\n",
        "    Sd[0] = 0.0\n",
        "    np.cumsum(dm * dm, out=Sd[1:])\n",
        "    return Sd\n",
        "\n",
        "\n",
        "def _cross_prefix(z, eps):\n",
        "    c = ((z[:-1] <= -eps) & (z[1:] >= eps)) | ((z[:-1] >= eps) & (z[1:] <= -eps))\n",
        "    C = np.empty(c.size + 1, dtype=np.int32)\n",
        "    C[0] = 0\n",
        "    np.cumsum(c.astype(np.int32), out=C[1:])\n",
        "    return C\n",
        "\n",
        "\n",
        "# --- small helpers (fast top-k mean on |x|; no full sort) ---------------------\n",
        "def _topk_mean_abs(x: np.ndarray, k: int) -> float:\n",
        "    n = x.size\n",
        "    if n == 0:\n",
        "        return 0.0\n",
        "    kk = min(k, n)\n",
        "    ax = np.abs(x)\n",
        "    # take k largest by abs via partition (O(n))\n",
        "    # np.partition keeps the k largest in the last kk positions (unordered)\n",
        "    idx = np.argpartition(ax, n - kk)[-kk:]\n",
        "    return float(ax[idx].mean())\n",
        "\n",
        "\n",
        "# --- rolling jumps from precomputed prefixes ----------------------------------\n",
        "def _roll_logstd_jump_stats_from_cumsums(\n",
        "    S: np.ndarray, S2: np.ndarray, w: int, min_pos: int, topk: int\n",
        ") -> tuple[float, float, float]:\n",
        "    \"\"\"Returns (maxpos, minneg, topkabs_mean) for Δ log-std between adjacent windows of size w.\"\"\"\n",
        "    n = S.size - 1  # since S is cumsum with S[0]=0 of length n+1\n",
        "    pos = n - 2 * w + 1\n",
        "    if n < 2 * w or pos < min_pos:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    # rolling mean and var (population) for all windows\n",
        "    # for t in [0..n-w]: sum = S[t+w]-S[t], mean = sum/w\n",
        "    sum_w = S[w:] - S[:-w]  # length n-w+1\n",
        "    mean_w = sum_w / float(w)\n",
        "    sumsq_w = S2[w:] - S2[:-w]  # length n-w+1\n",
        "    var_w = np.maximum(sumsq_w / float(w) - mean_w * mean_w, 0.0)  # numerical safety\n",
        "    logstd_w = np.log(np.sqrt(var_w) + EPS)  # length n-w+1\n",
        "\n",
        "    # adjacent jump J[t] = R[t] - L[t] with R = logstd[t+w], L = logstd[t], t in [0..n-2w]\n",
        "    L = logstd_w[:-w]\n",
        "    R = logstd_w[w:]\n",
        "    J = R - L\n",
        "    if J.size == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    return float(np.max(J)), float(np.min(J)), _topk_mean_abs(J, topk)\n",
        "\n",
        "\n",
        "def _roll_logrms_jump_stats_from_cumsums(\n",
        "    Sd: np.ndarray, w: int, min_pos: int, topk: int\n",
        ") -> tuple[float, float, float]:\n",
        "    \"\"\"Returns (maxpos, minneg, topkabs_mean) for Δ log-RMS of diff_detrended between adjacent windows of size w.\"\"\"\n",
        "    n = Sd.size - 1  # Sd is cumsum of dm^2 with Sd[0]=0\n",
        "    pos = n - 2 * w + 1\n",
        "    if n < 2 * w or pos < min_pos:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    sumsq_w = Sd[w:] - Sd[:-w]  # length n-w+1\n",
        "    rms_w = np.sqrt(np.maximum(sumsq_w / float(w), 0.0))\n",
        "    logrms_w = np.log(rms_w + EPS)\n",
        "\n",
        "    L = logrms_w[:-w]\n",
        "    R = logrms_w[w:]\n",
        "    J = R - L\n",
        "    if J.size == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    return float(np.max(J)), float(np.min(J)), _topk_mean_abs(J, topk)\n",
        "\n",
        "\n",
        "def _roll_crossrate_logit_jump_from_prefix(\n",
        "    C: np.ndarray, w: int, min_pos: int, topk: int\n",
        ") -> tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    C is prefix sum over 'cross' of length n-1 (C[0]=0, C[t]=sum_{u< t} cross[u]).\n",
        "    Window [i, i+w-1] has (w-1) transitions: count = C[i+w-1]-C[i].\n",
        "    p_hat = (count + 0.5) / ((w-1) + 1.0)   (Jeffreys), logit(p_hat) then adjacent jump.\n",
        "    \"\"\"\n",
        "    # C length = n_trans+1 where n_trans = n-1; valid start i: 0..(n-w)\n",
        "    n_trans = C.size - 1  # = (n-1)\n",
        "    n = n_trans + 1\n",
        "    pos = n - 2 * w + 1\n",
        "    if n < 2 * w or pos < min_pos or w < 2:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    # for i in [0..n-w], transitions idx span [i .. i+w-2] ⇒ count = C[i+w-1]-C[i]\n",
        "    cnt_w = C[w - 1 :] - C[: -w + 1]  # length n-w+1\n",
        "    denom = (w - 1) + 1.0  # Jeffreys (w-1 transitions + 1.0)\n",
        "    p = (cnt_w + 0.5) / denom\n",
        "    p = np.clip(p, EPS, 1.0 - EPS)\n",
        "    logit = np.log(p) - np.log(1.0 - p)\n",
        "\n",
        "    L = logit[:-w]\n",
        "    R = logit[w:]\n",
        "    J = R - L\n",
        "    if J.size == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    return float(np.max(J)), float(np.min(J)), _topk_mean_abs(J, topk)\n",
        "\n",
        "\n",
        "def _ewvar_last(x, hl):\n",
        "    \"\"\"\n",
        "    EWMA of x^2 with half-life hl; return last value.\n",
        "    alpha = 1 - 2^(-1/hl)\n",
        "    \"\"\"\n",
        "    if x.size == 0:\n",
        "        return 0.0\n",
        "    alpha = 1.0 - 2.0 ** (-1.0 / max(hl, 1.0))\n",
        "    v = float(x[0] * x[0])\n",
        "    for xi in x[1:]:\n",
        "        v = (1 - alpha) * v + alpha * float(xi * xi)\n",
        "    return v\n",
        "\n",
        "\n",
        "def compute_rolling_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    windows: tuple = ROLL_WINDOWS,\n",
        "    min_positions_per_half: int = ROLL_MIN_POS_PER_HALF,\n",
        "    ewvar_half_lives: tuple = EWVAR_HALFLIVES,\n",
        "    crossing_rate_deadband: float = CROSSING_RATE_DEADBAND,\n",
        "    topk: int = ROLL_TOPK,  # <— add this\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fast rolling / localized-change features.\n",
        "      • Rolling log-std jumps on 'clipped' (winsorized standardized)\n",
        "      • Rolling log-RMS jumps on 'diff_detrended'\n",
        "      • Rolling crossing-rate (Jeffreys-logit) jumps on 'clipped'\n",
        "      • EWVAR logratios at the boundary (two half-lives)\n",
        "    All per-id; deltas = AFTER − BEFORE. Returns float32 DataFrame.\n",
        "    \"\"\"\n",
        "    prefix = \"rolling\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    zc_b = _group_series(X_prep, \"clipped\", 0).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    zc_a = _group_series(X_prep, \"clipped\", 1).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    dm_b = _group_series(X_prep, \"diff_detrended\", 0).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    dm_a = _group_series(X_prep, \"diff_detrended\", 1).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    ids = zc_b.index\n",
        "\n",
        "    rows = []\n",
        "    for i in ids:\n",
        "        zb, za = zc_b.loc[i], zc_a.loc[i]\n",
        "        dmb, dma = dm_b.loc[i], dm_a.loc[i]\n",
        "        nb, na = zb.size, za.size\n",
        "\n",
        "        feats = {}\n",
        "\n",
        "        # precompute prefixes once per half\n",
        "        S_b, S2_b = _cumsums_z(zb)\n",
        "        S_a, S2_a = _cumsums_z(za)\n",
        "        Sd_b = _cumsums_dm2(dmb)\n",
        "        Sd_a = _cumsums_dm2(dma)\n",
        "        C_b = _cross_prefix(zb, crossing_rate_deadband)\n",
        "        C_a = _cross_prefix(za, crossing_rate_deadband)\n",
        "\n",
        "        for w in windows:\n",
        "            # initialize columns for every id (avoid NaNs when invalid)\n",
        "            feats[f\"roll_logstd_jump_w{w}_maxpos_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_logstd_jump_w{w}_maxneg_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_logstd_jump_w{w}_topkabs_mean_delta\"] = np.float32(0.0)\n",
        "\n",
        "            feats[f\"roll_rms_jump_w{w}_maxpos_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_rms_jump_w{w}_maxneg_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_rms_jump_w{w}_topkabs_mean_delta\"] = np.float32(0.0)\n",
        "\n",
        "            feats[f\"roll_crossrate_logit_w{w}_maxpos_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_crossrate_logit_w{w}_maxneg_delta\"] = np.float32(0.0)\n",
        "            feats[f\"roll_crossrate_logit_w{w}_topkabs_mean_delta\"] = np.float32(0.0)\n",
        "\n",
        "            # quick feasibility check for both halves\n",
        "            if (nb - 2 * w + 1) < min_positions_per_half or (\n",
        "                na - 2 * w + 1\n",
        "            ) < min_positions_per_half:\n",
        "                continue\n",
        "\n",
        "            # log-std (zc)\n",
        "            smax_b, smin_b, stop_b = _roll_logstd_jump_stats_from_cumsums(\n",
        "                S_b, S2_b, w, min_positions_per_half, topk\n",
        "            )\n",
        "            smax_a, smin_a, stop_a = _roll_logstd_jump_stats_from_cumsums(\n",
        "                S_a, S2_a, w, min_positions_per_half, topk\n",
        "            )\n",
        "            feats[f\"roll_logstd_jump_w{w}_maxpos_delta\"] = np.float32(smax_a - smax_b)\n",
        "            feats[f\"roll_logstd_jump_w{w}_maxneg_delta\"] = np.float32(smin_a - smin_b)\n",
        "            feats[f\"roll_logstd_jump_w{w}_topkabs_mean_delta\"] = np.float32(\n",
        "                stop_a - stop_b\n",
        "            )\n",
        "\n",
        "            # log-RMS (diff_detrended)\n",
        "            rmax_b, rmin_b, rtop_b = _roll_logrms_jump_stats_from_cumsums(\n",
        "                Sd_b, w, min_positions_per_half, topk\n",
        "            )\n",
        "            rmax_a, rmin_a, rtop_a = _roll_logrms_jump_stats_from_cumsums(\n",
        "                Sd_a, w, min_positions_per_half, topk\n",
        "            )\n",
        "            feats[f\"roll_rms_jump_w{w}_maxpos_delta\"] = np.float32(rmax_a - rmax_b)\n",
        "            feats[f\"roll_rms_jump_w{w}_maxneg_delta\"] = np.float32(rmin_a - rmin_b)\n",
        "            feats[f\"roll_rms_jump_w{w}_topkabs_mean_delta\"] = np.float32(\n",
        "                rtop_a - rtop_b\n",
        "            )\n",
        "\n",
        "            # crossing-rate (zc, Jeffreys-logit)\n",
        "            cmax_b, cmin_b, ctop_b = _roll_crossrate_logit_jump_from_prefix(\n",
        "                C_b, w, min_positions_per_half, topk\n",
        "            )\n",
        "            cmax_a, cmin_a, ctop_a = _roll_crossrate_logit_jump_from_prefix(\n",
        "                C_a, w, min_positions_per_half, topk\n",
        "            )\n",
        "            feats[f\"roll_crossrate_logit_w{w}_maxpos_delta\"] = np.float32(\n",
        "                cmax_a - cmax_b\n",
        "            )\n",
        "            feats[f\"roll_crossrate_logit_w{w}_maxneg_delta\"] = np.float32(\n",
        "                cmin_a - cmin_b\n",
        "            )\n",
        "            feats[f\"roll_crossrate_logit_w{w}_topkabs_mean_delta\"] = np.float32(\n",
        "                ctop_a - ctop_b\n",
        "            )\n",
        "\n",
        "        # EWVAR boundary logratios (unchanged)\n",
        "        for hl in ewvar_half_lives:\n",
        "            vb = _ewvar_last(zb, hl)\n",
        "            va = _ewvar_last(za, hl)\n",
        "            feats[f\"ewvar_hl{hl}_logratio\"] = np.float32(\n",
        "                np.log(va + EPS) - np.log(vb + EPS)\n",
        "            )\n",
        "\n",
        "        rows.append((i, feats))\n",
        "\n",
        "    out = pd.DataFrame({idx: f for idx, f in rows}).T\n",
        "    out.index = ids\n",
        "    out = out.astype(np.float32)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# ROLLING BLOCK\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _design(y: np.ndarray, p: int):\n",
        "    n = y.size\n",
        "    if n <= p:\n",
        "        return None, None\n",
        "    Y = y[p:]\n",
        "    X = np.column_stack(\n",
        "        [np.ones(n - p, dtype=np.float32)] + [y[p - j : n - j] for j in range(1, p + 1)]\n",
        "    ).astype(np.float32, copy=False)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def _ridge_fit(y: np.ndarray, p: int, lam: float):\n",
        "    X, Y = _design(y, p)\n",
        "    if X is None:\n",
        "        return None\n",
        "    # (X^T X + λ*I_p)^{-1} X^T Y, but do NOT penalize intercept\n",
        "    XtX = X.T @ X\n",
        "    reg = np.eye(p + 1, dtype=np.float32) * lam\n",
        "    reg[0, 0] = 0.0  # intercept unpenalized\n",
        "    beta = np.linalg.solve(XtX + reg, X.T @ Y).astype(np.float32)\n",
        "    c = float(beta[0])\n",
        "    phi = beta[1:].astype(np.float32)\n",
        "    # train residual variance\n",
        "    mu = X @ beta\n",
        "    e = (Y - mu).astype(np.float32)\n",
        "    sigma2 = float(np.mean(e * e) + EPS)\n",
        "    return phi, c, sigma2\n",
        "\n",
        "\n",
        "def _mean_nll_window(y: np.ndarray, p: int, phi: np.ndarray, c: float, sigma2: float):\n",
        "    X, Y = _design(y, p)\n",
        "    if X is None:\n",
        "        return 0.0, np.zeros(0, dtype=np.float32)\n",
        "    mu = c + X[:, 1:] @ phi\n",
        "    inv_s2 = 1.0 / (sigma2 + 1e-12)\n",
        "    resid = (Y - mu).astype(np.float32)\n",
        "    nll = 0.5 * (np.log(2.0 * np.pi * (sigma2 + 1e-12)) + (resid * resid) * inv_s2)\n",
        "    return float(np.mean(nll)), resid\n",
        "\n",
        "\n",
        "def compute_ar_block(\n",
        "    X_prep: pd.DataFrame,\n",
        "    force: bool = False,\n",
        "    inference: bool = False,\n",
        "    p: int = AR_ORDER,  # AR order\n",
        "    ridge_lambda: float = AR_RIDGE_LAMBDA,\n",
        "    score_cap: int = AR_SCORE_CAP,  # equal-length scoring cap H\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ridge AR(p) on 'detrended', emit ONLY:\n",
        "        ar_ridge_nll_logratio = log( meanNLL(AFTER_head) / meanNLL(BEFORE_hold) )\n",
        "\n",
        "    Train on BEFORE (excluding a length-H holdout at the end), score on:\n",
        "      • BEFORE_hold = last H of BEFORE\n",
        "      • AFTER_head  = first H of AFTER\n",
        "    with H = min(score_cap, floor(nb/2), na).\n",
        "\n",
        "    No guards: if series are too short, this will raise (as requested).\n",
        "    \"\"\"\n",
        "    prefix = \"ar\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force and not inference:\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    EPS = globals().get(\"EPS\", 1e-8)\n",
        "\n",
        "    # pull detrended series\n",
        "    zb = _group_series(X_prep, \"detrended\", 0).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    za = _group_series(X_prep, \"detrended\", 1).apply(\n",
        "        lambda s: s.to_numpy(np.float32, copy=False)\n",
        "    )\n",
        "    ids = zb.index\n",
        "\n",
        "    rows = []\n",
        "    for i in ids:\n",
        "        b = zb.loc[i]\n",
        "        a = za.loc[i]\n",
        "        nb, na = b.size, a.size\n",
        "\n",
        "        # equal-length scoring window\n",
        "        H = int(min(score_cap, nb // 2, na))\n",
        "\n",
        "        # split BEFORE into train + hold (no guards)\n",
        "        n_train_b = nb - H\n",
        "        b_train = b[:n_train_b]\n",
        "        b_hold = b[nb - H :]\n",
        "\n",
        "        # fit on BEFORE-train (no guards)\n",
        "        phi_b, c_b, s2_b = _ridge_fit(b_train, p, ridge_lambda)\n",
        "\n",
        "        # mean NLL on BEFORE_hold and AFTER_head\n",
        "        nll_bh, _ = _mean_nll_window(b_hold, p, phi_b, c_b, s2_b)\n",
        "        nll_af, _ = _mean_nll_window(a[:H], p, phi_b, c_b, s2_b)\n",
        "\n",
        "        nll_logratio = float(np.log((nll_af + EPS) / (nll_bh + EPS)))\n",
        "\n",
        "        rows.append((i, {\"ar_ridge_nll_logratio\": np.float32(nll_logratio)}))\n",
        "\n",
        "    out = pd.DataFrame({idx: feats for idx, feats in rows}).T\n",
        "    out.index = ids\n",
        "    out = out.astype(np.float32)\n",
        "\n",
        "    if not inference:\n",
        "        _save_cache(out, prefix)\n",
        "    return out\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Build features wrapper\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# @crunch/keep:on\n",
        "\n",
        "FEATURE_BLOCKS = {\n",
        "    \"moments\": compute_moments_block,\n",
        "    \"quantiles\": compute_quantiles_block,\n",
        "    \"rates\": compute_rates_block,\n",
        "    \"autocorrelation\": compute_autocorr_block,\n",
        "    \"tests_distances\": compute_tests_distances_block,\n",
        "    \"frequency\": compute_frequency_block,\n",
        "    \"differences\": compute_differences_block,\n",
        "    \"absolute\": compute_absolute_block,\n",
        "    \"squared\": compute_squared_block,\n",
        "    \"boundary_local\": compute_boundary_local_block,\n",
        "    \"boundary_edge\": compute_boundary_edge_block,\n",
        "    \"curvature\": compute_curvature_block,\n",
        "    \"rolling\": compute_rolling_block,\n",
        "    \"ar\": compute_ar_block,\n",
        "}\n",
        "\n",
        "# @crunch/keep:off\n",
        "\n",
        "\n",
        "def build_features(\n",
        "    X_train: pd.DataFrame,\n",
        "    force_prep: bool = False,\n",
        "    force_all: bool = False,\n",
        "    force: dict[str, bool] | None = None,\n",
        "    inference: bool = False,\n",
        "    feature_blocks: dict = FEATURE_BLOCKS\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build and return a wide per-id feature table by:\n",
        "      1) Preprocessing the raw, long-format input (delegated to `build_preprocessed`).\n",
        "      2) Computing each registered feature block in `BLOCKS` (each block caches itself).\n",
        "      3) Merging all block outputs on the index (id).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "        Raw input in the expected MultiIndex (id, time) format for preprocessing.\n",
        "    force_prep : bool, optional\n",
        "        If True, recompute preprocessing and ignore the upstream preprocess cache.\n",
        "    force_all : bool, optional\n",
        "        If True, recompute *all* feature blocks (overrides per-block cache).\n",
        "    force : dict[str, bool] | None, optional\n",
        "        Per-block override, e.g. {'moments': True, 'quantiles': False}.\n",
        "        Only used when `force_all` is False. Keys must match `BLOCKS` names.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Wide feature matrix with one row per id and columns from all blocks.\n",
        "    \"\"\"\n",
        "    # Decide which blocks to recompute. If force_all is True, force all blocks.\n",
        "    force = force or {}\n",
        "    if inference:\n",
        "        force_prep = True\n",
        "        force_all = True\n",
        "    if force_all:\n",
        "        force = {name: True for name in feature_blocks}\n",
        "\n",
        "    # Do not recompute anything if everything in force is False\n",
        "    prefix = \"all\"\n",
        "    cache = _latest_cache(prefix)\n",
        "    if cache and not force_prep and not any(force.values()):\n",
        "        print(f\"Loading cached data from {cache}\")\n",
        "        return pd.read_parquet(cache)\n",
        "\n",
        "    # 1) Preprocess raw X\n",
        "    X_prep = build_preprocessed(X_train, force_prep, inference)\n",
        "\n",
        "    # Sanity check 1\n",
        "    detect_non_finite(X_prep)\n",
        "\n",
        "    # 2) Compute each block (respect per-block force flags)\n",
        "    parts = []\n",
        "    for name, fn in feature_blocks.items():\n",
        "        part = fn(X_prep, force.get(name, False), inference)\n",
        "        parts.append(part)\n",
        "\n",
        "    # 3) Merge all blocks on id (inner join ensures only ids present in all blocks remain)\n",
        "    #    If you add optional blocks later, consider 'outer' join + fillna for flexibility.\n",
        "    feats = parts[0].join(parts[1:], how=\"outer\")\n",
        "\n",
        "    # Sanity check 2\n",
        "    ids = X_train.index.get_level_values(\"id\").unique().size\n",
        "    n = feats.index.size\n",
        "    if ids != n:\n",
        "        raise ValueError(f\"Feature table has {n} ids, but input had {ids} ids\")\n",
        "    detect_non_finite(feats)\n",
        "\n",
        "    # Save features\n",
        "    if not inference:\n",
        "        _save_cache(feats, prefix)\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "# Train one model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg-MWebkkX7o"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────\n",
        "#  1. build estimator\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def make_estimator(\n",
        "    model_type: str,\n",
        "    params: dict,\n",
        ") -> ClassifierMixin:\n",
        "    \"\"\"\n",
        "    Return an untrained binary classifier with default settings updated by `params`.\n",
        "    \"\"\"\n",
        "    params = params.copy()  # don't mutate caller's dict\n",
        "\n",
        "    # ---------------- XGBoost ----------------\n",
        "    if \"xgb\" in model_type:\n",
        "        if xgb is None:\n",
        "            raise ImportError(\"XGBoost not installed in the runner.\")\n",
        "\n",
        "        defaults = dict(\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"auc\",\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbosity=0,\n",
        "            max_bin=MAX_BIN,\n",
        "            n_jobs=-1,\n",
        "            use_label_encoder=False,\n",
        "            tree_method=\"hist\",  # faster histogram optimized\n",
        "        )\n",
        "        defaults.update(params)\n",
        "\n",
        "        return xgb.XGBClassifier(**defaults)\n",
        "\n",
        "    # ---------------- LightGBM -------------\n",
        "    elif \"lgb\" in model_type:\n",
        "        if lgb is None:\n",
        "            raise ImportError(\"LightGBM not installed in the runner.\")\n",
        "        defaults = dict(\n",
        "            objective=\"binary\",\n",
        "            metric=\"auc\",\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbosity=-1,\n",
        "            max_bin=MAX_BIN,  # controls histogram granularity\n",
        "            n_jobs=-1,  # controls how many threads you use, -1 is for max\n",
        "            force_row_wise=True,  # parallelizes the computation of the histogram along rows instead of columns\n",
        "        )\n",
        "        defaults.update(params)\n",
        "\n",
        "        return lgb.LGBMClassifier(**defaults)\n",
        "\n",
        "    # ---------------- CatBoost -------------------------------\n",
        "    elif \"cat\" in model_type:\n",
        "        if CatBoostClassifier is None:\n",
        "            raise ImportError(\"CatBoost not installed in the runner.\")\n",
        "        defaults = dict(\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            random_seed=RANDOM_STATE,\n",
        "            verbose=False,\n",
        "            allow_writing_files=False,\n",
        "        )\n",
        "        defaults.update(params)\n",
        "        return CatBoostClassifier(**defaults)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "#  2. build pipeline\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "class DataFrameScaler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Wrap a StandardScaler (or any sklearn transformer) so that both fit_transform\n",
        "    and transform return a DataFrame with the original columns & index.\n",
        "\n",
        "    Set `exclude` to a list of feature names that should be passed through\n",
        "    without scaling (if present in X).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scaler=None, exclude=None):\n",
        "        self.scaler = scaler if scaler is not None else StandardScaler()\n",
        "        self.exclude = set(exclude) if exclude is not None else set()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.columns_ = list(X.columns)\n",
        "\n",
        "        # self.exclude is now a list of *keywords*, not exact column names\n",
        "        kw = [k.lower() for k in self.exclude]\n",
        "\n",
        "        # columns to exclude = any keyword appears as a substring of the column name\n",
        "        self.exclude_ = [c for c in self.columns_ if any(k in c.lower() for k in kw)]\n",
        "        self.scale_cols_ = [c for c in self.columns_ if c not in self.exclude_]\n",
        "\n",
        "        # fit on the subset to be scaled (if any)\n",
        "        if len(self.scale_cols_) > 0:\n",
        "            self.scaler.fit(X[self.scale_cols_], y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # align columns to training order; missing columns will raise KeyError\n",
        "        X = X.loc[:, self.columns_]\n",
        "        if len(self.scale_cols_) > 0:\n",
        "            Xt_scaled = self.scaler.transform(X[self.scale_cols_])\n",
        "            X_out = X.copy()\n",
        "            X_out.loc[:, self.scale_cols_] = Xt_scaled\n",
        "        else:\n",
        "            X_out = X.copy()\n",
        "        return pd.DataFrame(X_out.values, columns=self.columns_, index=X.index)\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return np.asarray(self.columns_)\n",
        "\n",
        "\n",
        "def _fast_auc_binary(x: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"Mann–Whitney U / (n_pos*n_neg). NaN-safe (drops NaNs).\"\"\"\n",
        "    mask = np.isfinite(x)\n",
        "    x = x[mask]\n",
        "    yb = y[mask]\n",
        "    n1 = int((yb == 1).sum())\n",
        "    n0 = int((yb == 0).sum())\n",
        "    if n1 == 0 or n0 == 0:\n",
        "        return 0.5\n",
        "    r = rankdata(x)  # average ranks for ties\n",
        "    R1 = r[yb == 1].sum()\n",
        "    U1 = R1 - n1 * (n1 + 1) / 2.0\n",
        "    return float(U1 / (n1 * n0))\n",
        "\n",
        "\n",
        "class TopKUnivariateAUC(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Keep top-K features by univariate AUC with the label.\n",
        "    Always keep any columns listed in `always_keep` (if present in X).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        k: int = TOPK_FEATURES,\n",
        "        min_auc: float = TOPK_MIN_AUC,\n",
        "        always_keep=None,\n",
        "    ):\n",
        "        self.k = int(k)\n",
        "        self.min_auc = float(min_auc)\n",
        "        self.always_keep = list(always_keep) if always_keep is not None else []\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
        "        self.original_n_features_ = X.shape[1]\n",
        "\n",
        "        aucs = np.array(\n",
        "            [\n",
        "                _fast_auc_binary(X[c].to_numpy(dtype=float), y.to_numpy())\n",
        "                for c in X.columns\n",
        "            ]\n",
        "        )\n",
        "        score = np.abs(aucs - 0.5)\n",
        "\n",
        "        order = np.argsort(score)[::-1]\n",
        "        keep_idx = order[: self.k]\n",
        "        keep_idx = keep_idx[score[keep_idx] >= (self.min_auc - 0.5)]\n",
        "        selected = list(X.columns[keep_idx])\n",
        "\n",
        "        always = [c for c in self.always_keep if c in X.columns]\n",
        "        selected_set = set(selected) | set(always)\n",
        "        rest = sorted(\n",
        "            [c for c in selected_set if c not in always],\n",
        "            key=lambda c: (-score[X.columns.get_loc(c)], c),\n",
        "        )\n",
        "        self.keep_cols_ = always + rest\n",
        "\n",
        "        # bookkeeping\n",
        "        self.n_features_selected_ = len(self.keep_cols_)\n",
        "        self.selection_rate_ = self.n_features_selected_ / max(\n",
        "            1, self.original_n_features_\n",
        "        )\n",
        "        self.feature_scores_ = pd.Series(\n",
        "            score, index=X.columns\n",
        "        )  # optional: for inspection\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame):\n",
        "        cols = [c for c in self.keep_cols_ if c in X.columns]\n",
        "        return X.loc[:, cols]\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return np.asarray(self.keep_cols_)\n",
        "\n",
        "\n",
        "def make_pipeline(model_type: str, params: dict) -> Pipeline:\n",
        "    \"\"\"\n",
        "    Construct a sklearn Pipeline that standardizes features then applies\n",
        "    the specified model with given params.\n",
        "\n",
        "    [ TopK Filter ] → [ StandardScaler ] → [ model ]\n",
        "    \"\"\"\n",
        "    # There is one feature selection step I did which isn't shown here:\n",
        "    # After computing features I computed the correlation for each feature pair\n",
        "    # and deleted the ones with >0.98 in this file directly.\n",
        "    topk_auc = TopKUnivariateAUC(\n",
        "        k=TOPK_FEATURES, min_auc=TOPK_MIN_AUC, always_keep=TOPK_ALWAYS_KEEP\n",
        "    )\n",
        "    scaler = DataFrameScaler(exclude=EXCLUDE_FEATURE_KEYWORDS)\n",
        "    model = make_estimator(model_type, params)\n",
        "    return Pipeline(\n",
        "        [\n",
        "            (\"topk_auc\", topk_auc),\n",
        "            (\"scaler\", scaler),\n",
        "            (\"model\", model),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "#  4. one-fold fit util\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def fit_one_fold(\n",
        "    model_type: str,\n",
        "    pipeline: Pipeline,\n",
        "    Xtr: pd.DataFrame,\n",
        "    ytr: pd.Series,\n",
        "    Xva: pd.DataFrame,\n",
        "    yva: pd.Series,\n",
        "    early_stopping: int = EARLY_STOPPING,\n",
        ") -> Pipeline:\n",
        "    \"\"\"\n",
        "    Fit the pipeline on one CV fold, but transform eval_set through the\n",
        "    *fitted preprocessors* before passing it to the final estimator.\n",
        "    Prevents feature-mismatch with steps like TopK selectors.\n",
        "    \"\"\"\n",
        "    # split pipeline into [preprocessors] and [final model]\n",
        "    pre = Pipeline(pipeline.steps[:-1])  # e.g. (\"topk_auc\",\"scaler\")\n",
        "    model = pipeline.steps[-1][1]  # the estimator instance\n",
        "\n",
        "    # fit preprocessors on training split only (CV-safe) and transform both sets\n",
        "    Xtr_p = pre.fit_transform(Xtr, ytr)\n",
        "    Xva_p = pre.transform(Xva)\n",
        "\n",
        "    # Define model specific kwargs\n",
        "    fit_kwargs = {}\n",
        "\n",
        "    if \"xgb\" in model_type:\n",
        "        fit_kwargs[\"eval_set\"] = [(Xva_p, yva)]\n",
        "        (\n",
        "            model.set_params(\n",
        "                callbacks=[\n",
        "                    XGBEarlyStop(rounds=early_stopping, save_best=True, maximize=True)\n",
        "                ]\n",
        "            ),\n",
        "        )\n",
        "        fit_kwargs[\"verbose\"] = 0  # silence XGBoost logging\n",
        "    elif \"lgb\" in model_type:\n",
        "        fit_kwargs[\"eval_set\"] = [(Xva_p, yva)]\n",
        "        fit_kwargs[\"callbacks\"] = [lgb.early_stopping(early_stopping, verbose=False)]\n",
        "\n",
        "    elif \"cat\" in model_type:\n",
        "        fit_kwargs[\"eval_set\"] = (Xva_p, yva)\n",
        "        fit_kwargs[\"use_best_model\"] = True\n",
        "        fit_kwargs[\"early_stopping_rounds\"] = early_stopping\n",
        "\n",
        "    # Fit final model with early stopping on the *transformed* eval set\n",
        "    model.fit(Xtr_p, ytr, **fit_kwargs)\n",
        "\n",
        "    # rebuild a single pipeline carrying the fitted preprocessors + model\n",
        "    fitted = Pipeline(pre.steps + [(\"model\", model)])\n",
        "    return fitted\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "#  5. hyper-parameter tuning\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def default_param_grid(model_type: str):\n",
        "    \"\"\"Optuna search space for each model family.\"\"\"\n",
        "    if model_type == \"xgb_main\":\n",
        "        return {\n",
        "            \"learning_rate\": (0.02, 0.06, True),\n",
        "            \"max_depth\": (3, 5),\n",
        "            \"min_child_weight\": (70.0, 180.0, True),\n",
        "            \"gamma\": (1.0, 3.0, True),\n",
        "            \"subsample\": (0.65, 0.90),\n",
        "            \"colsample_bytree\": (0.50, 0.70),\n",
        "            \"colsample_bylevel\": (0.70, 0.90),\n",
        "            \"colsample_bynode\": (0.70, 0.90),\n",
        "            \"reg_alpha\": (0.5, 4.0, True),\n",
        "            \"reg_lambda\": (8.0, 40.0, True),\n",
        "            \"scale_pos_weight\": (0.9, 1.2),\n",
        "        }\n",
        "\n",
        "    elif model_type == \"xgb_lite\":\n",
        "        return {\n",
        "            \"learning_rate\": (0.035, 0.07, True),\n",
        "            \"max_depth\": (3, 3),  # not tuned, keep truly shallow\n",
        "            \"min_child_weight\": (120.0, 250.0, True),\n",
        "            \"gamma\": (1.4, 3.5, True),\n",
        "            \"subsample\": (0.60, 0.80),\n",
        "            \"colsample_bytree\": (0.45, 0.62),\n",
        "            \"colsample_bylevel\": (0.70, 0.90),\n",
        "            \"colsample_bynode\": (0.70, 0.90),\n",
        "            \"reg_alpha\": (1.0, 4.0, True),\n",
        "            \"reg_lambda\": (15.0, 40.0, True),\n",
        "            \"scale_pos_weight\": (0.9, 1.2),\n",
        "        }\n",
        "\n",
        "    elif model_type == \"lgb_main\":\n",
        "        return {\n",
        "            \"learning_rate\": (0.02, 0.06, True),\n",
        "            \"max_depth\": (4, 6),\n",
        "            \"num_leaves\": (24, 48),  # consistent with depth 4–6\n",
        "            \"min_child_samples\": (40, 120),\n",
        "            \"min_split_gain\": (0.0, 0.20),\n",
        "            \"feature_fraction\": (0.60, 0.85),\n",
        "            \"bagging_fraction\": (0.60, 0.85),\n",
        "            \"lambda_l1\": (0.2, 2.5, True),\n",
        "            \"lambda_l2\": (5.0, 25.0, True),\n",
        "            \"scale_pos_weight\": (0.9, 1.2),\n",
        "            # ⬇️ constants (not tuned)\n",
        "            \"extra_trees\": True,\n",
        "            \"bagging_freq\": 1,\n",
        "        }\n",
        "\n",
        "    elif model_type == \"cat_main\":\n",
        "        return {\n",
        "            \"learning_rate\": (0.02, 0.05, True),\n",
        "            \"depth\": (3, 5),\n",
        "            \"l2_leaf_reg\": (15.0, 60.0, True),\n",
        "            \"bootstrap_type\": [\"Bayesian\"],  # fix scheme; tune temperature\n",
        "            \"bagging_temperature\": (1.2, 2.6),\n",
        "            \"rsm\": (0.50, 0.70),\n",
        "            \"random_strength\": (1.2, 2.2),\n",
        "            \"min_data_in_leaf\": (150, 260),\n",
        "        }\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def optuna_objective(\n",
        "    trial: Trial,\n",
        "    model_type: str,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    groups: pd.Index,\n",
        "    cv: BaseCrossValidator,\n",
        ") -> tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    Optuna objective for each model family (and variants).\n",
        "    Supports:\n",
        "      • XGBoost\n",
        "      • LightGBM\n",
        "      • CatBoost\n",
        "    Returns (mean OOF AUC, sampled_hp_dict).\n",
        "    \"\"\"\n",
        "    grid = default_param_grid(model_type)\n",
        "\n",
        "    # ----- strict helpers: no fallbacks -----\n",
        "    def ffloat(name):\n",
        "        rng = grid[name]\n",
        "        if isinstance(rng, tuple) and len(rng) == 3:\n",
        "            return trial.suggest_float(name, rng[0], rng[1], log=rng[2])\n",
        "        return trial.suggest_float(name, rng[0], rng[1])\n",
        "\n",
        "    def fint(name):\n",
        "        rng = grid[name]\n",
        "        return trial.suggest_int(name, rng[0], rng[1])\n",
        "\n",
        "    def fcat(name):\n",
        "        return trial.suggest_categorical(name, grid[name])\n",
        "\n",
        "    is_xgb = \"xgb\" in model_type\n",
        "    is_lgb = \"lgb\" in model_type\n",
        "    is_cat = \"cat\" in model_type\n",
        "\n",
        "    # ----------------- XGBoost -----------------\n",
        "    if is_xgb:\n",
        "        hp = dict(\n",
        "            learning_rate=ffloat(\"learning_rate\"),\n",
        "            max_depth=fint(\"max_depth\"),\n",
        "            min_child_weight=ffloat(\"min_child_weight\"),\n",
        "            gamma=ffloat(\"gamma\"),\n",
        "            reg_alpha=ffloat(\"reg_alpha\"),\n",
        "            reg_lambda=ffloat(\"reg_lambda\"),\n",
        "            subsample=ffloat(\"subsample\"),\n",
        "            colsample_bytree=ffloat(\"colsample_bytree\"),\n",
        "            colsample_bylevel=ffloat(\"colsample_bylevel\"),\n",
        "            colsample_bynode=ffloat(\"colsample_bynode\"),\n",
        "            scale_pos_weight=ffloat(\"scale_pos_weight\"),\n",
        "        )\n",
        "\n",
        "    # ----------------- LightGBM -----------------\n",
        "    elif is_lgb:\n",
        "        hp = dict(\n",
        "            learning_rate=ffloat(\"learning_rate\"),\n",
        "            max_depth=fint(\"max_depth\"),\n",
        "            num_leaves=fint(\"num_leaves\"),\n",
        "            min_child_samples=fint(\"min_child_samples\"),\n",
        "            min_split_gain=ffloat(\"min_split_gain\"),\n",
        "            feature_fraction=ffloat(\"feature_fraction\"),\n",
        "            bagging_fraction=ffloat(\"bagging_fraction\"),\n",
        "            lambda_l1=ffloat(\"lambda_l1\"),\n",
        "            lambda_l2=ffloat(\"lambda_l2\"),\n",
        "            scale_pos_weight=ffloat(\"scale_pos_weight\"),\n",
        "            extra_trees=grid[\"extra_trees\"],\n",
        "            bagging_freq=grid[\"bagging_freq\"],\n",
        "        )\n",
        "\n",
        "    # ----------------- CatBoost -----------------\n",
        "    elif is_cat:\n",
        "        hp = dict(\n",
        "            learning_rate=ffloat(\"learning_rate\"),\n",
        "            depth=fint(\"depth\"),\n",
        "            l2_leaf_reg=ffloat(\"l2_leaf_reg\"),\n",
        "            rsm=ffloat(\"rsm\"),\n",
        "            random_strength=ffloat(\"random_strength\"),\n",
        "            min_data_in_leaf=fint(\"min_data_in_leaf\"),\n",
        "            bootstrap_type=fcat(\"bootstrap_type\"),\n",
        "        )\n",
        "        if hp[\"bootstrap_type\"] == \"Bayesian\":\n",
        "            hp[\"bagging_temperature\"] = ffloat(\"bagging_temperature\")\n",
        "        elif hp[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "            hp[\"subsample\"] = ffloat(\"subsample\")\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown model_type: {model_type}\")\n",
        "\n",
        "    # ensure every grid key is represented either as a sampled HP or a constant we injected\n",
        "    expected = set(grid.keys())\n",
        "    present = set(hp.keys())\n",
        "    missing = expected - present\n",
        "    if missing:\n",
        "        raise KeyError(f\"Optuna grid keys not used for {model_type}: {sorted(missing)}\")\n",
        "\n",
        "    # ----------------- CV training -----------------\n",
        "    pipe = make_pipeline(model_type, hp)\n",
        "    oof = np.zeros(len(X_train), dtype=float)\n",
        "\n",
        "    for tr_idx, va_idx in cv.split(X_train, y_train, groups):\n",
        "        pipe_fold = fit_one_fold(\n",
        "            model_type,\n",
        "            pipe,\n",
        "            X_train.iloc[tr_idx],\n",
        "            y_train.iloc[tr_idx],\n",
        "            X_train.iloc[va_idx],\n",
        "            y_train.iloc[va_idx],\n",
        "        )\n",
        "        oof[va_idx] = pipe_fold.predict_proba(X_train.iloc[va_idx])[:, 1]\n",
        "\n",
        "    return roc_auc_score(y_train, oof), hp\n",
        "\n",
        "\n",
        "def default_params(model_type: str):\n",
        "    \"\"\"Default parameters if no optuna tuning.\"\"\"\n",
        "    if model_type == \"xgb_main\":\n",
        "        return {\n",
        "            \"booster\": \"gbtree\",\n",
        "            \"n_estimators\": 2400,\n",
        "            \"learning_rate\": 0.035,\n",
        "            \"max_depth\": 4,\n",
        "            \"min_child_weight\": 110.0,\n",
        "            \"gamma\": 1.8,\n",
        "            \"subsample\": 0.75,\n",
        "            \"colsample_bytree\": 0.58,\n",
        "            \"colsample_bylevel\": 0.80,\n",
        "            \"colsample_bynode\": 0.80,\n",
        "            \"reg_alpha\": 1.5,\n",
        "            \"reg_lambda\": 18.0,\n",
        "            \"scale_pos_weight\": 1.0,\n",
        "        }\n",
        "\n",
        "    elif model_type == \"xgb_lite\":\n",
        "        return {\n",
        "            \"booster\": \"gbtree\",\n",
        "            \"n_estimators\": 1800,\n",
        "            \"learning_rate\": 0.050,\n",
        "            \"max_depth\": 3,\n",
        "            \"min_child_weight\": 170.0,\n",
        "            \"gamma\": 2.2,\n",
        "            \"subsample\": 0.68,\n",
        "            \"colsample_bytree\": 0.52,\n",
        "            \"colsample_bylevel\": 0.80,\n",
        "            \"colsample_bynode\": 0.80,\n",
        "            \"reg_alpha\": 2.0,\n",
        "            \"reg_lambda\": 25.0,\n",
        "            \"scale_pos_weight\": 1.0,\n",
        "        }\n",
        "\n",
        "    elif model_type == \"lgb_main\":\n",
        "        return {\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"n_estimators\": 2400,\n",
        "            \"learning_rate\": 0.035,\n",
        "            \"max_depth\": 5,\n",
        "            \"num_leaves\": 32,\n",
        "            \"min_child_samples\": 64,\n",
        "            \"min_sum_hessian_in_leaf\": 5.0,\n",
        "            \"min_split_gain\": 0.1,\n",
        "            \"feature_fraction\": 0.75,\n",
        "            \"bagging_freq\": 1,\n",
        "            \"bagging_fraction\": 0.75,\n",
        "            \"lambda_l1\": 1.0,\n",
        "            \"lambda_l2\": 10.0,\n",
        "            \"extra_trees\": True,\n",
        "            \"scale_pos_weight\": 1.0,\n",
        "            \"force_row_wise\": True,\n",
        "        }\n",
        "\n",
        "    elif model_type == \"cat_main\":\n",
        "        return {\n",
        "            \"n_estimators\": 2200,\n",
        "            \"learning_rate\": 0.035,\n",
        "            \"depth\": 4,\n",
        "            \"l2_leaf_reg\": 30.0,\n",
        "            \"bootstrap_type\": \"Bayesian\",\n",
        "            \"bagging_temperature\": 1.8,\n",
        "            \"rsm\": 0.58,\n",
        "            \"random_strength\": 1.7,\n",
        "            \"min_data_in_leaf\": 200,\n",
        "            \"od_type\": \"Iter\",\n",
        "            \"od_wait\": 100,\n",
        "        }\n",
        "\n",
        "\n",
        "def tune_params(\n",
        "    model_type: str,\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    groups: pd.Index,\n",
        "    cv: BaseCrossValidator,\n",
        "    n_optuna_trials: int,\n",
        ") -> dict:\n",
        "    \"\"\"Run Optuna and return best hyper-param dict.\"\"\"\n",
        "    if n_optuna_trials == 0 or \"dart\" in model_type:\n",
        "        return default_params(model_type)\n",
        "\n",
        "    def obj(trial):\n",
        "        score, _ = optuna_objective(trial, model_type, X_train, y_train, groups, cv)\n",
        "        return score\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
        "    )\n",
        "    study.optimize(obj, n_trials=n_optuna_trials, show_progress_bar=False)\n",
        "\n",
        "    _, best_hp = optuna_objective(\n",
        "        study.best_trial, model_type, X_train, y_train, groups, cv\n",
        "    )\n",
        "    return best_hp\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "#  6. main train_model() entry point\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _outer_cv(K, seed):\n",
        "    return StratifiedGroupKFold(n_splits=K, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "def _inner_cv(\n",
        "    X_tr,\n",
        "    y_tr,\n",
        "    g_tr,\n",
        "    seed: int,\n",
        "    fold_idx: int,\n",
        "    K_max_inner: int = K_MAX_INNER,\n",
        "    K_stop_inner: int = K_STOP_INNER,\n",
        ") -> BaseCrossValidator:\n",
        "    \"\"\"\n",
        "    Build an inner CV that *generates only the first K_INNER_STOP folds*\n",
        "    from a StratifiedGroupKFold constructed with n_splits=K_INNER_MAX.\n",
        "\n",
        "    Examples:\n",
        "      - K_INNER_MAX=2, K_INNER_STOP=1  -> single 50/50 holdout\n",
        "      - K_INNER_MAX=5, K_INNER_STOP=1  -> single ~80/20 holdout\n",
        "      - K_INNER_MAX=5, K_INNER_STOP=3  -> use 3 of the 5 folds (early-stop CV)\n",
        "    \"\"\"\n",
        "    if K_stop_inner < 1 or K_stop_inner > K_max_inner:\n",
        "        raise ValueError(\"Require 1 <= K_stop_inner <= K_max_inner\")\n",
        "\n",
        "    rng = seed + 69 + fold_idx\n",
        "\n",
        "    # Fast path: single holdout, precompute once and reuse (like your old _One)\n",
        "    if K_stop_inner == 1:\n",
        "        sgk = StratifiedGroupKFold(n_splits=K_max_inner, shuffle=True, random_state=rng)\n",
        "        tr_idx, va_idx = next(sgk.split(X_tr, y_tr, g_tr))\n",
        "\n",
        "        class _One(BaseCrossValidator):\n",
        "            def get_n_splits(self, *_, **__):\n",
        "                return 1\n",
        "            def split(self, X=None, y=None, groups=None):\n",
        "                # Reuse the precomputed indices\n",
        "                yield tr_idx, va_idx\n",
        "\n",
        "        return _One()\n",
        "\n",
        "    # General case: lazily yield only the first K_stop_inner folds (no list(...))\n",
        "    class _LimitedCV(BaseCrossValidator):\n",
        "        def __init__(self, n_splits, n_take, random_state):\n",
        "            self.n_splits = int(n_splits)\n",
        "            self.n_take = int(n_take)\n",
        "            self.random_state = int(random_state)\n",
        "\n",
        "        def get_n_splits(self, *_, **__):\n",
        "            return self.n_take\n",
        "\n",
        "        def split(self, X=None, y=None, groups=None):\n",
        "            sgk = StratifiedGroupKFold(\n",
        "                n_splits=self.n_splits,\n",
        "                shuffle=True,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "            # yield only the first n_take folds on demand\n",
        "            for i, (tr, va) in enumerate(sgk.split(X, y, groups)):\n",
        "                if i >= self.n_take:\n",
        "                    break\n",
        "                yield tr, va\n",
        "\n",
        "    return _LimitedCV(K_max_inner, K_stop_inner, rng)\n",
        "\n",
        "\n",
        "def _fold_dir(root: Path) -> Path:\n",
        "    d = Path(root) / \"fold_models\"\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    return d\n",
        "\n",
        "\n",
        "def _fold_paths(root: Path, k: int):\n",
        "    d = _fold_dir(root)\n",
        "    return {\n",
        "        \"pipe\": d / f\"fold_{k}.joblib\",\n",
        "        \"hp\": d / f\"fold_{k}_hp.json\",\n",
        "        \"met\": d / f\"fold_{k}_metrics.json\",\n",
        "    }\n",
        "\n",
        "\n",
        "def _extract_best_iteration(pipe: Pipeline, model_type: str) -> int | None:\n",
        "    est = pipe.named_steps.get(\"model\")\n",
        "    if est is None:\n",
        "        return None\n",
        "    if \"xgb\" in model_type:\n",
        "        bi = getattr(est, \"best_iteration\", None)\n",
        "        if bi is None:\n",
        "            booster = getattr(est, \"get_booster\", lambda: None)()\n",
        "            bi = getattr(booster, \"best_ntree_limit\", None)\n",
        "        return int(bi) if bi else None\n",
        "    if \"lgb\" in model_type:\n",
        "        bi = getattr(est, \"best_iteration_\", None)\n",
        "        return int(bi) if bi else None\n",
        "    if \"cat\" in model_type:\n",
        "        bi = est.get_best_iteration()\n",
        "        return int(bi) if bi else None\n",
        "    return None\n",
        "\n",
        "\n",
        "def _save_fold(\n",
        "    root: Path,\n",
        "    k: int,\n",
        "    pipe: Pipeline,\n",
        "    hp: dict,\n",
        "    train_auc: float,\n",
        "    val_auc: float,\n",
        "    best_iter: int | None,\n",
        "):\n",
        "    p = _fold_paths(root, k)\n",
        "    dump(pipe, p[\"pipe\"])\n",
        "    with open(p[\"hp\"], \"w\") as f:\n",
        "        json.dump(hp, f)\n",
        "    with open(p[\"met\"], \"w\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"train_auc\": float(train_auc),\n",
        "                \"val_auc\": float(val_auc),\n",
        "                \"best_iter\": best_iter,\n",
        "            },\n",
        "            f,\n",
        "        )\n",
        "\n",
        "\n",
        "def _has_full_cv_cache(root: Path, model_type: str, K: int, N: int) -> bool:\n",
        "    oof_path = Path(root) / f\"{model_type}_oof.npy\"\n",
        "    if not oof_path.exists():\n",
        "        return False\n",
        "    if np.load(oof_path).shape[0] != N:\n",
        "        return False\n",
        "    for k in range(K):\n",
        "        p = _fold_paths(root, k)\n",
        "        if not (p[\"pipe\"].exists() and p[\"hp\"].exists() and p[\"met\"].exists()):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def _load_cv_cache(root: Path, model_type: str, K: int):\n",
        "    oof = np.load(Path(root) / f\"{model_type}_oof.npy\")\n",
        "    hps, val_aucs, best_iters = [], [], []\n",
        "    for k in range(K):\n",
        "        p = _fold_paths(root, k)\n",
        "        with open(p[\"hp\"], \"r\") as f:\n",
        "            hps.append(json.load(f))\n",
        "        with open(p[\"met\"], \"r\") as f:\n",
        "            m = json.load(f)\n",
        "            val_aucs.append(float(m[\"val_auc\"]))\n",
        "            bi = m.get(\"best_iter\", None)\n",
        "            if isinstance(bi, (int, float)) and int(bi) > 0:\n",
        "                best_iters.append(int(bi))\n",
        "    return oof, hps, val_aucs, best_iters\n",
        "\n",
        "\n",
        "def _select_full_hps(\n",
        "    mode: str, model_type: str, fold_hps: list[dict], val_aucs: list[float]\n",
        ") -> dict:\n",
        "    if mode == \"best_outer\":\n",
        "        j = int(np.nanargmax(val_aucs))\n",
        "        return fold_hps[j]\n",
        "    if mode == \"consensus\":\n",
        "        # median for numerics, majority for bools\n",
        "        out = {}\n",
        "        for k in fold_hps[0].keys():\n",
        "            vals = [hp[k] for hp in fold_hps]\n",
        "            if all(isinstance(v, (bool, np.bool_)) for v in vals):\n",
        "                out[k] = sum(vals) >= (len(vals) - sum(vals))\n",
        "            elif all(\n",
        "                isinstance(v, (int, float, np.integer, np.floating)) for v in vals\n",
        "            ):\n",
        "                med = float(np.median(vals))\n",
        "                out[k] = (\n",
        "                    int(round(med))\n",
        "                    if all(isinstance(v, (int, np.integer)) for v in vals)\n",
        "                    else med\n",
        "                )\n",
        "        return out\n",
        "    return default_params(model_type)\n",
        "\n",
        "\n",
        "# ---------- main training ----------\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_type: str,\n",
        "    K_outer: int = K_OUTER,\n",
        "    K_max_inner: int = K_MAX_INNER,\n",
        "    K_stop_inner: int = K_STOP_INNER,\n",
        "    n_optuna_trials: int = N_OPTUNA_TRIALS,\n",
        "    seed: int = RANDOM_STATE,\n",
        "    model_dir: str = MODEL_DIR,\n",
        "    full_refit: bool = FULL_REFIT,\n",
        "    full_hp_selection: str = FULL_HP_SELECTION,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple, cache-aware training:\n",
        "      • If complete CV cache exists → load OOF/hps/metrics.\n",
        "      • Else run outer CV: tune per fold, early-stop, save each fold’s model+hp+metrics, save OOF.\n",
        "      • If full_refit: set capacity to median(best_iter) and fit/load cached full model.\n",
        "    Returns OOF predictions (np.ndarray).\n",
        "    \"\"\"\n",
        "    model_dir = Path(model_dir)\n",
        "    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    N = len(X_train)\n",
        "    groups = X_train.index  # id per row\n",
        "\n",
        "    # --- CV phase (load or train) ---\n",
        "    if _has_full_cv_cache(model_dir, model_type, K_outer, N):\n",
        "        print(f\"[{model_type}] Using cached CV.\")\n",
        "        oof, fold_hps, val_aucs, best_iters = _load_cv_cache(\n",
        "            model_dir, model_type, K_outer\n",
        "        )\n",
        "    else:\n",
        "        oof = np.zeros(N, dtype=float)\n",
        "        fold_hps, val_aucs, best_iters = [], [], []\n",
        "\n",
        "        for fold_idx, (tr_idx, va_idx) in enumerate(\n",
        "            _outer_cv(K_outer, seed).split(X_train, y_train, groups)\n",
        "        ):\n",
        "            X_tr, y_tr, g_tr = (\n",
        "                X_train.iloc[tr_idx],\n",
        "                y_train.iloc[tr_idx],\n",
        "                groups[tr_idx],\n",
        "            )\n",
        "            X_va, y_va = X_train.iloc[va_idx], y_train.iloc[va_idx]\n",
        "\n",
        "            inner = _inner_cv(X_tr, y_tr, g_tr, seed, fold_idx, K_max_inner, K_stop_inner)\n",
        "            hp = tune_params(model_type, X_tr, y_tr, g_tr, inner, n_optuna_trials)\n",
        "\n",
        "            # fit this outer fold with early stopping on (X_va, y_va)\n",
        "            pipe = make_pipeline(model_type, hp)\n",
        "            pipe = fit_one_fold(model_type, pipe, X_tr, y_tr, X_va, y_va)\n",
        "\n",
        "            # metrics + OOF\n",
        "            p_tr = pipe.predict_proba(X_tr)[:, 1]\n",
        "            p_va = pipe.predict_proba(X_va)[:, 1]\n",
        "            auc_tr = float(roc_auc_score(y_tr, p_tr))\n",
        "            auc_va = float(roc_auc_score(y_va, p_va))\n",
        "            oof[va_idx] = p_va\n",
        "\n",
        "            # save fold\n",
        "            bi = _extract_best_iteration(pipe, model_type)\n",
        "            _save_fold(model_dir, fold_idx, pipe, hp, auc_tr, auc_va, bi)\n",
        "\n",
        "            fold_hps.append(hp)\n",
        "            val_aucs.append(auc_va)\n",
        "\n",
        "            print(f\"[{model_type}] Fold {fold_idx}: TRAIN {auc_tr:.4f} | VAL {auc_va:.4f}\")\n",
        "\n",
        "        # save OOF\n",
        "        np.save(model_dir / f\"{model_type}_oof.npy\", oof)\n",
        "        print(f\"[{model_type}] OOF AUC = {roc_auc_score(y_train, oof):.4f}\")\n",
        "\n",
        "        # collect best_iters from what we just wrote\n",
        "        for k in range(K_outer):\n",
        "            with open(_fold_paths(model_dir, k)[\"met\"], \"r\") as f:\n",
        "                bi = json.load(f).get(\"best_iter\", None)\n",
        "                if isinstance(bi, (int, float)) and int(bi) > 0:\n",
        "                    best_iters.append(int(bi))\n",
        "\n",
        "    # --- Full refit (optional) ---\n",
        "    if full_refit:\n",
        "        hp_final = _select_full_hps(full_hp_selection, model_type, fold_hps, val_aucs)\n",
        "\n",
        "        # align capacity with early-stopped folds\n",
        "        hp_final[\"n_estimators\"] = int(np.median(best_iters))\n",
        "\n",
        "        # propagate seed\n",
        "        hp_final = hp_final.copy()\n",
        "        if \"cat\" in model_type:\n",
        "            hp_final.setdefault(\"random_seed\", seed)\n",
        "        else:\n",
        "            hp_final.setdefault(\"random_state\", seed)\n",
        "\n",
        "        full_path = model_dir / f\"{model_type}_full.joblib\"\n",
        "        if full_path.exists():\n",
        "            pipe_full = load(full_path)\n",
        "            print(f\"[{model_type}] Loaded full refit from cache.\")\n",
        "        else:\n",
        "            pipe_full = make_pipeline(model_type, hp_final)\n",
        "            pipe_full.fit(X_train, y_train)\n",
        "            dump(pipe_full, full_path)\n",
        "\n",
        "        # quick sanity\n",
        "        auc_full = roc_auc_score(y_train, pipe_full.predict_proba(X_train)[:, 1])\n",
        "        print(f\"[{model_type}] Full-data TRAIN AUC = {auc_full:.4f}\")\n",
        "\n",
        "    return oof\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9bYgvdbkX7r"
      },
      "source": [
        "# Stacking / The actual `train()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRGbEMIckX7w"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Banners\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _print_base_header(model_type: str, width: int = 72):\n",
        "    line = \"═\" * width\n",
        "    title = f\" TRAINING BASE — {model_type} \"\n",
        "    print(\"\\n\" + line)\n",
        "    print(title.center(width, \"═\"))\n",
        "    print(line)\n",
        "\n",
        "\n",
        "def _print_seed_header(\n",
        "    model_type: str, seed: int, idx: int, total: int, width: int = 72\n",
        "):\n",
        "    # one thin line above, left-aligned label\n",
        "    line = \"─\" * width\n",
        "    title = f\"[{model_type}] SEED {seed} ({idx}/{total}) — START\"\n",
        "    print(\"\\n\" + line)\n",
        "    print(title)\n",
        "\n",
        "\n",
        "def _print_auc_highlight(scope: str, auc: float, top_k: int, width: int = 72):\n",
        "    bar = \"█\" * width\n",
        "    msg = f\"{scope} — TOP{top_k} avg OOF AUC = {auc:.4f}\"\n",
        "    print(\"\\n\" + bar)\n",
        "    print(msg.center(width))\n",
        "    print(bar)\n",
        "\n",
        "\n",
        "def _print_meta_header(model_type: str, width: int = 72):\n",
        "    line = \"═\" * width\n",
        "    title = f\" TRAINING META — {model_type} \"\n",
        "    print(\"\\n\" + line)\n",
        "    print(title.center(width, \"═\"))\n",
        "    print(line)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Small utilities\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _seed_list(seed0: int, n: int) -> list[int]:\n",
        "    return [int(seed0 + 17 * i) for i in range(n)]\n",
        "\n",
        "\n",
        "def _avg_topk(records: list[dict], k: int) -> tuple[np.ndarray, list[dict], float]:\n",
        "    ranked = sorted(records, key=lambda r: r[\"auc\"], reverse=True)[: max(1, k)]\n",
        "    oofs = np.vstack([r[\"oof\"] for r in ranked])\n",
        "    avg = np.mean(oofs, axis=0)\n",
        "    return avg, ranked, float(np.mean([r[\"auc\"] for r in ranked]))\n",
        "\n",
        "\n",
        "def _ensure_dir(p: Path) -> Path:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Base learners (multi-seed)\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _train_base_one_seed(\n",
        "    X_feat: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    model_type: str,\n",
        "    seed: int,\n",
        "    root: Path,\n",
        "    K_outer: int = K_OUTER,\n",
        "    K_max_inner: int = K_MAX_INNER,\n",
        "    K_stop_inner: int = K_STOP_INNER,\n",
        "    n_optuna_trials: int = N_OPTUNA_TRIALS,\n",
        "    full_refit: bool = FULL_REFIT,\n",
        "    full_hp_selection: str = FULL_HP_SELECTION,\n",
        ") -> dict:\n",
        "    \"\"\"Train one seed for a base learner under: root/base/<model_type>/seed_<seed>/\"\"\"\n",
        "    seed_dir = _ensure_dir(root / \"base\" / model_type / f\"seed_{seed}\")\n",
        "    oof = train_model(\n",
        "        X_feat,\n",
        "        y,\n",
        "        model_type=model_type,\n",
        "        K_outer=K_outer,\n",
        "        K_max_inner=K_max_inner,\n",
        "        K_stop_inner=K_stop_inner,\n",
        "        n_optuna_trials=n_optuna_trials,\n",
        "        seed=seed,\n",
        "        model_dir=str(seed_dir),\n",
        "        full_refit=full_refit,\n",
        "        full_hp_selection=full_hp_selection,\n",
        "    )\n",
        "    np.save(seed_dir / \"oof.npy\", oof.astype(np.float32))\n",
        "    auc = float(roc_auc_score(y, oof))\n",
        "    return {\"seed\": seed, \"oof\": oof, \"auc\": auc, \"dir\": str(seed_dir)}\n",
        "\n",
        "\n",
        "def _train_base_multi_seeds(\n",
        "    X_feat: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    model_type: str,\n",
        "    root: Path,\n",
        "    seed0: int,\n",
        "    n_seeds: int = N_SEEDS,\n",
        "    top_seeds: int = TOP_SEEDS,\n",
        ") -> dict:\n",
        "    \"\"\"Train N_SEEDS seeds; keep top_seeds by OOF AUC; save avg OOF + a small summary.\"\"\"\n",
        "    model_root = _ensure_dir(root / \"base\" / model_type)\n",
        "    records = []\n",
        "    seeds = _seed_list(seed0, n_seeds)\n",
        "    for idx, s in enumerate(seeds, 1):\n",
        "        _print_seed_header(model_type, s, idx, len(seeds))\n",
        "        rec = _train_base_one_seed(X_feat, y, model_type, s, root)\n",
        "        records.append(rec)\n",
        "\n",
        "    avg_oof, top, avg_auc = _avg_topk(records, top_seeds)\n",
        "    np.save(model_root / f\"avg_top{top_seeds}_oof.npy\", avg_oof.astype(np.float32))\n",
        "\n",
        "    best_txt = \"\\n\".join(\n",
        "        f\"seed={r['seed']} auc={r['auc']:.6f} dir={r['dir']}\" for r in top\n",
        "    )\n",
        "    (model_root / f\"best_top{top_seeds}.txt\").write_text(\n",
        "        best_txt + f\"\\nAVG_AUC={avg_auc:.6f}\\n\"\n",
        "    )\n",
        "\n",
        "    _print_auc_highlight(f\"[BASE {model_type}]\", avg_auc, top_seeds)\n",
        "    return {\n",
        "        \"model_type\": model_type,\n",
        "        \"avg_oof\": avg_oof,\n",
        "        \"top\": top,\n",
        "        \"root\": str(model_root),\n",
        "    }\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# XGBoost meta-learner (multi-seed)\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _stack_features(S_base: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Build meta features: logits + simple stats.\"\"\"\n",
        "\n",
        "    def _logit_clip(p: np.ndarray, eps: float = 1e-5) -> np.ndarray:\n",
        "        p = np.clip(p.astype(np.float32), eps, 1 - eps)\n",
        "        return np.log(p) - np.log(1.0 - p)\n",
        "\n",
        "    L = pd.DataFrame(\n",
        "        {f\"{c}_logit\": _logit_clip(S_base[c].values) for c in S_base.columns},\n",
        "        index=S_base.index,\n",
        "    )\n",
        "    F = pd.concat(\n",
        "        [\n",
        "            L,\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"logit_mean\": L.mean(axis=1).astype(np.float32),\n",
        "                    \"logit_std\": L.std(axis=1).astype(np.float32),\n",
        "                },\n",
        "                index=S_base.index,\n",
        "            ),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "    return F\n",
        "\n",
        "\n",
        "def _train_xgb(F, y, params, num_round):\n",
        "    dtrain = xgb.DMatrix(F.values, label=y.values, feature_names=F.columns.tolist())\n",
        "    return xgb.train(\n",
        "        params=params, dtrain=dtrain, num_boost_round=num_round, verbose_eval=False\n",
        "    )\n",
        "\n",
        "\n",
        "def _meta_xgb_oof(\n",
        "    F: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    seed: int,\n",
        "    params: dict,\n",
        "    K_outer: int = K_OUTER,\n",
        ") -> tuple[float, np.ndarray]:\n",
        "    # Do cross validation\n",
        "    yv = y.astype(int).to_numpy()\n",
        "    groups = F.index.to_numpy()\n",
        "    skf = StratifiedGroupKFold(n_splits=K_outer, shuffle=True, random_state=seed)\n",
        "\n",
        "    oof = np.zeros(len(F), dtype=np.float32)\n",
        "    cols = F.columns.tolist()\n",
        "\n",
        "    for k, (tr_idx, va_idx) in enumerate(skf.split(F, yv, groups), 1):\n",
        "        # Split\n",
        "        Ftr, Fva = F.iloc[tr_idx], F.iloc[va_idx]\n",
        "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
        "\n",
        "        # Train xgb for this meta fold\n",
        "        hp = params.copy()\n",
        "        num_round = int(hp.pop(\"n_estimators\", 200))\n",
        "        dtr = xgb.DMatrix(Ftr.values, label=ytr.values, feature_names=cols)\n",
        "        dva = xgb.DMatrix(Fva.values, label=yva.values, feature_names=cols)\n",
        "        booster = xgb.train(\n",
        "            params=hp, dtrain=dtr, num_boost_round=num_round, verbose_eval=False\n",
        "        )\n",
        "\n",
        "        # Predict + metrics\n",
        "        p_tr = booster.predict(dtr).astype(np.float32)\n",
        "        p_va = booster.predict(dva).astype(np.float32)\n",
        "        oof[va_idx] = p_va\n",
        "\n",
        "        auc_tr = roc_auc_score(ytr, p_tr)\n",
        "        auc_va = roc_auc_score(yva, p_va)\n",
        "        print(\n",
        "            f\"[META-XGB] fold {k}/{K_outer}: TRAIN AUC={auc_tr:.4f} | VAL AUC={auc_va:.4f}\"\n",
        "        )\n",
        "\n",
        "    # Log perf\n",
        "    auc_oof = roc_auc_score(y, oof)\n",
        "    print(f\"[META-XGB] OOF AUC = {auc_oof:.4f}\")\n",
        "    return auc_oof, oof\n",
        "\n",
        "\n",
        "def _train_meta_xgb_one_seed(\n",
        "    S_base: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    seed: int,\n",
        "    out_dir: Path,\n",
        "    params: dict | None = None,\n",
        "    K_outer: int = K_OUTER,\n",
        ") -> dict:\n",
        "    \"\"\"Fit one XGB meta; compute **meta OOF** via K-fold; save final booster + OOF.\"\"\"\n",
        "    # 1) meta features\n",
        "    F = _stack_features(S_base)\n",
        "\n",
        "    # 2) default params\n",
        "    hp = dict(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"auc\",\n",
        "        max_depth=2,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=200,  # used as num_boost_round\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=1.0,\n",
        "        min_child_weight=20.0,\n",
        "        gamma=2.0,\n",
        "        reg_lambda=20.0,\n",
        "        reg_alpha=0.0,\n",
        "        max_bin=64,\n",
        "        tree_method=\"hist\",\n",
        "        verbosity=0,\n",
        "        seed=seed,\n",
        "    )\n",
        "    if params:\n",
        "        hp.update(params)\n",
        "\n",
        "    # 3) meta OOF\n",
        "    auc_oof, oof = _meta_xgb_oof(F, y, seed=seed, params=hp, K_outer=K_outer)\n",
        "\n",
        "    # 4) train final meta on all data (for inference)\n",
        "    hp_full = hp.copy()\n",
        "    num_round = int(hp_full.pop(\"n_estimators\", 200))\n",
        "    booster = _train_xgb(F, y, hp_full, num_round)\n",
        "\n",
        "    # 5) persist\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    booster.save_model(str(out_dir / \"model.json\"))\n",
        "    np.save(out_dir / \"oof.npy\", oof)\n",
        "\n",
        "    # 6) Log perf\n",
        "    dfull = xgb.DMatrix(F.values, feature_names=F.columns.tolist())\n",
        "    p_full = booster.predict(dfull).astype(np.float32)  # probabilities for class 1\n",
        "    auc_full = roc_auc_score(y, p_full)\n",
        "    print(f\"[META-XGB] Full-data TRAIN AUC = {auc_full:.4f}\")\n",
        "\n",
        "    return {\"seed\": seed, \"auc\": auc_oof, \"oof\": oof, \"dir\": str(out_dir)}\n",
        "\n",
        "\n",
        "def _train_meta_xgb_multi_seeds(\n",
        "    S_base: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    root: Path,\n",
        "    seed0: int,\n",
        "    n_seeds: int = N_SEEDS,\n",
        "    top_seeds: int = TOP_SEEDS,\n",
        ") -> dict:\n",
        "    \"\"\"Train multiple meta seeds; rank by **meta OOF AUC**; save artifact with top paths.\"\"\"\n",
        "    base_dir = root / \"meta\" / \"xgb\"\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    records = []\n",
        "    seeds = _seed_list(seed0, n_seeds)\n",
        "    for idx, s in enumerate(seeds, 1):\n",
        "        _print_seed_header(\"META-XGB\", s, idx, len(seeds))\n",
        "        rec = _train_meta_xgb_one_seed(S_base, y, s, base_dir / f\"seed_{s}\")\n",
        "        records.append(rec)\n",
        "\n",
        "    avg_oof, top, avg_auc = _avg_topk(records, top_seeds)\n",
        "    np.save(base_dir / f\"avg_top{top_seeds}_oof.npy\", avg_oof.astype(np.float32))\n",
        "\n",
        "    artifact = {\n",
        "        \"type\": \"xgb_meta\",\n",
        "        \"top_seeds\": [r[\"seed\"] for r in top],\n",
        "        \"model_paths\": [\n",
        "            str(base_dir / f\"seed_{r['seed']}\" / \"model.json\") for r in top\n",
        "        ],\n",
        "    }\n",
        "    dump(artifact, root / \"meta_artifact.joblib\")\n",
        "\n",
        "    _print_auc_highlight(\"[META-XGB]\", avg_auc, top_seeds)\n",
        "    return {\"avg_oof\": avg_oof, \"auc\": avg_auc, \"artifact\": artifact, \"top\": top}\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Orchestrator\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    base_models: tuple[str] = BASE_LEARNERS,\n",
        "    seed: int = RANDOM_STATE,\n",
        "    model_dir: str = MODEL_DIR,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    1) Build per-id features once.\n",
        "    2) Train each base across N_SEEDS; keep TOP_SEEDS; average their OOFs.\n",
        "    3) Stack averaged OOF columns → train XGB meta (multi-seed); keep TOP_SEEDS; save artifact.\n",
        "    \"\"\"\n",
        "    root = _ensure_dir(Path(model_dir))\n",
        "\n",
        "    # 1) features\n",
        "    X_feat = build_features(X_train, force_prep=False, force_all=False)\n",
        "\n",
        "    # Sanity check\n",
        "    train_n = X_train.index.get_level_values(\"id\").unique().size\n",
        "    feat_n = len(X_feat)\n",
        "    if train_n != feat_n:\n",
        "        raise ValueError(\n",
        "            \"train_n != feat_n, you should probably delete resources/features which was computed on a different X_train\"\n",
        "        )\n",
        "\n",
        "    # 2) bases\n",
        "    oof_cols: dict[str, np.ndarray] = {}\n",
        "    base_info: dict[str, dict] = {}\n",
        "    for i, m in enumerate(base_models):\n",
        "        _print_base_header(m)\n",
        "        res = _train_base_multi_seeds(\n",
        "            X_feat, y_train, model_type=m, root=root, seed0=seed + 100 * (i + 1)\n",
        "        )\n",
        "        oof_cols[f\"{m}_oof\"] = res[\"avg_oof\"]\n",
        "        base_info[m] = res\n",
        "\n",
        "    S_base = pd.DataFrame(oof_cols, index=X_feat.index)\n",
        "\n",
        "    # 3) meta (only XGB)\n",
        "    _print_meta_header(\"XGB\")\n",
        "    meta_res = _train_meta_xgb_multi_seeds(\n",
        "        S_base, y_train, root=root, seed0=seed + 2000\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[STACK] Final meta OOF AUC = {meta_res['auc']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"stack_base\": S_base,\n",
        "        \"per_base\": base_info,\n",
        "        \"meta_oof\": meta_res[\"avg_oof\"],\n",
        "        \"meta_auc\": meta_res[\"auc\"],\n",
        "        \"meta_artifact\": meta_res[\"artifact\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMtDzt_vkX70"
      },
      "source": [
        "# The `infer()` Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5-_SxMZkX70"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Loaders\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _best_seeds(base_dir: Path) -> list[int]:\n",
        "    \"\"\"Parse best_top*.txt and return the listed seeds (sorted).\"\"\"\n",
        "    txt = sorted(base_dir.glob(\"best_top*.txt\"))[-1]\n",
        "    seeds = []\n",
        "    for line in txt.read_text().splitlines():\n",
        "        if line.startswith(\"seed=\"):\n",
        "            seeds.append(int(line.split(\"seed=\")[1].split()[0]))\n",
        "    return sorted(seeds)\n",
        "\n",
        "\n",
        "def _load_fold_pipelines(seed_root: Path) -> list:\n",
        "    \"\"\"Load all fold_{k}.joblib under a seed path, sorted by k.\"\"\"\n",
        "    fold_dir = seed_root / \"fold_models\"\n",
        "    folds = []\n",
        "    fold_re = re.compile(r\"fold_(\\d+)\\.joblib$\")\n",
        "    for p in sorted(\n",
        "        fold_dir.glob(\"fold_*.joblib\"),\n",
        "        key=lambda q: int(fold_re.search(q.name).group(1)),\n",
        "    ):\n",
        "        folds.append(load(p))\n",
        "    return folds\n",
        "\n",
        "\n",
        "def _load_base_fold_models(\n",
        "    model_dir: Path, base_learners: Iterable[str]\n",
        ") -> dict[str, dict[int, list]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      { model_type: { seed: [fold_pipeline0, fold_pipeline1, ...] } }\n",
        "    \"\"\"\n",
        "    out: dict[str, dict[int, list]] = {}\n",
        "    base_root = model_dir / \"base\"\n",
        "    for m in base_learners:\n",
        "        m_root = base_root / m\n",
        "        seed_map: dict[int, list] = {}\n",
        "        for s in _best_seeds(m_root):\n",
        "            folds = _load_fold_pipelines(m_root / f\"seed_{s}\")\n",
        "            seed_map[s] = folds\n",
        "        out[m] = seed_map\n",
        "    return out\n",
        "\n",
        "\n",
        "def _load_base_full_models(\n",
        "    model_dir: Path, base_learners: Iterable[str]\n",
        ") -> dict[str, dict[int, object]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      { model_type: { seed: full_refit_pipeline } }\n",
        "    Expects a file named {model_type}_full.joblib under each seed directory.\n",
        "    \"\"\"\n",
        "    out: dict[str, dict[int, object]] = {}\n",
        "    base_root = model_dir / \"base\"\n",
        "    for m in base_learners:\n",
        "        m_root = base_root / m\n",
        "        seed_map: dict[int, object] = {}\n",
        "        for s in _best_seeds(m_root):\n",
        "            p = m_root / f\"seed_{s}\" / f\"{m}_full.joblib\"\n",
        "            seed_map[s] = load(p)\n",
        "        out[m] = seed_map\n",
        "    return out\n",
        "\n",
        "\n",
        "def _load_meta_model_paths(model_dir: Path) -> list[str]:\n",
        "    \"\"\"Load meta_artifact.joblib and return list of XGB model paths.\"\"\"\n",
        "    art = load(model_dir / \"meta_artifact.joblib\")\n",
        "    return list(art[\"model_paths\"])\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# Prediction helpers\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _predict_base_fold_ensemble(fold_pipes: list, X: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Average predictions across all folds for ONE seed.\"\"\"\n",
        "    preds = [\n",
        "        pipe.predict_proba(X)[:, 1].astype(np.float32, copy=False)\n",
        "        for pipe in fold_pipes\n",
        "    ]\n",
        "    return np.mean(np.vstack(preds), axis=0).astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "def _predict_base_across_seeds_fold(\n",
        "    models_per_seed: dict[int, list], X: pd.DataFrame\n",
        ") -> np.ndarray:\n",
        "    \"\"\"For one base (fold mode): average across folds per seed, then average across seeds.\"\"\"\n",
        "    per_seed = [\n",
        "        _predict_base_fold_ensemble(folds, X) for folds in models_per_seed.values()\n",
        "    ]\n",
        "    return np.mean(np.vstack(per_seed), axis=0).astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "def _predict_base_across_seeds_full(\n",
        "    models_per_seed: dict[int, object], X: pd.DataFrame\n",
        ") -> np.ndarray:\n",
        "    \"\"\"For one base (full-refit mode): average full-refit pipelines across seeds.\"\"\"\n",
        "    per_seed = [\n",
        "        pipe.predict_proba(X)[:, 1].astype(np.float32, copy=False)\n",
        "        for pipe in models_per_seed.values()\n",
        "    ]\n",
        "    return np.mean(np.vstack(per_seed), axis=0).astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "def _predict_meta_xgb(model_paths: list[str], F_star: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Average predictions across the meta top seeds (all XGBoost).\"\"\"\n",
        "    dtest = xgb.DMatrix(F_star.values, feature_names=list(F_star.columns))\n",
        "    preds = []\n",
        "    for mp in model_paths:\n",
        "        booster = xgb.Booster()\n",
        "        booster.load_model(mp)\n",
        "        preds.append(booster.predict(dtest).astype(np.float32, copy=False))\n",
        "    return np.mean(np.vstack(preds), axis=0).astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "# iinfer(): choose between fold-ensemble or full-refit bases\n",
        "# ─────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def infer(\n",
        "    X_test: Iterable[pd.DataFrame],\n",
        "    model_dir: Path = MODEL_DIR,\n",
        "    inference_mode: str = INFERENCE_MODE,  # \"fold\" | \"full\"\n",
        "    base_learners: list = BASE_LEARNERS,\n",
        "):\n",
        "    \"\"\"\n",
        "    Inference pipeline:\n",
        "\n",
        "    inference_mode = \"fold\" (default):\n",
        "        • Load per-seed fold models for each base.\n",
        "        • For each shard (one id):\n",
        "            - build per-id features (inference=True),\n",
        "            - base preds: average across folds per seed, then across seeds,\n",
        "            - stack → meta features → XGB meta (avg across meta top seeds),\n",
        "            - yield np.ndarray([prob]).\n",
        "\n",
        "    inference_mode = \"full\":\n",
        "        • Load per-seed FULL-REFIT pipelines for each base ({model}_full.joblib).\n",
        "        • Same as above but base preds are averaged across seeds only (no folds).\n",
        "    \"\"\"\n",
        "    if inference_mode == \"fold\":\n",
        "        base_models = _load_base_fold_models(model_dir, base_learners)\n",
        "        base_predict = _predict_base_across_seeds_fold\n",
        "    elif inference_mode == \"full\":\n",
        "        base_models = _load_base_full_models(model_dir, base_learners)\n",
        "        base_predict = _predict_base_across_seeds_full\n",
        "    else:\n",
        "        raise ValueError(\"inference_mode must be 'fold' or 'full'\")\n",
        "\n",
        "    meta_paths = _load_meta_model_paths(model_dir)\n",
        "\n",
        "    # signal readiness\n",
        "    yield\n",
        "\n",
        "    # process each dataset exactly once (each shard = one id's long df)\n",
        "    for df_raw in X_test:\n",
        "        # one id per shard\n",
        "        id1 = df_raw.index.get_level_values(\"id\").unique()[0]\n",
        "\n",
        "        # per-id features (no cache I/O)\n",
        "        X_feat = build_features(df_raw, inference=True).reindex([id1])\n",
        "\n",
        "        # base predictions → stacked frame\n",
        "        base_stack = {\n",
        "            f\"{m}_oof\": base_predict(models_per_seed, X_feat)\n",
        "            for m, models_per_seed in base_models.items()\n",
        "        }\n",
        "        S_star = pd.DataFrame(base_stack, index=[id1])\n",
        "\n",
        "        # meta features + XGB meta prediction\n",
        "        F_star = _stack_features(S_star)\n",
        "        pred = _predict_meta_xgb(meta_paths, F_star)\n",
        "\n",
        "        yield pred.astype(np.float32, copy=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "# Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDZeP-4--0fU",
        "outputId": "8bfdb1e2-446e-4cfe-9888-c9f07fb4aeab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23:17:30 \n",
            "23:17:31 started\n",
            "23:17:31 running local test\n",
            "23:17:31 internet access isn't restricted, no check will be done\n",
            "23:17:31 \n",
            "23:17:32 starting unstructured loop...\n",
            "23:17:32 executing - command=train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train: skip param with default value: base_models=('xgb_main', 'xgb_lite', 'lgb_main', 'cat_main')\n",
            "train: skip param with default value: seed=69\n",
            "train: skip param with default value: model_dir=resources/model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cached data from resources/features/all_0930_1346.parquet\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "═══════════════════════ TRAINING BASE — xgb_main ═══════════════════════\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 169 (1/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7387 | VAL 0.6898\n",
            "[xgb_main] Fold 1: TRAIN 0.7407 | VAL 0.6989\n",
            "[xgb_main] Fold 2: TRAIN 0.7221 | VAL 0.6787\n",
            "[xgb_main] Fold 3: TRAIN 0.7453 | VAL 0.6730\n",
            "[xgb_main] Fold 4: TRAIN 0.7544 | VAL 0.6584\n",
            "[xgb_main] OOF AUC = 0.6781\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7373\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 186 (2/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7396 | VAL 0.6801\n",
            "[xgb_main] Fold 1: TRAIN 0.7549 | VAL 0.6928\n",
            "[xgb_main] Fold 2: TRAIN 0.7495 | VAL 0.6908\n",
            "[xgb_main] Fold 3: TRAIN 0.7369 | VAL 0.6752\n",
            "[xgb_main] Fold 4: TRAIN 0.7503 | VAL 0.6659\n",
            "[xgb_main] OOF AUC = 0.6777\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7512\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 203 (3/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7594 | VAL 0.6918\n",
            "[xgb_main] Fold 1: TRAIN 0.7634 | VAL 0.6970\n",
            "[xgb_main] Fold 2: TRAIN 0.7300 | VAL 0.6819\n",
            "[xgb_main] Fold 3: TRAIN 0.7408 | VAL 0.6794\n",
            "[xgb_main] Fold 4: TRAIN 0.7291 | VAL 0.6488\n",
            "[xgb_main] OOF AUC = 0.6799\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7381\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 220 (4/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7399 | VAL 0.6916\n",
            "[xgb_main] Fold 1: TRAIN 0.7434 | VAL 0.6853\n",
            "[xgb_main] Fold 2: TRAIN 0.7550 | VAL 0.6625\n",
            "[xgb_main] Fold 3: TRAIN 0.7453 | VAL 0.7033\n",
            "[xgb_main] Fold 4: TRAIN 0.7596 | VAL 0.6837\n",
            "[xgb_main] OOF AUC = 0.6848\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7445\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 237 (5/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7363 | VAL 0.6940\n",
            "[xgb_main] Fold 1: TRAIN 0.7473 | VAL 0.6603\n",
            "[xgb_main] Fold 2: TRAIN 0.7656 | VAL 0.6706\n",
            "[xgb_main] Fold 3: TRAIN 0.7361 | VAL 0.6715\n",
            "[xgb_main] Fold 4: TRAIN 0.7565 | VAL 0.6893\n",
            "[xgb_main] OOF AUC = 0.6748\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7420\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 254 (6/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7237 | VAL 0.6704\n",
            "[xgb_main] Fold 1: TRAIN 0.7447 | VAL 0.6681\n",
            "[xgb_main] Fold 2: TRAIN 0.7433 | VAL 0.7008\n",
            "[xgb_main] Fold 3: TRAIN 0.7577 | VAL 0.6922\n",
            "[xgb_main] Fold 4: TRAIN 0.7489 | VAL 0.6616\n",
            "[xgb_main] OOF AUC = 0.6764\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7588\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 271 (7/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7452 | VAL 0.6931\n",
            "[xgb_main] Fold 1: TRAIN 0.7069 | VAL 0.6695\n",
            "[xgb_main] Fold 2: TRAIN 0.7559 | VAL 0.6724\n",
            "[xgb_main] Fold 3: TRAIN 0.7557 | VAL 0.6726\n",
            "[xgb_main] Fold 4: TRAIN 0.7491 | VAL 0.6833\n",
            "[xgb_main] OOF AUC = 0.6753\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7369\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 288 (8/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7333 | VAL 0.6825\n",
            "[xgb_main] Fold 1: TRAIN 0.7551 | VAL 0.6956\n",
            "[xgb_main] Fold 2: TRAIN 0.7575 | VAL 0.6837\n",
            "[xgb_main] Fold 3: TRAIN 0.7592 | VAL 0.6656\n",
            "[xgb_main] Fold 4: TRAIN 0.7509 | VAL 0.6779\n",
            "[xgb_main] OOF AUC = 0.6797\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7501\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 305 (9/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7641 | VAL 0.6830\n",
            "[xgb_main] Fold 1: TRAIN 0.7576 | VAL 0.6983\n",
            "[xgb_main] Fold 2: TRAIN 0.7588 | VAL 0.6707\n",
            "[xgb_main] Fold 3: TRAIN 0.7441 | VAL 0.6918\n",
            "[xgb_main] Fold 4: TRAIN 0.7523 | VAL 0.6669\n",
            "[xgb_main] OOF AUC = 0.6819\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7515\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 322 (10/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7520 | VAL 0.6969\n",
            "[xgb_main] Fold 1: TRAIN 0.7564 | VAL 0.6705\n",
            "[xgb_main] Fold 2: TRAIN 0.7544 | VAL 0.6684\n",
            "[xgb_main] Fold 3: TRAIN 0.7427 | VAL 0.6987\n",
            "[xgb_main] Fold 4: TRAIN 0.7421 | VAL 0.6841\n",
            "[xgb_main] OOF AUC = 0.6832\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7522\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 339 (11/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7516 | VAL 0.6995\n",
            "[xgb_main] Fold 1: TRAIN 0.7483 | VAL 0.6960\n",
            "[xgb_main] Fold 2: TRAIN 0.7463 | VAL 0.6768\n",
            "[xgb_main] Fold 3: TRAIN 0.7475 | VAL 0.6768\n",
            "[xgb_main] Fold 4: TRAIN 0.7447 | VAL 0.6638\n",
            "[xgb_main] OOF AUC = 0.6814\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7451\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 356 (12/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7207 | VAL 0.7078\n",
            "[xgb_main] Fold 1: TRAIN 0.7234 | VAL 0.6786\n",
            "[xgb_main] Fold 2: TRAIN 0.7514 | VAL 0.6658\n",
            "[xgb_main] Fold 3: TRAIN 0.7346 | VAL 0.6655\n",
            "[xgb_main] Fold 4: TRAIN 0.7446 | VAL 0.6882\n",
            "[xgb_main] OOF AUC = 0.6750\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7392\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 373 (13/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7530 | VAL 0.6916\n",
            "[xgb_main] Fold 1: TRAIN 0.7636 | VAL 0.6901\n",
            "[xgb_main] Fold 2: TRAIN 0.7585 | VAL 0.6962\n",
            "[xgb_main] Fold 3: TRAIN 0.7561 | VAL 0.6758\n",
            "[xgb_main] Fold 4: TRAIN 0.7507 | VAL 0.6604\n",
            "[xgb_main] OOF AUC = 0.6815\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7607\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 390 (14/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7625 | VAL 0.6908\n",
            "[xgb_main] Fold 1: TRAIN 0.7665 | VAL 0.6845\n",
            "[xgb_main] Fold 2: TRAIN 0.7596 | VAL 0.6821\n",
            "[xgb_main] Fold 3: TRAIN 0.7486 | VAL 0.6843\n",
            "[xgb_main] Fold 4: TRAIN 0.7714 | VAL 0.6882\n",
            "[xgb_main] OOF AUC = 0.6847\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7645\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 407 (15/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7398 | VAL 0.7011\n",
            "[xgb_main] Fold 1: TRAIN 0.7591 | VAL 0.6876\n",
            "[xgb_main] Fold 2: TRAIN 0.7434 | VAL 0.6723\n",
            "[xgb_main] Fold 3: TRAIN 0.7538 | VAL 0.6779\n",
            "[xgb_main] Fold 4: TRAIN 0.7443 | VAL 0.6682\n",
            "[xgb_main] OOF AUC = 0.6809\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7397\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 424 (16/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7557 | VAL 0.7039\n",
            "[xgb_main] Fold 1: TRAIN 0.7604 | VAL 0.6659\n",
            "[xgb_main] Fold 2: TRAIN 0.7351 | VAL 0.6821\n",
            "[xgb_main] Fold 3: TRAIN 0.7626 | VAL 0.7000\n",
            "[xgb_main] Fold 4: TRAIN 0.7476 | VAL 0.6638\n",
            "[xgb_main] OOF AUC = 0.6819\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7605\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 441 (17/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7386 | VAL 0.6962\n",
            "[xgb_main] Fold 1: TRAIN 0.7383 | VAL 0.6710\n",
            "[xgb_main] Fold 2: TRAIN 0.7537 | VAL 0.6753\n",
            "[xgb_main] Fold 3: TRAIN 0.7433 | VAL 0.6866\n",
            "[xgb_main] Fold 4: TRAIN 0.7540 | VAL 0.6877\n",
            "[xgb_main] OOF AUC = 0.6817\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7471\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 458 (18/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7670 | VAL 0.6906\n",
            "[xgb_main] Fold 1: TRAIN 0.7252 | VAL 0.6797\n",
            "[xgb_main] Fold 2: TRAIN 0.7602 | VAL 0.7072\n",
            "[xgb_main] Fold 3: TRAIN 0.7606 | VAL 0.6576\n",
            "[xgb_main] Fold 4: TRAIN 0.7578 | VAL 0.6831\n",
            "[xgb_main] OOF AUC = 0.6836\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7583\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 475 (19/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7442 | VAL 0.6949\n",
            "[xgb_main] Fold 1: TRAIN 0.7325 | VAL 0.7032\n",
            "[xgb_main] Fold 2: TRAIN 0.7647 | VAL 0.6647\n",
            "[xgb_main] Fold 3: TRAIN 0.7701 | VAL 0.6687\n",
            "[xgb_main] Fold 4: TRAIN 0.7547 | VAL 0.6751\n",
            "[xgb_main] OOF AUC = 0.6793\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7491\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_main] SEED 492 (20/20) — START\n",
            "[xgb_main] Fold 0: TRAIN 0.7472 | VAL 0.6989\n",
            "[xgb_main] Fold 1: TRAIN 0.7585 | VAL 0.6753\n",
            "[xgb_main] Fold 2: TRAIN 0.7604 | VAL 0.7151\n",
            "[xgb_main] Fold 3: TRAIN 0.7601 | VAL 0.6616\n",
            "[xgb_main] Fold 4: TRAIN 0.7594 | VAL 0.6798\n",
            "[xgb_main] OOF AUC = 0.6854\n",
            "[xgb_main] Full-data TRAIN AUC = 0.7505\n",
            "\n",
            "████████████████████████████████████████████████████████████████████████\n",
            "              [BASE xgb_main] — TOP2 avg OOF AUC = 0.6851               \n",
            "████████████████████████████████████████████████████████████████████████\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "═══════════════════════ TRAINING BASE — xgb_lite ═══════════════════════\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 269 (1/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7156\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 286 (2/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7198\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 303 (3/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7192\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 320 (4/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7174\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 337 (5/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7189\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 354 (6/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7160\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 371 (7/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7145\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 388 (8/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7135\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 405 (9/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7171\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 422 (10/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7121\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 439 (11/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7169\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 456 (12/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7169\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 473 (13/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7189\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 490 (14/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7257\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 507 (15/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7226\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 524 (16/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7153\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 541 (17/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7180\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 558 (18/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7174\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 575 (19/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7179\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[xgb_lite] SEED 592 (20/20) — START\n",
            "[xgb_lite] Using cached CV.\n",
            "[xgb_lite] Loaded full refit from cache.\n",
            "[xgb_lite] Full-data TRAIN AUC = 0.7230\n",
            "\n",
            "████████████████████████████████████████████████████████████████████████\n",
            "              [BASE xgb_lite] — TOP2 avg OOF AUC = 0.6751               \n",
            "████████████████████████████████████████████████████████████████████████\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "═══════════════════════ TRAINING BASE — lgb_main ═══════════════════════\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 369 (1/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7536\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 386 (2/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7579\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 403 (3/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7529\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 420 (4/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7721\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 437 (5/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7669\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 454 (6/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7581\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 471 (7/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7674\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 488 (8/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7567\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 505 (9/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7539\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 522 (10/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7545\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 539 (11/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7479\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 556 (12/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7589\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 573 (13/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7677\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 590 (14/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7606\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 607 (15/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7695\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 624 (16/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7596\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 641 (17/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7595\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 658 (18/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7551\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 675 (19/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7589\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[lgb_main] SEED 692 (20/20) — START\n",
            "[lgb_main] Using cached CV.\n",
            "[lgb_main] Loaded full refit from cache.\n",
            "[lgb_main] Full-data TRAIN AUC = 0.7549\n",
            "\n",
            "████████████████████████████████████████████████████████████████████████\n",
            "              [BASE lgb_main] — TOP2 avg OOF AUC = 0.6861               \n",
            "████████████████████████████████████████████████████████████████████████\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "═══════════════════════ TRAINING BASE — cat_main ═══════════════════════\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 469 (1/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.9255\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 486 (2/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8837\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 503 (3/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8466\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 520 (4/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.9165\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 537 (5/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8404\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 554 (6/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8790\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 571 (7/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8547\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 588 (8/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8790\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 605 (9/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8906\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 622 (10/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8710\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 639 (11/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8667\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 656 (12/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8469\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 673 (13/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8648\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 690 (14/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8439\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 707 (15/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8759\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 724 (16/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8160\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 741 (17/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.9250\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 758 (18/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8579\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 775 (19/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8791\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[cat_main] SEED 792 (20/20) — START\n",
            "[cat_main] Using cached CV.\n",
            "[cat_main] Loaded full refit from cache.\n",
            "[cat_main] Full-data TRAIN AUC = 0.8503\n",
            "\n",
            "████████████████████████████████████████████████████████████████████████\n",
            "              [BASE cat_main] — TOP2 avg OOF AUC = 0.6993               \n",
            "████████████████████████████████████████████████████████████████████████\n",
            "\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "═════════════════════════ TRAINING META — XGB ══════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2069 (1/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7156 | VAL AUC=0.7135\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7130 | VAL AUC=0.7165\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7193 | VAL AUC=0.6927\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7165 | VAL AUC=0.7049\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7222 | VAL AUC=0.6808\n",
            "[META-XGB] OOF AUC = 0.7010\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7170\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2086 (2/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7195 | VAL AUC=0.6895\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7182 | VAL AUC=0.6954\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7149 | VAL AUC=0.7127\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7193 | VAL AUC=0.6933\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7171 | VAL AUC=0.7022\n",
            "[META-XGB] OOF AUC = 0.6970\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7162\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2103 (3/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7182 | VAL AUC=0.6953\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7128 | VAL AUC=0.7239\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7211 | VAL AUC=0.6889\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7184 | VAL AUC=0.7096\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7182 | VAL AUC=0.6982\n",
            "[META-XGB] OOF AUC = 0.7028\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7166\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2120 (4/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7194 | VAL AUC=0.6970\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7120 | VAL AUC=0.7309\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7217 | VAL AUC=0.6823\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7176 | VAL AUC=0.7011\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7205 | VAL AUC=0.6901\n",
            "[META-XGB] OOF AUC = 0.6984\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7166\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2137 (5/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7127 | VAL AUC=0.7225\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7171 | VAL AUC=0.7076\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7204 | VAL AUC=0.6894\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7210 | VAL AUC=0.6827\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7180 | VAL AUC=0.6996\n",
            "[META-XGB] OOF AUC = 0.6993\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7174\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2154 (6/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7151 | VAL AUC=0.7145\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7150 | VAL AUC=0.7109\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7153 | VAL AUC=0.7104\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7236 | VAL AUC=0.6790\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7220 | VAL AUC=0.6816\n",
            "[META-XGB] OOF AUC = 0.6977\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7177\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2171 (7/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7140 | VAL AUC=0.7218\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7117 | VAL AUC=0.7300\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7233 | VAL AUC=0.6685\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7198 | VAL AUC=0.6913\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7186 | VAL AUC=0.7028\n",
            "[META-XGB] OOF AUC = 0.7013\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7167\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2188 (8/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7136 | VAL AUC=0.7196\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7159 | VAL AUC=0.7098\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7217 | VAL AUC=0.6905\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7222 | VAL AUC=0.6853\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7174 | VAL AUC=0.6997\n",
            "[META-XGB] OOF AUC = 0.6994\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7160\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2205 (9/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7192 | VAL AUC=0.6950\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7110 | VAL AUC=0.7262\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7163 | VAL AUC=0.7069\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7164 | VAL AUC=0.7059\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7245 | VAL AUC=0.6733\n",
            "[META-XGB] OOF AUC = 0.6990\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7166\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2222 (10/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7150 | VAL AUC=0.7102\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7131 | VAL AUC=0.7239\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7191 | VAL AUC=0.6957\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7211 | VAL AUC=0.6840\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7217 | VAL AUC=0.6912\n",
            "[META-XGB] OOF AUC = 0.6985\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7169\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2239 (11/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7139 | VAL AUC=0.7173\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7149 | VAL AUC=0.7170\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7199 | VAL AUC=0.6934\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7204 | VAL AUC=0.6884\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7190 | VAL AUC=0.6978\n",
            "[META-XGB] OOF AUC = 0.7015\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7147\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2256 (12/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7134 | VAL AUC=0.7206\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7214 | VAL AUC=0.6822\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7205 | VAL AUC=0.6920\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7185 | VAL AUC=0.7034\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7163 | VAL AUC=0.7045\n",
            "[META-XGB] OOF AUC = 0.6988\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7172\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2273 (13/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7140 | VAL AUC=0.7261\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7235 | VAL AUC=0.6785\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7149 | VAL AUC=0.7065\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7160 | VAL AUC=0.7057\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7212 | VAL AUC=0.6881\n",
            "[META-XGB] OOF AUC = 0.6994\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7165\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2290 (14/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7128 | VAL AUC=0.7229\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7174 | VAL AUC=0.6997\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7190 | VAL AUC=0.6982\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7204 | VAL AUC=0.6960\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7202 | VAL AUC=0.6950\n",
            "[META-XGB] OOF AUC = 0.7007\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7167\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2307 (15/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7146 | VAL AUC=0.7186\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7181 | VAL AUC=0.6952\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7170 | VAL AUC=0.7005\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7217 | VAL AUC=0.6813\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7156 | VAL AUC=0.7083\n",
            "[META-XGB] OOF AUC = 0.6990\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7163\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2324 (16/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7183 | VAL AUC=0.7036\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7167 | VAL AUC=0.7040\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7171 | VAL AUC=0.7058\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7174 | VAL AUC=0.7065\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7209 | VAL AUC=0.6844\n",
            "[META-XGB] OOF AUC = 0.6997\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7164\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2341 (17/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7162 | VAL AUC=0.7079\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7174 | VAL AUC=0.7009\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7214 | VAL AUC=0.6850\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7150 | VAL AUC=0.7069\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7189 | VAL AUC=0.6993\n",
            "[META-XGB] OOF AUC = 0.6988\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7164\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2358 (18/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7128 | VAL AUC=0.7232\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7195 | VAL AUC=0.7021\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7192 | VAL AUC=0.6864\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7181 | VAL AUC=0.6967\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7174 | VAL AUC=0.7031\n",
            "[META-XGB] OOF AUC = 0.7017\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7163\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2375 (19/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7217 | VAL AUC=0.6892\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7184 | VAL AUC=0.6973\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7191 | VAL AUC=0.6946\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7111 | VAL AUC=0.7322\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7203 | VAL AUC=0.6887\n",
            "[META-XGB] OOF AUC = 0.6998\n",
            "[META-XGB] Full-data TRAIN AUC = 0.7159\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────\n",
            "[META-XGB] SEED 2392 (20/20) — START\n",
            "[META-XGB] fold 1/5: TRAIN AUC=0.7125 | VAL AUC=0.7215\n",
            "[META-XGB] fold 2/5: TRAIN AUC=0.7194 | VAL AUC=0.6958\n",
            "[META-XGB] fold 3/5: TRAIN AUC=0.7173 | VAL AUC=0.7032\n",
            "[META-XGB] fold 4/5: TRAIN AUC=0.7161 | VAL AUC=0.7085\n",
            "[META-XGB] fold 5/5: TRAIN AUC=0.7234 | VAL AUC=0.6831\n",
            "[META-XGB] OOF AUC = 0.7012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "23:44:46 executing - command=infer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[META-XGB] Full-data TRAIN AUC = 0.7170\n",
            "\n",
            "████████████████████████████████████████████████████████████████████████\n",
            "                 [META-XGB] — TOP2 avg OOF AUC = 0.7022                 \n",
            "████████████████████████████████████████████████████████████████████████\n",
            "\n",
            "[STACK] Final meta OOF AUC = 0.7022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "infer: skip param with default value: inference_mode=full\n",
            "infer: skip param with default value: base_learners=('xgb_main', 'xgb_lite', 'lgb_main', 'cat_main')\n",
            "23:45:08 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "23:45:08 executing - command=infer\n",
            "23:45:13 determinism check: passed\n",
            "23:45:13 save prediction - path=data/prediction.parquet\n",
            "23:45:13 ended\n",
            "23:45:13 duration - time=00:27:42\n",
            "23:45:13 memory - before=\"268.93 MB\" after=\"266.75 MB\" consumed=\"-2179072 bytes\"\n"
          ]
        }
      ],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "# Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "ly5q68sA-0fU",
        "outputId": "4ab7c035-6bdc-41dc-85b5-c11c8ddc6d1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>0.160384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>0.200757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10003</th>\n",
              "      <td>0.131639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>0.168218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10005</th>\n",
              "      <td>0.197960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10097</th>\n",
              "      <td>0.208788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10098</th>\n",
              "      <td>0.130918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10099</th>\n",
              "      <td>0.104289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10100</th>\n",
              "      <td>0.244442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10101</th>\n",
              "      <td>0.249520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       prediction\n",
              "id               \n",
              "10001    0.160384\n",
              "10002    0.200757\n",
              "10003    0.131639\n",
              "10004    0.168218\n",
              "10005    0.197960\n",
              "...           ...\n",
              "10097    0.208788\n",
              "10098    0.130918\n",
              "10099    0.104289\n",
              "10100    0.244442\n",
              "10101    0.249520\n",
              "\n",
              "[101 rows x 1 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyCrjpzv-0fU",
        "outputId": "4dd5e952-db2d-465f-b231-7bf606161b19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.711737089201878"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Machine Learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}