# Structural Break Detection - Stacked Ensemble

## Context
This repository contains my solution code for the **CrunchDAO - Structural Break** competition.  
ðŸ‘‰ Competition page: https://hub.crunchdao.com/competitions/structural-break


## Task & Data
Binary classification on univariate time series. For each time series ID we observe a univariate series `value[t]` split into two segments by a `period` flag (e.g., `0 = before`, `1 = after`). The label is `y[id] âˆˆ {0,1}`, where `1` means the after segment exhibits a structural break relative to before. A structural break is defined as a change in the underlying data-generating process. The goal is to maximize ROC-AUC on hidden IDs.

![Structural break example](docs/break_example.png)

## My Approach In 5 Bullets

- **Preprocessing (per ID & period):** Robust standardization using BEFORE-segment median/MAD, winsorization based on BEFORE-segment thresholds, and per-segment detrending and mean-centering; outputs a long table with parallel series (e.g., `standardized`, `clipped`, `detrended`, diffs, abs, squared), one column per series needed downstream.
- **Feature blocks:** 200+ features including quantile ratios, threshold-crossing rates, autocorrelation deltas, statistical tests, distances, and FFT/bandpower metrics over standardized `z`, first differences `Î”z`, second differences `Î”Â²z`, absolute values `|z|`, squared values `zÂ²`. Also include boundary/local features around the beforeâ†’after breakpoint and rolling-window statistics. Each block is cached to Parquet and merged by `id`.
- **Feature Selection:** Prune collinear features by dropping one from each pair with |Ï| > 0.98 (I did that manually that's why it's not in the code), then retain all features whose univariate ROC-AUC exceeds a set threshold.
- **Base Learners:** Train multiple base learners (e.g., XGB/LGB/Cat) with nested CV. Outer CV to produce OOF predictions on held-out IDs, lightweight inner CV via Optuna to select hyperparameters. Repeat across seeds then take the average top seeds for predictions.
- **Meta-Learner/Stacking:** Build meta-features from base OOFs (per-model logits + `logit_mean`/`logit_std`), train a small XGBoost meta-learner across seeds, average top seeds for final predictions.

## Repo Layout

~~~text
ADIA-STRUCTURAL-BREAK/
â”œâ”€ data/                                      
â”‚  â”œâ”€ ids_holdout.csv                          # X_train IDs reserved as a holdout; append to X_test.index to create a larger local test set for evaluation
â”‚  â”œâ”€ prediction.parquet                       # latest runâ€™s per-ID predicted probabilities on X_test.reduced
â”‚  â”œâ”€ X_test.reduced.parquet                  
â”‚  â”œâ”€ X_train.part1.parquet                    # X_train is split in 3 because GitHub only allows files < 100 MB
â”‚  â”œâ”€ X_train.part2.parquet
â”‚  â”œâ”€ X_train.part3.parquet
â”‚  â”œâ”€ y_test.reduced.parquet                  
â”‚  â””â”€ y_train.parquet                         
â”œâ”€ docs/
â”‚  â””â”€ break_example.png                        # example series with a structural break (referenced in README)
â”‚  â””â”€ preprocess_columns.md                    # column spec for preprocess_*.parquet (definitions + which feature blocks use which series)
â”œâ”€ notebooks/
â”‚  â”œâ”€ exploratory_data_analysis.ipynb
â”‚  â””â”€ final_submission.ipynb                        
â”œâ”€ resources/                                  # all cached artifacts
â”‚  â”œâ”€ features/                                
â”‚  â”‚  â”œâ”€ preprocessed_*.parquet                # preprocessed time series; input to feature blocks (columns documented in docs/preprocess_columns.md)
â”‚  â”‚  â”œâ”€ all_*.parquet                         # join of all feature blocks; input to base learners
â”‚  â”‚  â”œâ”€ moments_*.parquet                     # moments
â”‚  â”‚  â”œâ”€ quantiles_*.parquet                   # robust quantile stats
â”‚  â”‚  â”œâ”€ rates_*.parquet                       # change/rate features
â”‚  â”‚  â”œâ”€ autocorr_*.parquet                    # ACF/PACF-style summaries
â”‚  â”‚  â”œâ”€ tests_distances_*.parquet             # statistical tests & distances
â”‚  â”‚  â”œâ”€ frequency_*.parquet                   # FFT/bandpower features
â”‚  â”‚  â”œâ”€ differences_*.parquet                 # diffs/abs-diffs/curvature
â”‚  â”‚  â”œâ”€ absolute_*.parquet                    # |z| features
â”‚  â”‚  â”œâ”€ squared_*.parquet                     # zÂ² features
â”‚  â”‚  â”œâ”€ boundary_local_*.parquet              # local boundary stats
â”‚  â”‚  â”œâ”€ boundary_edge_*.parquet               # edge/before-after deltas
â”‚  â”‚  â”œâ”€ curvature_*.parquet                   # 2nd-diff summaries
â”‚  â”‚  â”œâ”€ rolling_*.parquet                     # rolling windows
â”‚  â”‚  â””â”€ ar_*.parquet                          # AR model coefficients
â”‚  â””â”€ model/
â”‚     â”œâ”€ base/             
â”‚     â”‚  â”œâ”€ xgb_main/
â”‚     â”‚  â”‚  â”œâ”€ seed_169/
â”‚     â”‚  â”‚  â”‚  â”œâ”€ fold_models/                 
â”‚     â”‚  â”‚  â”‚  â”‚  â”œâ”€ fold_0.joblib             # serialized sklearn pipeline
â”‚     â”‚  â”‚  â”‚  â”‚  â”œâ”€ fold_0_hp.json            # hyperparameters selected via inner-CV/Optuna
â”‚     â”‚  â”‚  â”‚  â”‚  ... (one per outer fold)
â”‚     â”‚  â”‚  â”‚  â”œâ”€ cvmeta.joblib                # cache signature (inputs/knobs)
â”‚     â”‚  â”‚  â”‚  â”œâ”€ full_data_refit.joblib       # full-data refit (optional)
â”‚     â”‚  â”‚  â”‚  â”œâ”€ metrics.json                 # per-fold AUCs + pooled OOF AUC + full-data AUC
â”‚     â”‚  â”‚  â”‚  â””â”€ oof.npy                      # OOF predictions
â”‚     â”‚  â”‚  â”œâ”€ seed_186/
â”‚     â”‚  â”‚  â”œâ”€ seed_203/
â”‚     â”‚  â”‚  â”œâ”€ avg_top2_oof.npy                # averaged OOF of top-K seeds
â”‚     â”‚  â”‚  â””â”€ avg_top2.txt                    # list of top-K seeds + ensemble AUC
â”‚     â”‚  â”œâ”€ cat_main/                             
â”‚     â”‚  â”œâ”€ lgb_main/              
â”‚     â”‚  â””â”€ xgb_lite/                      
â”‚     â””â”€ meta/         
â”‚        â””â”€ xgb/
â”‚           â”œâ”€ seed_2069/
â”‚           â”‚  â”œâ”€ meta.json                    # cache signature (inputs/knobs)
â”‚           â”‚  â”œâ”€ metrics.json                 # per-fold AUCs + pooled OOF AUC + full-data AUC
â”‚           â”‚  â”œâ”€ model.json                   # XGBoost booster
â”‚           â”‚  â””â”€ oof.npy                      # OOF predictions
â”‚           â”œâ”€ seed_2086/
â”‚           â”œâ”€ seed_2103/
â”‚           â”œâ”€ avg_top2_oof.npy
â”‚           â””â”€ avg_top2.txt
â”‚     â””â”€ meta_artifact.joblib                  # selected meta seeds + model paths for inference
â”œâ”€ src/
â”‚  â”œâ”€ config.py                                # global knobs (CV, seeds, paths, thresholds)
â”‚  â”œâ”€ preprocess.py                            # heavy preprocessing (standardize/winsorize/detrend)
â”‚  â”œâ”€ feature_extraction.py                    # computes each feature block
â”‚  â”œâ”€ train.py                                 # base-learner
â”‚  â”œâ”€ stacking.py                              # meta-learner
â”‚  â”œâ”€ infer.py                                 # fold-ensemble â†’ seed-average â†’ meta
â”‚  â””â”€ utils.py                                 # small helpers
â”œâ”€ .gitignore                                  
â”œâ”€ main.ipynb                                  
â”œâ”€ readme.md                                   
â””â”€ requirements.txt                          
~~~